Directory structure:
â””â”€â”€ cocoindex-io-cocoindex/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ Cargo.toml
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ ruff.toml
    â”œâ”€â”€ .env.lib_debug
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ dev/
    â”‚   â”œâ”€â”€ neo4j.yaml
    â”‚   â””â”€â”€ postgres.yaml
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ docusaurus.config.ts
    â”‚   â”œâ”€â”€ package.json
    â”‚   â”œâ”€â”€ sidebars.ts
    â”‚   â”œâ”€â”€ tsconfig.json
    â”‚   â”œâ”€â”€ docs/
    â”‚   â”‚   â”œâ”€â”€ query.mdx
    â”‚   â”‚   â”œâ”€â”€ about/
    â”‚   â”‚   â”‚   â”œâ”€â”€ community.md
    â”‚   â”‚   â”‚   â””â”€â”€ contributing.md
    â”‚   â”‚   â”œâ”€â”€ ai/
    â”‚   â”‚   â”‚   â””â”€â”€ llm.mdx
    â”‚   â”‚   â”œâ”€â”€ core/
    â”‚   â”‚   â”‚   â”œâ”€â”€ basics.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ cli.mdx
    â”‚   â”‚   â”‚   â”œâ”€â”€ custom_function.mdx
    â”‚   â”‚   â”‚   â”œâ”€â”€ data_types.mdx
    â”‚   â”‚   â”‚   â”œâ”€â”€ flow_def.mdx
    â”‚   â”‚   â”‚   â”œâ”€â”€ flow_methods.mdx
    â”‚   â”‚   â”‚   â””â”€â”€ settings.mdx
    â”‚   â”‚   â”œâ”€â”€ getting_started/
    â”‚   â”‚   â”‚   â”œâ”€â”€ installation.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ markdown_files.zip
    â”‚   â”‚   â”‚   â”œâ”€â”€ overview.md
    â”‚   â”‚   â”‚   â””â”€â”€ quickstart.md
    â”‚   â”‚   â””â”€â”€ ops/
    â”‚   â”‚       â”œâ”€â”€ functions.md
    â”‚   â”‚       â”œâ”€â”€ sources.md
    â”‚   â”‚       â””â”€â”€ targets.md
    â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”‚   â””â”€â”€ HomepageFeatures/
    â”‚   â”‚   â”‚       â”œâ”€â”€ index.tsx
    â”‚   â”‚   â”‚       â””â”€â”€ styles.module.css
    â”‚   â”‚   â”œâ”€â”€ css/
    â”‚   â”‚   â”‚   â””â”€â”€ custom.css
    â”‚   â”‚   â””â”€â”€ theme/
    â”‚   â”‚       â””â”€â”€ Root.js
    â”‚   â””â”€â”€ static/
    â”‚       â”œâ”€â”€ robots.txt
    â”‚       â””â”€â”€ .nojekyll
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ amazon_s3_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ .env.example
    â”‚   â”œâ”€â”€ azure_blob_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ .env.example
    â”‚   â”œâ”€â”€ code_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ docs_to_knowledge_graph/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ face_recognition/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ fastapi_server_docker/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ compose.yaml
    â”‚   â”‚   â”œâ”€â”€ dockerfile
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”‚   â”œâ”€â”€ .dockerignore
    â”‚   â”‚   â””â”€â”€ files/
    â”‚   â”‚       â””â”€â”€ 1810.04805v2.md
    â”‚   â”œâ”€â”€ gdrive_text_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ .env.example
    â”‚   â”œâ”€â”€ image_search/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ frontend/
    â”‚   â”‚       â”œâ”€â”€ index.html
    â”‚   â”‚       â”œâ”€â”€ package.json
    â”‚   â”‚       â”œâ”€â”€ vite.config.js
    â”‚   â”‚       â””â”€â”€ src/
    â”‚   â”‚           â”œâ”€â”€ App.jsx
    â”‚   â”‚           â”œâ”€â”€ main.jsx
    â”‚   â”‚           â””â”€â”€ style.css
    â”‚   â”œâ”€â”€ manuals_llm_extraction/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ paper_metadata/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ .env.example
    â”‚   â”œâ”€â”€ patient_intake_extraction/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ .env.example
    â”‚   â”‚   â””â”€â”€ data/
    â”‚   â”‚       â”œâ”€â”€ README.md
    â”‚   â”‚       â””â”€â”€ patient_forms/
    â”‚   â”‚           â”œâ”€â”€ Patient_Intake_Form_David_Artificial.docx
    â”‚   â”‚           â””â”€â”€ Patient_Intake_From_Jane_Artificial.docx
    â”‚   â”œâ”€â”€ pdf_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ product_recommendation/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ products/
    â”‚   â”‚       â”œâ”€â”€ p1.json
    â”‚   â”‚       â”œâ”€â”€ p2.json
    â”‚   â”‚       â”œâ”€â”€ p3.json
    â”‚   â”‚       â”œâ”€â”€ p4.json
    â”‚   â”‚       â”œâ”€â”€ p5.json
    â”‚   â”‚       â”œâ”€â”€ p6.json
    â”‚   â”‚       â”œâ”€â”€ p7.json
    â”‚   â”‚       â”œâ”€â”€ p8.json
    â”‚   â”‚       â””â”€â”€ p9.json
    â”‚   â”œâ”€â”€ text_embedding/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ Text_Embedding.ipynb
    â”‚   â”‚   â””â”€â”€ markdown_files/
    â”‚   â”‚       â”œâ”€â”€ 1706.03762v7.md
    â”‚   â”‚       â”œâ”€â”€ 1810.04805v2.md
    â”‚   â”‚       â””â”€â”€ rfc8259.md
    â”‚   â””â”€â”€ text_embedding_qdrant/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ main.py
    â”‚       â”œâ”€â”€ pyproject.toml
    â”‚       â””â”€â”€ markdown_files/
    â”‚           â””â”€â”€ rfc8259.md
    â”œâ”€â”€ python/
    â”‚   â””â”€â”€ cocoindex/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ auth_registry.py
    â”‚       â”œâ”€â”€ cli.py
    â”‚       â”œâ”€â”€ convert.py
    â”‚       â”œâ”€â”€ flow.py
    â”‚       â”œâ”€â”€ functions.py
    â”‚       â”œâ”€â”€ index.py
    â”‚       â”œâ”€â”€ lib.py
    â”‚       â”œâ”€â”€ llm.py
    â”‚       â”œâ”€â”€ op.py
    â”‚       â”œâ”€â”€ py.typed
    â”‚       â”œâ”€â”€ runtime.py
    â”‚       â”œâ”€â”€ setting.py
    â”‚       â”œâ”€â”€ setup.py
    â”‚       â”œâ”€â”€ sources.py
    â”‚       â”œâ”€â”€ targets.py
    â”‚       â”œâ”€â”€ typing.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â””â”€â”€ tests/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ test_convert.py
    â”‚           â”œâ”€â”€ test_optional_database.py
    â”‚           â””â”€â”€ test_typing.py
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ lib.rs
    â”‚   â”œâ”€â”€ lib_context.rs
    â”‚   â”œâ”€â”€ prelude.rs
    â”‚   â”œâ”€â”€ server.rs
    â”‚   â”œâ”€â”€ settings.rs
    â”‚   â”œâ”€â”€ base/
    â”‚   â”‚   â”œâ”€â”€ duration.rs
    â”‚   â”‚   â”œâ”€â”€ field_attrs.rs
    â”‚   â”‚   â”œâ”€â”€ json_schema.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â”œâ”€â”€ schema.rs
    â”‚   â”‚   â”œâ”€â”€ spec.rs
    â”‚   â”‚   â””â”€â”€ value.rs
    â”‚   â”œâ”€â”€ builder/
    â”‚   â”‚   â”œâ”€â”€ analyzed_flow.rs
    â”‚   â”‚   â”œâ”€â”€ analyzer.rs
    â”‚   â”‚   â”œâ”€â”€ exec_ctx.rs
    â”‚   â”‚   â”œâ”€â”€ flow_builder.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â””â”€â”€ plan.rs
    â”‚   â”œâ”€â”€ execution/
    â”‚   â”‚   â”œâ”€â”€ db_tracking.rs
    â”‚   â”‚   â”œâ”€â”€ db_tracking_setup.rs
    â”‚   â”‚   â”œâ”€â”€ dumper.rs
    â”‚   â”‚   â”œâ”€â”€ evaluator.rs
    â”‚   â”‚   â”œâ”€â”€ indexing_status.rs
    â”‚   â”‚   â”œâ”€â”€ live_updater.rs
    â”‚   â”‚   â”œâ”€â”€ memoization.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â”œâ”€â”€ row_indexer.rs
    â”‚   â”‚   â”œâ”€â”€ source_indexer.rs
    â”‚   â”‚   â””â”€â”€ stats.rs
    â”‚   â”œâ”€â”€ llm/
    â”‚   â”‚   â”œâ”€â”€ anthropic.rs
    â”‚   â”‚   â”œâ”€â”€ gemini.rs
    â”‚   â”‚   â”œâ”€â”€ litellm.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â”œâ”€â”€ ollama.rs
    â”‚   â”‚   â”œâ”€â”€ openai.rs
    â”‚   â”‚   â”œâ”€â”€ openrouter.rs
    â”‚   â”‚   â”œâ”€â”€ vertex_ai.rs
    â”‚   â”‚   â”œâ”€â”€ vllm.rs
    â”‚   â”‚   â””â”€â”€ voyage.rs
    â”‚   â”œâ”€â”€ ops/
    â”‚   â”‚   â”œâ”€â”€ factory_bases.rs
    â”‚   â”‚   â”œâ”€â”€ interface.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â”œâ”€â”€ py_factory.rs
    â”‚   â”‚   â”œâ”€â”€ registration.rs
    â”‚   â”‚   â”œâ”€â”€ registry.rs
    â”‚   â”‚   â”œâ”€â”€ sdk.rs
    â”‚   â”‚   â”œâ”€â”€ functions/
    â”‚   â”‚   â”‚   â”œâ”€â”€ embed_text.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ extract_by_llm.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ parse_json.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ split_recursively.rs
    â”‚   â”‚   â”‚   â””â”€â”€ test_utils.rs
    â”‚   â”‚   â”œâ”€â”€ sources/
    â”‚   â”‚   â”‚   â”œâ”€â”€ amazon_s3.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ azure_blob.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ google_drive.rs
    â”‚   â”‚   â”‚   â”œâ”€â”€ local_file.rs
    â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
    â”‚   â”‚   â””â”€â”€ targets/
    â”‚   â”‚       â”œâ”€â”€ kuzu.rs
    â”‚   â”‚       â”œâ”€â”€ mod.rs
    â”‚   â”‚       â”œâ”€â”€ neo4j.rs
    â”‚   â”‚       â”œâ”€â”€ postgres.rs
    â”‚   â”‚       â”œâ”€â”€ qdrant.rs
    â”‚   â”‚       â””â”€â”€ shared/
    â”‚   â”‚           â”œâ”€â”€ mod.rs
    â”‚   â”‚           â”œâ”€â”€ property_graph.rs
    â”‚   â”‚           â””â”€â”€ table_columns.rs
    â”‚   â”œâ”€â”€ py/
    â”‚   â”‚   â”œâ”€â”€ convert.rs
    â”‚   â”‚   â””â”€â”€ mod.rs
    â”‚   â”œâ”€â”€ service/
    â”‚   â”‚   â”œâ”€â”€ error.rs
    â”‚   â”‚   â”œâ”€â”€ flows.rs
    â”‚   â”‚   â””â”€â”€ mod.rs
    â”‚   â”œâ”€â”€ setup/
    â”‚   â”‚   â”œâ”€â”€ auth_registry.rs
    â”‚   â”‚   â”œâ”€â”€ components.rs
    â”‚   â”‚   â”œâ”€â”€ db_metadata.rs
    â”‚   â”‚   â”œâ”€â”€ driver.rs
    â”‚   â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”‚   â””â”€â”€ states.rs
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ concur_control.rs
    â”‚       â”œâ”€â”€ db.rs
    â”‚       â”œâ”€â”€ fingerprint.rs
    â”‚       â”œâ”€â”€ immutable.rs
    â”‚       â”œâ”€â”€ mod.rs
    â”‚       â”œâ”€â”€ retryable.rs
    â”‚       â””â”€â”€ yaml_ser.rs
    â”œâ”€â”€ .cargo/
    â”‚   â””â”€â”€ config.toml
    â””â”€â”€ .github/
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ ğŸ›-bug-report.md
        â”‚   â””â”€â”€ ğŸ’¡-feature-request.md
        â”œâ”€â”€ scripts/
        â”‚   â””â”€â”€ update_version.sh
        â””â”€â”€ workflows/
            â”œâ”€â”€ _doc_release.yml
            â”œâ”€â”€ _test.yml
            â”œâ”€â”€ CI.yml
            â”œâ”€â”€ docs.yml
            â”œâ”€â”€ format.yml
            â””â”€â”€ release.yml


Files Content:

(Files content cropped to 300k characters, download full ingest to see more)
================================================
FILE: README.md
================================================
<p align="center">
    <img src="https://cocoindex.io/images/github.svg" alt="CocoIndex">
</p>

<h1 align="center">Data transformation for AI</h1>

<div align="center">

[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)
[![Documentation](https://img.shields.io/badge/Documentation-394e79?logo=readthedocs&logoColor=00B9FF)](https://cocoindex.io/docs/getting_started/quickstart)
[![License](https://img.shields.io/badge/license-Apache%202.0-5B5BD6?logoColor=white)](https://opensource.org/licenses/Apache-2.0)
[![PyPI version](https://img.shields.io/pypi/v/cocoindex?color=5B5BD6)](https://pypi.org/project/cocoindex/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/cocoindex)](https://pypistats.org/packages/cocoindex)
[![CI](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml/badge.svg?event=push&color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/CI.yml)
[![release](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml/badge.svg?event=push&color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/release.yml)
[![Discord](https://img.shields.io/discord/1314801574169673738?logo=discord&color=5B5BD6&logoColor=white)](https://discord.com/invite/zpA9S2DR7s)

</div>

<div align="center">
    <a href="https://trendshift.io/repositories/13939" target="_blank"><img src="https://trendshift.io/api/badge/repositories/13939" alt="cocoindex-io%2Fcocoindex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>


Ultra performant data transformation framework for AI, with core engine written in Rust. Support incremental processing and data lineage out-of-box.  Exceptional developer velocity. Production-ready at day 0.

â­ Drop a star to help us grow!

<div align="center">

<!-- Keep these links. Translations will automatically update with the README. -->
[Deutsch](https://readme-i18n.com/cocoindex-io/cocoindex?lang=de) |
[English](https://readme-i18n.com/cocoindex-io/cocoindex?lang=en) |
[EspaÃ±ol](https://readme-i18n.com/cocoindex-io/cocoindex?lang=es) |
[franÃ§ais](https://readme-i18n.com/cocoindex-io/cocoindex?lang=fr) |
[æ—¥æœ¬èª](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ja) |
[í•œêµ­ì–´](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ko) |
[PortuguÃªs](https://readme-i18n.com/cocoindex-io/cocoindex?lang=pt) |
[Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://readme-i18n.com/cocoindex-io/cocoindex?lang=ru) |
[ä¸­æ–‡](https://readme-i18n.com/cocoindex-io/cocoindex?lang=zh)

</div>

</br>

<p align="center">
    <img src="https://cocoindex.io/images/transformation.svg" alt="CocoIndex Transformation">
</p>

</br>

CocoIndex makes it super easy to transform data with AI workloads, and keep source data and target in sync effortlessly.

</br>

<p align="center">
    <img src="https://cocoindex.io/images/venn-features.png" alt="CocoIndex Features" width='400'>
</p>

</br>

Either creating embedding, building knowledge graphs, or any data transformations - beyond traditional SQL.

## Exceptional velocity
Just declare transformation in dataflow with ~100 lines of python

```python
# import
data['content'] = flow_builder.add_source(...)

# transform
data['out'] = data['content']
    .transform(...)
    .transform(...)

# collect data
collector.collect(...)

# export to db, vector db, graph db ...
collector.export(...)
```

CocoIndex follows the idea of [Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box.

**Particularly**, developers don't explicitly mutate data by creating, updating and deleting. They just need to define transformation/formula for a set of source data.

## Build like LEGO
Native builtins for different source, targets and transformations. Standardize interface, make it 1-line code switch between different components.

<p align="center">
    <img src="https://cocoindex.io/images/components.svg" alt="CocoIndex Features">
</p>

## Data Freshness
CocoIndex keep source data and target in sync effortlessly.

<p align="center">
    <img src="https://github.com/user-attachments/assets/f4eb29b3-84ee-4fa0-a1e2-80eedeeabde6" alt="Incremental Processing" width="700">
</p>

It has out-of-box support for incremental indexing:
- minimal recomputation on source or logic change.
- (re-)processing necessary portions; reuse cache when possible

## Quick Start:
If you're new to CocoIndex, we recommend checking out
- ğŸ“– [Documentation](https://cocoindex.io/docs)
- âš¡  [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart)
- ğŸ¬ [Quick Start Video Tutorial](https://youtu.be/gv5R8nOXsWU?si=9ioeKYkMEnYevTXT)

### Setup

1. Install CocoIndex Python library

```bash
pip install -U cocoindex
```

2. [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one. CocoIndex uses it for incremental processing.


## Define data flow

Follow [Quick Start Guide](https://cocoindex.io/docs/getting_started/quickstart) to define your first indexing flow. An example flow looks like:

```python
@cocoindex.flow_def(name="TextEmbedding")
def text_embedding_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    # Add a data source to read files from a directory
    data_scope["documents"] = flow_builder.add_source(cocoindex.sources.LocalFile(path="markdown_files"))

    # Add a collector for data to be exported to the vector index
    doc_embeddings = data_scope.add_collector()

    # Transform data of each document
    with data_scope["documents"].row() as doc:
        # Split the document into chunks, put into `chunks` field
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown", chunk_size=2000, chunk_overlap=500)

        # Transform data of each chunk
        with doc["chunks"].row() as chunk:
            # Embed the chunk, put into `embedding` field
            chunk["embedding"] = chunk["text"].transform(
                cocoindex.functions.SentenceTransformerEmbed(
                    model="sentence-transformers/all-MiniLM-L6-v2"))

            # Collect the chunk into the collector.
            doc_embeddings.collect(filename=doc["filename"], location=chunk["location"],
                                   text=chunk["text"], embedding=chunk["embedding"])

    # Export collected data to a vector index.
    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
```

It defines an index flow like this:

<p align="center">
    <img width="400" alt="Data Flow" src="https://github.com/user-attachments/assets/2ea7be6d-3d94-42b1-b2bd-22515577e463" />
</p>

## ğŸš€ Examples and demo

| Example | Description |
|---------|-------------|
| [Text Embedding](examples/text_embedding) | Index text documents with embeddings for semantic search |
| [Code Embedding](examples/code_embedding) | Index code embeddings for semantic search |
| [PDF Embedding](examples/pdf_embedding) | Parse PDF and index text embeddings for semantic search |
| [Manuals LLM Extraction](examples/manuals_llm_extraction) | Extract structured information from a manual using LLM |
| [Amazon S3 Embedding](examples/amazon_s3_embedding) | Index text documents from Amazon S3 |
| [Azure Blob Storage Embedding](examples/azure_blob_embedding) | Index text documents from Azure Blob Storage |
| [Google Drive Text Embedding](examples/gdrive_text_embedding) | Index text documents from Google Drive |
| [Docs to Knowledge Graph](examples/docs_to_knowledge_graph) | Extract relationships from Markdown documents and build a knowledge graph |
| [Embeddings to Qdrant](examples/text_embedding_qdrant) | Index documents in a Qdrant collection for semantic search |
| [FastAPI Server with Docker](examples/fastapi_server_docker) | Run the semantic search server in a Dockerized FastAPI setup |
| [Product Recommendation](examples/product_recommendation) | Build real-time product recommendations with LLM and graph database|
| [Image Search with Vision API](examples/image_search) | Generates detailed captions for images using a vision model, embeds them, enables live-updating semantic search via FastAPI and served on a React frontend|
| [Face Recognition](examples/face_recognition) | Recognize faces in images and build embedding index |
| [Paper Metadata](examples/paper_metadata) | Index papers in PDF files, and build metadata tables for each paper |

More coming and stay tuned ğŸ‘€!

## ğŸ“– Documentation
For detailed documentation, visit [CocoIndex Documentation](https://cocoindex.io/docs), including a [Quickstart guide](https://cocoindex.io/docs/getting_started/quickstart).

## ğŸ¤ Contributing
We love contributions from our community â¤ï¸. For details on contributing or running the project for development, check out our [contributing guide](https://cocoindex.io/docs/about/contributing).

## ğŸ‘¥ Community
Welcome with a huge coconut hug ğŸ¥¥â‹†ï½¡ËšğŸ¤—. We are super excited for community contributions of all kinds - whether it's code improvements, documentation updates, issue reports, feature requests, and discussions in our Discord.

Join our community here:

- ğŸŒŸ [Star us on GitHub](https://github.com/cocoindex-io/cocoindex)
- ğŸ‘‹ [Join our Discord community](https://discord.com/invite/zpA9S2DR7s)
- â–¶ï¸ [Subscribe to our YouTube channel](https://www.youtube.com/@cocoindex-io)
- ğŸ“œ [Read our blog posts](https://cocoindex.io/blogs/)

## Support us:
We are constantly improving, and more features and examples are coming soon. If you love this project, please drop us a star â­ at GitHub repo [![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex) to stay tuned and help us grow.

## License
CocoIndex is Apache 2.0 licensed.



================================================
FILE: Cargo.toml
================================================
[package]
name = "cocoindex"
# Version used for local development is always higher than others to take precedence.
# Will be overridden for specific release versions.
version = "999.0.0"
edition = "2024"
rust-version = "1.88"

[profile.release]
codegen-units = 1
lto = true

[lib]
name = "cocoindex_engine"
crate-type = ["cdylib"]

[dependencies]
pyo3 = { version = "0.25.0", features = ["chrono", "auto-initialize", "uuid"] }
pythonize = "0.25.0"
pyo3-async-runtimes = { version = "0.25.0", features = ["tokio-runtime"] }

anyhow = { version = "1.0.97", features = ["std"] }
async-trait = "0.1.88"
axum = "0.8.4"
axum-extra = { version = "0.10.1", features = ["query"] }
base64 = "0.22.1"
chrono = "0.4.40"
config = "0.14.1"
const_format = "0.2.34"
futures = "0.3.31"
log = "0.4.27"
regex = "1.11.1"
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.140"
sqlx = { version = "0.8.3", features = [
    "chrono",
    "postgres",
    "runtime-tokio",
    "uuid",
] }
tokio = { version = "1.44.1", features = [
    "macros",
    "rt-multi-thread",
    "full",
    "tracing",
    "fs",
    "sync",
] }
tower = "0.5.2"
tower-http = { version = "0.6.2", features = ["cors", "trace"] }
indexmap = { version = "2.8.0", features = ["serde"] }
blake2 = "0.10.6"
pgvector = { version = "0.4.0", features = ["sqlx"] }
phf = { version = "0.11.3", features = ["macros"] }
indenter = "0.3.3"
itertools = "0.14.0"
derivative = "2.2.0"
hex = "0.4.3"
schemars = "0.8.22"
env_logger = "0.11.7"
reqwest = { version = "0.12.15", default-features = false, features = [
    "json",
    "rustls-tls",
] }
async-openai = "0.28.0"

tree-sitter = "0.25.3"
tree-sitter-language = "0.1.5"
# Per language tree-sitter parsers
tree-sitter-c = "0.23.4"
tree-sitter-cpp = "0.23.4"
tree-sitter-c-sharp = "0.23.1"
tree-sitter-css = "0.23.2"
tree-sitter-fortran = "0.5.1"
tree-sitter-go = "0.23.4"
tree-sitter-html = "0.23.2"
tree-sitter-java = "0.23.5"
tree-sitter-javascript = "0.23.1"
tree-sitter-json = "0.24.8"
# The other more popular crate tree-sitter-kotlin requires tree-sitter < 0.23 for now
tree-sitter-kotlin-ng = "1.1.0"
tree-sitter-md = "0.3.2"
tree-sitter-pascal = "0.10.0"
tree-sitter-php = "0.23.11"
tree-sitter-python = "0.23.6"
tree-sitter-r = "1.1.0"
tree-sitter-ruby = "0.23.1"
tree-sitter-rust = "0.23.2"
tree-sitter-scala = "0.23.4"
tree-sitter-sequel = "0.3.8"
tree-sitter-swift = "0.7.0"
tree-sitter-toml-ng = "0.7.0"
tree-sitter-typescript = "0.23.2"
tree-sitter-xml = "0.7.0"
tree-sitter-yaml = "0.7.0"

globset = "0.4.16"
unicase = "2.8.1"
google-drive3 = "6.0.0"
hyper-util = "0.1.11"
hyper-rustls = { version = "0.27.5" }
yup-oauth2 = "12.1.0"
rustls = { version = "0.23.25" }
http-body-util = "0.1.3"
yaml-rust2 = "0.10.1"
urlencoding = "2.1.3"
qdrant-client = "1.13.0"
uuid = { version = "1.16.0", features = ["serde", "v4", "v8"] }
tokio-stream = "0.1.17"
async-stream = "0.3.6"
neo4rs = "0.8.0"
bytes = "1.10.1"
rand = "0.9.0"
indoc = "2.0.6"
owo-colors = "4.2.0"
json5 = "0.4.1"
aws-config = "1.6.2"
aws-sdk-s3 = "1.85.0"
aws-sdk-sqs = "1.67.0"
time = { version = "0.3", features = ["macros", "serde"] }
numpy = "0.25.0"
infer = "0.19.0"
serde_with = { version = "3.13.0", features = ["base64"] }
google-cloud-aiplatform-v1 = { version = "0.4.0", default-features = false, features = [
    "prediction-service",
] }

azure_identity = { version = "0.21.0", default-features = false, features = [
    "enable_reqwest_rustls",
] }
azure_core = "0.21.0"
azure_storage = { version = "0.21.0", default-features = false, features = [
    "enable_reqwest_rustls",
    "hmac_rust",
] }
azure_storage_blobs = { version = "0.21.0", default-features = false, features = [
    "enable_reqwest_rustls",
    "hmac_rust",
] }



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
conduct@cocoindex.io.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.



================================================
FILE: CONTRIBUTING.md
================================================
We love contributions from our community â¤ï¸. Please check out our [contributing guide](https://cocoindex.io/docs/about/contributing).



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["maturin>=1.7.8,<2.0"]
build-backend = "maturin"

[project]
name = "cocoindex"
dynamic = ["version"]
description = "With CocoIndex, users declare the transformation, CocoIndex creates & maintains an index, and keeps the derived index up to date based on source update, with minimal computation and changes."
authors = [{ name = "CocoIndex", email = "cocoindex.io@gmail.com" }]
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "click>=8.1.8",
    "rich>=14.0.0",
    "python-dotenv>=1.1.0",
    "watchfiles>=1.1.0",
    "numpy>=1.23.2",
]
license = "Apache-2.0"
urls = { Homepage = "https://cocoindex.io/" }

[project.scripts]
cocoindex = "cocoindex.cli:cli"

[tool.maturin]
bindings = "pyo3"
python-source = "python"
module-name = "cocoindex._engine"
features = ["pyo3/extension-module"]

[project.optional-dependencies]
dev = ["pytest", "ruff", "mypy", "pre-commit"]
embeddings = ["sentence-transformers>=3.3.1"]
all = ["cocoindex[embeddings]"]

[tool.mypy]
python_version = "3.11"
strict = true
files = "python/cocoindex"
exclude = "(\\.venv|site-packages)"
disable_error_code = ["unused-ignore"]



================================================
FILE: ruff.toml
================================================
[format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"



================================================
FILE: .env.lib_debug
================================================
export RUST_LOG=warn,cocoindex_engine=trace,tower_http=trace
export RUST_BACKTRACE=1

export COCOINDEX_SERVER_CORS_ORIGINS=http://localhost:3000,https://cocoindex.io



================================================
FILE: .pre-commit-config.yaml
================================================
ci:
  autofix_prs: false
  autoupdate_schedule: 'monthly'

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-case-conflict
        # Check for files with names that would conflict on a case-insensitive
        # filesystem like MacOS HFS+ or Windows FAT.
      - id: check-merge-conflict
        # Check for files that contain merge conflict strings.
      - id: check-symlinks
        # Checks for symlinks which do not point to anything.
        exclude: ".*(.github.*)$"
      - id: detect-private-key
        # Checks for the existence of private keys.
      - id: end-of-file-fixer
        # Makes sure files end in a newline and only a newline.
        exclude: ".*(data.*|licenses.*|_static.*|\\.ya?ml|\\.jpe?g|\\.png|\\.svg|\\.webp)$"
      - id: trailing-whitespace
        # Trims trailing whitespace.
        exclude_types: [python]  # Covered by Ruff W291.
        exclude: ".*(data.*|licenses.*|_static.*|\\.ya?ml|\\.jpe?g|\\.png|\\.svg|\\.webp)$"

  - repo: local
    hooks:
        - id: maturin-develop
          name: maturin develop
          entry: maturin develop -E all,dev
          language: system
          files: ^(python/|src/|Cargo\.toml|pyproject\.toml)
          pass_filenames: false

        - id: cargo-fmt
          name: cargo fmt
          entry: cargo fmt
          language: system
          types: [rust]
          pass_filenames: false

        - id: cargo-test
          name: cargo test
          entry: cargo test
          language: system
          types: [rust]
          pass_filenames: false

        - id: mypy-check
          name: mypy type check
          entry: mypy
          language: system
          types: [python]
          pass_filenames: false

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.12.0
    hooks:
      - id: ruff-format
        types: [python]
        pass_filenames: true
  
  - repo: https://github.com/christophmeissner/pytest-pre-commit
    rev: 1.0.0
    hooks:
      - id: pytest
        language: system
        types: [python]
        pass_filenames: false
        always_run: false



================================================
FILE: dev/neo4j.yaml
================================================
name: cocoindex-neo4j
services:
  neo4j:
    image: neo4j:5-enterprise
    volumes:
        - /$HOME/neo4j/logs:/logs
        - /$HOME/neo4j/config:/config
        - /$HOME/neo4j/data:/data
        - /$HOME/neo4j/plugins:/plugins
    environment:
        - NEO4J_AUTH=neo4j/cocoindex
        - NEO4J_PLUGINS='["graph-data-science"]'
        - NEO4J_ACCEPT_LICENSE_AGREEMENT=eval

        # Uncomment to enable query logging
        # - NEO4J_db_logs_query_enabled=VERBOSE
        # - NEO4J_db_logs_query_transaction_enabled=VERBOSE
        # - NEO4J_db_logs_query_parameter__logging__enabled=true
        # - NEO4J_dbms_logs_http_enabled=true
        # - NEO4J_server_logs_debug_enabled=true

    ports:
      - "7474:7474"
      - "7687:7687"
    restart: always


================================================
FILE: dev/postgres.yaml
================================================
name: cocoindex-postgres
services:
  postgres:
    image: pgvector/pgvector:pg17
    restart: always
    environment:
      POSTGRES_PASSWORD: cocoindex
      POSTGRES_USER: cocoindex
      POSTGRES_DB: cocoindex
    ports:
      - 5432:5432



================================================
FILE: docs/README.md
================================================
<p align="center">
    <img src="https://cocoindex.io/images/github.svg" alt="CocoIndex">
</p>

<h2 align="center">ğŸ“– Documentation https://cocoindex.io/docs </h2>

<div align="center">

[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)
[![License](https://img.shields.io/badge/license-Apache%202.0-5B5BD6?logo=opensourceinitiative&logoColor=white)](https://opensource.org/licenses/Apache-2.0)
[![docs](https://github.com/cocoindex-io/cocoindex/actions/workflows/docs.yml/badge.svg?event=push&color=5B5BD6)](https://github.com/cocoindex-io/cocoindex/actions/workflows/docs.yml)

</div>



This directory is the source code for the CocoIndex documentation website, built using [Docusaurus](https://docusaurus.io/).

### Installation

```
$ yarn
```

### Local Development

```
$ yarn start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```
$ yarn build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```
$ USE_SSH=true yarn deploy
```

Not using SSH:

```
$ GIT_USER=<Your GitHub username> yarn deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.



================================================
FILE: docs/docusaurus.config.ts
================================================
import webpack from 'webpack';
import { themes as prismThemes } from 'prism-react-renderer';
import type { Config } from '@docusaurus/types';
import type * as Preset from '@docusaurus/preset-classic';

// This runs in Node.js - Don't use client-side code here (browser APIs, JSX...)

const config: Config = {
  title: 'CocoIndex',
  tagline: 'Indexing infra for AI with exceptional velocity',
  favicon: 'img/favicon.ico',

  // Set the production url of your site here
  url: 'https://cocoindex.io',
  // Set the /<baseUrl>/ pathname under which your site is served
  // For GitHub pages deployment, it is often '/<projectName>/'
  baseUrl: '/docs/',

  // GitHub pages deployment config.
  // If you aren't using GitHub pages, you don't need these.
  organizationName: 'cocoindex-io', // Usually your GitHub org/user name.
  projectName: 'docs', // Usually your repo name.
  trailingSlash: false,

  onBrokenLinks: 'throw',
  onBrokenMarkdownLinks: 'warn',

  // Even if you don't use internationalization, you can use this field to set
  // useful metadata like html lang. For example, if your site is Chinese, you
  // may want to replace "en" with "zh-Hans".
  i18n: {
    defaultLocale: 'en',
    locales: ['en'],
  },

  markdown: {
    mermaid: true,
  },

  plugins: [
    () => ({
      name: 'load-env-vars',
      configureWebpack: () => ({
        mergeStrategy: { plugins: "append", resolve: "merge" },
        plugins: [
          new webpack.DefinePlugin({
            'process.env.COCOINDEX_DOCS_MIXPANEL_API_KEY': JSON.stringify(process.env.COCOINDEX_DOCS_MIXPANEL_API_KEY),
          })
        ],
      }),
    }),
    [
      '@docusaurus/plugin-client-redirects',
      {
        redirects: [
          {
            from: '/core/initialization',
            to: '/core/settings',
          },
          {
            from: '/ops/storages',
            to: '/ops/targets',
          },
        ],
      },
    ],
  ],

  presets: [
    [
      'classic',
      {
        docs: {
          routeBasePath: '/',
          sidebarPath: './sidebars.ts',
          // Please change this to your repo.
          // Remove this to remove the "edit this page" links.
          editUrl: 'https://github.com/cocoindex-io/cocoindex/tree/main/docs',
        },
        blog: false,
        theme: {
          customCss: './src/css/custom.css',
        },
      } satisfies Preset.Options,
    ],
  ],

  themes: ['@docusaurus/theme-mermaid'],
  themeConfig: {
    // Replace with your project's social card
    image: 'img/social-card.jpg',
    metadata: [{ name: 'description', content: 'Official documentation for CocoIndex - Learn how to use CocoIndex to build robust data indexing pipelines for AI applications. Comprehensive guides, API references, and best practices for implementing efficient data processing workflows.' }],
    colorMode: {
      defaultMode: 'light',
      disableSwitch: false,
      respectPrefersColorScheme: true,
    },
    navbar: {
      title: 'CocoIndex',
      logo: {
        alt: 'CocoIndex Logo',
        src: 'img/icon.svg',
        href: 'https://cocoindex.io',
        target: '_self' // This makes the logo click follow the link in the same window
      },
      items: [
        { to: '/docs/', label: 'Documentation', position: 'left', target: '_self' },
        { to: 'https://cocoindex.io/blogs/', label: 'Blog', position: 'left', target: '_self' },
        {
          type: 'html',
          position: 'right',
          value: '<iframe src="https://ghbtns.com/github-btn.html?user=cocoindex-io&repo=cocoindex&type=star&count=true" frameborder="0" scrolling="0" width="120" height="20" title="GitHub" style="vertical-align: middle;"></iframe>',
          className: 'navbar-github-link',
        },
      ],
    },
    footer: {
      style: 'light',
      links: [
        {
          title: 'CocoIndex',
          items: [
            {
              label: 'support@cocoindex.io',
              href: 'mailto:support@cocoindex.io',
            },
          ],
        },
        {
          title: 'Resources',
          items: [
            {
              label: 'Blog',
              to: 'https://cocoindex.io/blogs',
              target: '_self',
            },
            {
              label: 'Documentation',
              to: 'https://cocoindex.io/docs',
              target: '_self',
            },
            {
              label: 'YouTube',
              href: 'https://www.youtube.com/@cocoindex-io',
            },
          ],
        },
        {
          title: 'Community',
          items: [
            {
              label: 'GitHub',
              href: 'https://github.com/cocoindex-io/cocoindex',
            },
            {
              label: 'Discord Community',
              href: 'https://discord.com/invite/zpA9S2DR7s',
            },
            {
              label: 'Twitter',
              href: 'https://x.com/cocoindex_io',
            },
            {
              label: 'LinkedIn',
              href: 'https://www.linkedin.com/company/cocoindex/about/',
            },
          ],
        },
      ],
      copyright: `Â© ${new Date().getFullYear()} CocoIndex. All rights reserved.`,
    },
    prism: {
      theme: prismThemes.github,
      darkTheme: prismThemes.dracula,
      additionalLanguages: ['diff', 'json', 'bash', 'docker'],
    },
  } satisfies Preset.ThemeConfig,
};


if (!!process.env.COCOINDEX_DOCS_POSTHOG_API_KEY) {
  config.plugins.push([
    "posthog-docusaurus",
    {
      apiKey: process.env.COCOINDEX_DOCS_POSTHOG_API_KEY,
      appUrl: "https://us.i.posthog.com",
      enableInDevelopment: false,
    },
  ]);
}


if (!!process.env.COCOINDEX_DOCS_ALGOLIA_API_KEY && !!process.env.COCOINDEX_DOCS_ALGOLIA_APP_ID) {
  config.themeConfig.algolia = {
    appId: process.env.COCOINDEX_DOCS_ALGOLIA_APP_ID,
    apiKey: process.env.COCOINDEX_DOCS_ALGOLIA_API_KEY,
    indexName: 'cocoindex',
    contextualSearch: true,
    searchPagePath: 'search',
  };
}

export default config;



================================================
FILE: docs/package.json
================================================
{
  "name": "cocoindex-docs",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "docusaurus start",
    "build": "docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids",
    "typecheck": "tsc"
  },
  "dependencies": {
    "@docusaurus/core": "^3.8.0",
    "@docusaurus/plugin-client-redirects": "^3.8.0",
    "@docusaurus/preset-classic": "^3.8.0",
    "@docusaurus/theme-mermaid": "^3.8.0",
    "@mdx-js/react": "^3.0.0",
    "clsx": "^2.0.0",
    "mixpanel-browser": "^2.59.0",
    "posthog-docusaurus": "^2.0.2",
    "prism-react-renderer": "^2.4.0",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "react-player": "^2.16.0"
  },
  "devDependencies": {
    "@docusaurus/module-type-aliases": "3.7.0",
    "@docusaurus/tsconfig": "3.7.0",
    "@docusaurus/types": "3.7.0",
    "typescript": "~5.8.2"
  },
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 3 chrome version",
      "last 3 firefox version",
      "last 5 safari version"
    ]
  },
  "engines": {
    "node": ">=18.0"
  },
  "packageManager": "yarn@1.22.22+sha512.a6b2f7906b721bba3d67d4aff083df04dad64c399707841b7acf00f6b133b7ac24255f2652fa22ae3534329dc6180534e98d17432037ff6fd140556e2bb3137e"
}



================================================
FILE: docs/sidebars.ts
================================================
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';

const sidebars: SidebarsConfig = {
  tutorialSidebar: [
    {
      type: 'category',
      label: 'Getting Started',
      collapsed: false,
      items: [
        'getting_started/overview',
        'getting_started/quickstart',
        'getting_started/installation',
      ],
    },
    {
      type: 'category',
      label: 'CocoIndex Core',
      collapsed: false,
      items: [
        'core/basics',
        'core/data_types',
        'core/flow_def',
        'core/settings',
        'core/flow_methods',
        'core/cli',
        'core/custom_function',
      ],
    },
    {
      type: 'category',
      label: 'Built-in Operations',
      collapsed: false,
      items: [
        'ops/sources',
        'ops/functions',
        'ops/targets',
      ],
    },
    {
      type: 'category',
      label: 'AI Support',
      collapsed: false,
      items: [
        'ai/llm',
      ],
    },
    {
      type: 'doc',
      id: 'query',
      label: 'Query Support',
    },
    {
      type: 'category',
      label: 'About',
      collapsed: false,
      items: [
        'about/community',
        'about/contributing',
      ],
    },
  ],
};

export default sidebars;



================================================
FILE: docs/tsconfig.json
================================================
{
  // This file is not used in compilation. It is here just for a nice editor experience.
  "extends": "@docusaurus/tsconfig",
  "compilerOptions": {
    "baseUrl": "."
  },
  "exclude": [".docusaurus", "build"]
}



================================================
FILE: docs/docs/query.mdx
================================================
---
title: Query Support
description: CocoIndex supports vector search and text search.
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# CocoIndex Query Support

The main functionality of CocoIndex is indexing.
The goal of indexing is to enable efficient querying against your data.
You can use any libraries or frameworks of your choice to perform queries.
At the same time, CocoIndex provides seamless integration between indexing and querying workflows.
For example, you can share transformations between indexing and querying, and easily retrieve table names when using CocoIndex's default naming conventions.

## Transform Flow

Sometimes a part of the transformation logic needs to be shared between indexing and querying,
e.g. when we build a vector index and query against it, the embedding computation needs to be consistent between indexing and querying.

In this case, you can:

1.  Extract a sub-flow with the shared transformation logic into a standalone function.
    *   It takes one or more data slices as input.
    *   It returns one data slice as output.
    *   You need to annotate data types for both inputs and outputs as type parameter for `cocoindex.DataSlice[T]`. See [data types](./core/data_types.mdx) for more details about supported data types.

2.  When you're defining your indexing flow, you can directly call the function.
    The body will be executed, so that the transformation logic will be added as part of the indexing flow.

3.  At query time, you usually want to directly run the function with specific input data, instead of letting it called as part of a long-lived indexing flow.
    To do this, declare the function as a *transform flow*, by decorating it with `@cocoindex.transform_flow()`.
    This will add `eval()` and `eval_async()` methods to the function, so that you can directly call with specific input data.


<Tabs>
<TabItem value="python" label="Python">

The [quickstart](getting_started/quickstart#step-41-extract-common-transformations) shows an example:

```python
@cocoindex.transform_flow()
def text_to_embedding(text: cocoindex.DataSlice[str]) -> cocoindex.DataSlice[NDArray[np.float32]]:
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"))
```

When you're defining your indexing flow, you can directly call the function:

```python
with doc["chunks"].row() as chunk:
    chunk["embedding"] = text_to_embedding(chunk["text"])
```

or, using the `call()` method of the transform flow on the first argument, to make operations chainable:

```python
with doc["chunks"].row() as chunk:
    chunk["embedding"] = chunk["text"].call(text_to_embedding)
```

Any time, you can call the `eval()` method with specific string, which will return a `NDArray[np.float32]`:

```python
print(text_to_embedding.eval("Hello, world!"))
```

If you're in an async context, please call the `eval_async()` method instead:

```python
print(await text_to_embedding.eval_async("Hello, world!"))
```

</TabItem>
</Tabs>

## Get Target Native Names

In your indexing flow, when you export data to a target, you can specify the target name (e.g. a database table name, a collection name, the node label in property graph databases, etc.) explicitly,
or for some backends you can also omit it and let CocoIndex generate a default name for you.
For the latter case, CocoIndex provides a utility function `cocoindex.utils.get_target_default_name()` to get the default name.
It takes the following arguments:

*   `flow` (type: `cocoindex.Flow`): The flow to get the default name for.
*   `target_name` (type: `str`): The export target name, appeared in the `export()` call.

For example:

<Tabs>
<TabItem value="python" label="Python">

```python
table_name = cocoindex.utils.get_target_default_name(text_embedding_flow, "doc_embeddings")
query = f"SELECT filename, text FROM {table_name} ORDER BY embedding <=> %s DESC LIMIT 5"
...
```

</TabItem>
</Tabs>



================================================
FILE: docs/docs/about/community.md
================================================
---
title: Community
description: Join the CocoIndex community
---

# Community

Welcome with a huge coconut hug ğŸ¥¥â‹†ï½¡ËšğŸ¤—.

We are super excited for community contributions of all kinds - whether it's code improvements, documentation updates, issue reports, feature requests on [GitHub](https://github.com/cocoindex-io/cocoindex), and discussions in our [Discord](https://discord.com/invite/zpA9S2DR7s).

We would love to foster an inclusive, welcoming, and supportive environment. Contributing to CocoIndex should feel collaborative, friendly and enjoyable for everyone. Together, we can build better AI applications through robust data infrastructure.

:::tip Start hacking CocoIndex
Check out our [Contributing guide](./contributing) to get started!
:::


## Connect With Us

Join our community channels to get help, share ideas, and connect with other developers:

- [Discord Community](https://discord.com/invite/zpA9S2DR7s) - Chat with the community and get real-time support
- [Twitter](https://x.com/cocoindex_io) - Follow us for updates and announcements
- [LinkedIn](https://www.linkedin.com/company/cocoindex/about/) - Connect professionally
- [YouTube](https://www.youtube.com/@cocoindex-io) - Watch tutorials and demos

## Get Support

Need help? Here are the best ways to get support:

- Email us at [hi@cocoindex.io](mailto:hi@cocoindex.io)
- Check our [documentation](https://cocoindex.io/docs)
- Read our [blog posts](https://cocoindex.io/blogs)
- Ask questions in our [Discord Server](https://discord.com/invite/zpA9S2DR7s)



================================================
FILE: docs/docs/about/contributing.md
================================================
---
title: Contributing
description: Learn how to contribute to CocoIndex
---

# Contributing

[CocoIndex](https://github.com/cocoindex-io/cocoindex) is an open source project. We are respectful, open and friendly. This guide explains how to get involved and contribute to [CocoIndex](https://github.com/cocoindex-io/cocoindex).

## Issues:

We use [GitHub Issues](https://github.com/cocoindex-io/cocoindex/issues) to track bugs and feature requests.

## Good First Issues

We tag issues with the ["good first issue"](https://github.com/cocoindex-io/cocoindex/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) label for beginner contributors.

## How to Contribute
- If you decide to work on an issue, unless the PR can be sent immediately (e.g. just a few lines of code), we recommend you to leave a comment on the issue like **`I'm working on it`**  or **`Can I work on this issue?`** to avoid duplicating work.
- For larger features, we recommend you to discuss with us first in our [Discord server](https://discord.com/invite/zpA9S2DR7s) to coordinate the design and work.
- Our [Discord server](https://discord.com/invite/zpA9S2DR7s) is constantly open. If you are unsure about anything, it is a good place to discuss! We'd love to collaborate and will always be friendly.

## Start hacking! Setting Up Development Environment
Follow the steps below to get cocoindex built on the latest codebase locally - if you are making changes to cocoindex functionality and want to test it out.

-   ğŸ¦€ [Install Rust](https://rust-lang.org/tools/install)

    If you don't have Rust installed, run
    ```sh
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
    ```
    Already have Rust? Make sure it's up to date
    ```sh
    rustup update
    ```

-   Setup Python virtual environment:
    ```sh
    python3 -m venv .venv
    ```
    Activate the virtual environment, before any installing / building / running:

    ```sh
    . .venv/bin/activate
    ```

-   Install required tools:
    ```sh
    pip install maturin
    ```

-   Build the library. Run at the root of cocoindex directory:
    ```sh
    maturin develop -E all,dev
    ```

-   Install and enable pre-commit hooks. This ensures all checks run automatically before each commit:
    ```sh
    pre-commit install
    ```

-   Before running a specific example, set extra environment variables, for exposing extra traces, allowing dev UI, etc.
    ```sh
    . ./.env.lib_debug
    ```

## Submit Your Code
CocoIndex is committed to the highest standards of code quality. Please ensure your code is thoroughly tested before submitting a PR.

To submit your code:

1. Fork the [CocoIndex repository](https://github.com/cocoindex-io/cocoindex)
2. [Create a new branch](https://docs.github.com/en/desktop/making-changes-in-a-branch/managing-branches-in-github-desktop) on your fork
3. Make your changes
4. Run the pre-commit checks (automatically triggered on `git commit`)

    :::tip
    To run them manually (same as CI):
        ```sh
        pre-commit run --all-files
        ```
    :::

5. [Open a Pull Request (PR)](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) when your work is ready for review

In your PR description, please include:
- Description of the changes
- Motivation and context
- Note if it's a breaking change
- Reference any related GitHub issues

A core team member will review your PR within one business day and provide feedback on any required changes. Once approved and all tests pass, the reviewer will squash and merge your PR into the main branch.

Your contribution will then be part of CocoIndex! We'll highlight your contribution in our release notes ğŸŒ´.



================================================
FILE: docs/docs/ai/llm.mdx
================================================
---
title: LLM Support
description: LLMs integrated with CocoIndex for various built-in functions
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

CocoIndex provides builtin functions integrating with various LLM APIs, for various inference tasks:
*   [Text Generation](#text-generation): use LLM to generate text.
*   [Text Embedding](#text-embedding): embed text into a vector space.

## LLM API Types

We support integrating with LLM with different types of APIs.
Each LLM API type is specified by a `cocoindex.LlmApiType` enum.

We support the following types of LLM APIs:

| API Name | `LlmApiType` enum | Text Generation | Text Embedding |
|----------|---------------------|--------------------|--------------------|
| [OpenAI](#openai) | `LlmApiType.OPENAI` | âœ… | âœ… |
| [Ollama](#ollama) | `LlmApiType.OLLAMA` | âœ… | âŒ |
| [Google Gemini](#google-gemini) | `LlmApiType.GEMINI` | âœ… | âœ… |
| [Vertex AI](#vertex-ai) | `LlmApiType.VERTEX_AI` | âœ… | âœ… |
| [Anthropic](#anthropic) | `LlmApiType.ANTHROPIC` | âœ… | âŒ |
| [Voyage](#voyage) | `LlmApiType.VOYAGE` | âŒ | âœ… |
| [LiteLLM](#litellm) | `LlmApiType.LITE_LLM` | âœ… | âŒ |
| [OpenRouter](#openrouter) | `LlmApiType.OPEN_ROUTER` | âœ… | âŒ |
| [vLLM](#vllm) | `LlmApiType.VLLM` | âœ… | âŒ |

## LLM Tasks

### Text Generation

Generation is used as a building block for certain CocoIndex functions that process data using LLM generation.

We have one builtin functions using LLM generation for now:

*  [`ExtractByLlm`](/docs/ops/functions#extractbyllm): it extracts information from input text.

#### LLM Spec

When calling a CocoIndex function that uses LLM generation, you need to provide a `cocoindex.LlmSpec` dataclass, to configure the LLM you want to use in these functions.
It has the following fields:

*   `api_type` (type: [`cocoindex.LlmApiType`](/docs/ai/llm#llm-api-types), required): The type of integrated LLM API to use, e.g. `cocoindex.LlmApiType.OPENAI` or `cocoindex.LlmApiType.OLLAMA`.
    See supported LLM APIs in the [LLM API integrations](#llm-api-integrations) section below.
*   `model` (type: `str`, required): The name of the LLM model to use.
*   `address` (type: `str`, optional): The address of the LLM API.
*   `api_config` (optional): Specific configuration for the LLM API. Only needed for specific LLM APIs (see below).


### Text Embedding

Embedding means converting text into a vector space, usually for similarity matching.

We provide a builtin function [`EmbedText`](/docs/ops/functions#embedtext) that converts a given text into a vector space.
The spec takes the following fields:

*   `api_type` (type: `cocoindex.LlmApiType`, required)
*   `model` (type: `str`, required)
*   `address` (type: `str`, optional)
*   `output_dimension` (type: `int`, optional)
*   `task_type` (type: `str`, optional)

See documentation for [`EmbedText`](/docs/ops/functions#embedtext) for more details about these fields.

## LLM API Integrations

CocoIndex integrates with various LLM APIs for these functions.

### OpenAI

To use the OpenAI LLM API, you need to set the environment variable `OPENAI_API_KEY`.
You can generate the API key from [OpenAI Dashboard](https://platform.openai.com/api-keys).

Currently we don't support custom address for OpenAI API.

You can find the full list of models supported by OpenAI [here](https://platform.openai.com/docs/models).

For text generation, a spec for OpenAI looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.OPENAI,
    model="gpt-4o",
)
```

</TabItem>
</Tabs>

For text embedding, a spec for OpenAI looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.functions.EmbedText(
    api_type=cocoindex.LlmApiType.OPENAI,
    model="text-embedding-3-small",
)
```

</TabItem>
</Tabs>

### Ollama

[Ollama](https://ollama.com/) allows you to run LLM models on your local machine easily. To get started:

*   [Download](https://ollama.com/download) and install Ollama.
*   Pull your favorite LLM models by the `ollama pull` command, e.g.
    ```bash
    ollama pull llama3.2
    ```
You can find the [list of models](https://ollama.com/library) supported by Ollama.

A spec for Ollama looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.OLLAMA,
    model="llama3.2:latest",
    # Optional, use Ollama's default port (11434) on localhost if not specified
    address="http://localhost:11434",
)
```

</TabItem>
</Tabs>

### Google Gemini

Google exposes Gemini through Google AI Studio APIs.
Based on [Gemini API recommendation](https://cloud.google.com/ai/gemini?hl=en), this is recommended for experimenting and prototyping purposes.
You may use [Vertex AI](#vertex-ai) for production usages.

To use the Gemini by Google AI Studio API, you need to set the environment variable `GEMINI_API_KEY`.
You can generate the API key from [Google AI Studio](https://aistudio.google.com/apikey).

You can find the full list of models supported by Gemini [here](https://ai.google.dev/gemini-api/docs/models).

For text generation, a spec looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.GEMINI,
    model="gemini-2.5-flash",
)
```

</TabItem>
</Tabs>

For text embedding, a spec looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.functions.EmbedText(
    api_type=cocoindex.LlmApiType.GEMINI,
    model="text-embedding-004",
    task_type="SEMANTICS_SIMILARITY",
)
```

</TabItem>
</Tabs>

All supported embedding models can be found [here](https://ai.google.dev/gemini-api/docs/embeddings#embeddings-models).
Gemini supports task type (optional), which can be found [here](https://ai.google.dev/gemini-api/docs/embeddings#supported-task-types).


### Vertex AI

Google Cloud Vertex AI offers production-level integration with Google Gemini models.

To use the Vertex AI API:

1.  Register / login in *Google Cloud*.
2.  In [Google Cloud Console](https://console.cloud.google.com/).
    -   Search for *Vertex AI API*. Enable this API.
    -   Search for *Billing*. Set the billing account for the current project.
3.  Setup [application default credentials (ADC)](https://cloud.google.com/docs/authentication/application-default-credentials).

    The easiest way during development is to [install the `gcloud` CLI](https://cloud.google.com/sdk/docs/install-sdk) and run

    ```sh
    gcloud auth application-default login
    ```

Spec for Vertex AI takes additional `api_config` field, in type `cocoindex.llm.VertexAiConfig` with the following fields:
-   `project` (type: `str`, required): The project ID of the Google Cloud project.
-   `region` (type: `str`, optional): The region of the Google Cloud project. Use `global` if not specified.

You can find the full list of models supported by Vertex AI [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions).

For text generation, a spec for Vertex AI looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.VERTEX_AI,
    model="gemini-2.0-flash",
    api_config=cocoindex.llm.VertexAiConfig(project="your-project-id"),
)
```

</TabItem>
</Tabs>


For text embedding, a spec for Vertex AI looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.functions.EmbedText(
    api_type=cocoindex.LlmApiType.VERTEX_AI,
    model="text-embedding-005",
    task_type="SEMANTICS_SIMILARITY",
    api_config=cocoindex.llm.VertexAiConfig(project="your-project-id"),
)
```

</TabItem>
</Tabs>

Vertex AI API supports task type (optional), which can be found [here](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#parameter-list).

### Anthropic

To use the Anthropic LLM API, you need to set the environment variable `ANTHROPIC_API_KEY`.
You can generate the API key from [Anthropic API](https://console.anthropic.com/settings/keys).

A text generation spec for Anthropic looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.ANTHROPIC,
    model="claude-3-5-sonnet-latest",
)
```

</TabItem>
</Tabs>

You can find the full list of models supported by Anthropic [here](https://docs.anthropic.com/en/docs/about-claude/models/all-models).

### Voyage

To use the Voyage LLM API, you need to set the environment variable `VOYAGE_API_KEY`.
You can generate the API key from [Voyage dashboard](https://dashboard.voyageai.com/organization/api-keys).

A text embedding spec for Voyage looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.functions.EmbedText(
    api_type=cocoindex.LlmApiType.VOYAGE,
    model="voyage-code-3",
    task_type="document",
)
```

</TabItem>
</Tabs>

Voyage API supports `document` and `query` as task types (optional, a.k.a. `input_type` in Voyage API, see [Voyage API documentation](https://docs.voyageai.com/reference/embeddings-api) for details).

### LiteLLM

To use the LiteLLM API, you need to set the environment variable `LITELLM_API_KEY`.

#### 1. Install LiteLLM Proxy

```bash
pip install 'litellm[proxy]'
```

#### 2. Create a `config.yml` for LiteLLM

**Example for DeepSeek:**

Use this in your `config.yml`:

```yaml
model_list:
  - model_name: deepseek-chat
    litellm_params:
        model: deepseek/deepseek-chat
        api_key: os.environ/DEEPSEEK_API_KEY
```

You need to set the environment variable `DEEPSEEK_API_KEY` to your DeepSeek API key.

**Example for Groq:**

Use this in your `config.yml`:

```yaml
model_list:
  - model_name: groq-llama-3.3-70b-versatile
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: "os.environ/GROQ_API_KEY"
```

You need to set the environment variable `GROQ_API_KEY` to your Groq API key.


#### 3. Run LiteLLM Proxy

```bash
litellm --config config.yml
```

#### 4. A Spec for LiteLLM will look like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.LITE_LLM,
    model="deepseek-chat",
    address="http://127.0.0.1:4000", # default url of LiteLLM
)
```

</TabItem>
</Tabs>

You can find the full list of models supported by LiteLLM [here](https://docs.litellm.ai/docs/providers).

### OpenRouter

To use the OpenRouter API, you need to set the environment variable `OPENROUTER_API_KEY`.
You can generate the API key from [here](https://openrouter.ai/settings/keys).

A spec for OpenRouter looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.OPEN_ROUTER,
    model="deepseek/deepseek-r1:free",
)
```

</TabItem>
</Tabs>

You can find the full list of models supported by OpenRouter [here](https://openrouter.ai/models).

### vLLM

Install vLLM:

```bash
pip install vllm
```

Run vLLM Server

```bash
vllm serve deepseek-ai/deepseek-coder-1.3b-instruct
```


A spec for vLLM looks like this:

<Tabs>
<TabItem value="python" label="Python" default>

```python
cocoindex.LlmSpec(
    api_type=cocoindex.LlmApiType.VLLM,
    model="deepseek-ai/deepseek-coder-1.3b-instruct",
    address="http://127.0.0.1:8000/v1",
)
```

</TabItem>
</Tabs>



================================================
FILE: docs/docs/core/basics.md
================================================
---
title: Indexing Basics
description: "CocoIndex basic concepts for indexing: indexing flow, data, operations, data updates, etc."
---

# CocoIndex Indexing Basics

An **index** is a collection of data stored in a way that is easy for retrieval.

CocoIndex is an ETL framework for building indexes from specified data sources, a.k.a. **indexing**. It also offers utilities for users to retrieve data from the indexes.

An **indexing flow** extracts data from specified data sources, upon specified transformations, and puts the transformed data into specified target for later retrieval.

## Indexing flow elements

An indexing flow has two aspects: data and operations on data.

### Data

An indexing flow involves source data and transformed data (either as an intermediate result or the final result to be put into targets). All data within the indexing flow has **schema** determined at flow definition time.

Each piece of data has a **data type**, falling into one of the following categories:

*   *Basic type*.
*   *Struct type*: a collection of **fields**, each with a name and a type.
*   *Table type*: a collection of **rows**, each of which is a struct with specified schema. A table type can be a *KTable* (which has a key field) or a *LTable* (ordered but without key field).

An indexing flow always has a top-level struct, containing all data within and managed by the flow.

See [Data Types](data_types) for more details about data types.

### Operations

An **operation** in an indexing flow defines a step in the flow. An operation is defined by:

*   **Action**, which defines the behavior of the operation, e.g. *import*, *transform*, *for each*, *collect* and *export*.
    See [Flow Definition](flow_def) for more details for each action.

*   Some actions (i.e. "import", "transform" and "export") require an **Operation Spec**, which describes the specific behavior of the operation, e.g. a source to import from, a function describing the transformation behavior, a target to export to (as an index).
    *   Each operation spec has a **operation type**, e.g. `LocalFile` (data source), `SplitRecursively` (function), `SentenceTransformerEmbed` (function), `Postgres` (target).
    *   CocoIndex framework maintains a set of supported operation types. Users can also implement their own.

"import" and "transform" operations produce output data, whose data type is determined based on the operation spec and data types of input data (for "transform" operation only).

## An indexing flow example

For the example shown in the [Quickstart](../getting_started/quickstart) section, the indexing flow is as follows:

![Flow Example](flow_example.svg)

This creates the following data for the indexing flow:

*   The `LocalFile` source creates a `documents` field at the top level, with `filename` (key) and `content` sub fields.
*   A "for each" action works on each document, with the following transformations:
    *   The `SplitRecursively` function splits content into chunks, adds a `chunks` field into the current scope (each document), with `location` (key) and `text` sub fields.
    *   A "collect" action works on each chunk, with the following transformations:
        *   The `SentenceTransformerEmbed` function embeds the chunk into a vector space, adding a `embedding` field into the current scope (each chunk).

This shows schema and example data for the indexing flow:

![Data Example](data_example.svg)

## Life cycle of an indexing flow

An indexing flow, once set up, maintains a long-lived relationship between data source and target. This means:

1.  The target created by the flow remain available for querying at any time

2.  As source data changes (new data added, existing data updated or deleted), data in the target are updated to reflect those changes,
    on certain pace, according to the update mode:

    *   **One time update**: Once triggered, CocoIndex updates the target data to reflect the version of source data up to the current moment.
    *   **Live update**: CocoIndex continuously reacts to changes of source data and updates the target data accordingly, based on various **change capture mechanisms** for the source.

    See more details in the [build / update target data](flow_methods#build--update-target-data) section.

3.  CocoIndex intelligently reprocesses to propagate source changes to target by:

    *   Determining which parts of the target data need to be recomputed
    *   Reusing existing computations where possible
    *   Only reprocessing the minimum necessary data

    This is known as **incremental processing**.

You can think of an indexing flow similar to formulas in a spreadsheet:

*   In a spreadsheet, you define formulas that transform input cells into output cells
*   When input values change, the spreadsheet recalculates affected outputs
*   You focus on defining the transformation logic, not managing updates

CocoIndex works the same way, but with more powerful capabilities:

* Instead of flat tables, CocoIndex models data in nested data structures, making it more natural to model complex data
* Instead of simple cell-level formulas, you have operations like "for each" to apply the same formula across rows without repeating yourself

This means when writing your flow operations, you can treat source data as if it were static - focusing purely on defining the transformation logic. CocoIndex takes care of maintaining the dynamic relationship between sources and target data behind the scenes.

## Internal storage

As an indexing flow is long-lived, it needs to store intermediate data to keep track of the states.
CocoIndex uses internal storage for this purpose.

Currently, CocoIndex uses Postgres database as the internal storage.
See [Settings](settings#databaseconnectionspec) for configuring its location. The internal storage is managed by CocoIndex, see [Setup / drop flow](/docs/core/flow_methods#setup--drop-flow) for more details.



================================================
FILE: docs/docs/core/cli.mdx
================================================
---
title: CLI
description: CocoIndex CLI
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# CocoIndex CLI

CocoIndex CLI is a standalone tool for easily managing and inspecting your flows and indexes.

## Invoke the CLI

Once CocoIndex is installed, you can invoke the CLI directly using the `cocoindex` command. Most commands require an `APP_TARGET` argument, which tells the CLI where your flow definitions are located.

### APP_TARGET Format

The `APP_TARGET` can be:
1.  A **path to a Python file** defining your flows (e.g., `main.py`, `path/to/my_flows.py`).
2.  An **installed Python module name** that contains your flow definitions (e.g., `my_package.flows`).
3.  For commands that operate on a *specific flow* (like `show`, `update`, `evaluate`), you can combine the application reference with a flow name:
    * `path/to/my_flows.py:MyFlow`
    * `my_package.flows:MyFlow`

### Environment Variables

Environment variables are needed as CocoIndex library settings, as described in [CocoIndex Settings](settings#list-of-environment-variables).

You can set environment variables in an environment file.

*   By default, the `cocoindex` CLI searches upward from the current directory for a `.env` file.
*   You can use `--env-file <path>` to specify one explicitly:

    ```sh
    cocoindex --env-file path/to/custom.env <COMMAND> ...
    ```

Loaded variables do *NOT* override existing system ones.
If no file is found, only existing system environment variables are used.

### Global Options

CocoIndex CLI supports the following global options:

* `--env-file <path>`: Load environment variables from a specified `.env` file. If not provided, `.env` in the current directory is loaded if it exists.
* `--version`: Show the CocoIndex version and exit.
* `--help`: Show the main help message and exit.

## Subcommands

The following subcommands are available:

| Subcommand | Description |
| ---------- | ----------- |
| `ls` | List all flows present in the given file/module. Or list all persisted flows under the current app namespace if no file/module specified. |
| `show` | Show the spec and schema for a specific flow. |
| `setup` | Check and apply backend setup changes for flows, including the internal storage and target (to export). |
| `drop` | Drop the backend setup for specified flows. |
| `update` | Update the index defined by the flow. |
| `evaluate` | Evaluate the flow and dump flow outputs to files.  Instead of updating the index, it dumps what should be indexed to files. Mainly used for evaluation purpose. |
| `server` | Start a HTTP server providing REST APIs. It will allow tools like CocoInsight to access the server. |

Use `--help` to see the full list of subcommands, and `subcommand --help` to see the usage of a specific one.

```sh
cocoindex --help       # Show all subcommands
cocoindex show --help  # Show usage of "show" subcommand
```



================================================
FILE: docs/docs/core/custom_function.mdx
================================================
---
title: Custom Functions
description: Build Custom Functions
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

A custom function can be defined in one of the following ways:

*   A standalone function. It's simpler and doesn't allow additional configurations and setup logic.
*   A function spec and an executor. It's more powerful, allows additional configurations and setup logic.

## Option 1: By a standalone function

It fits into simple cases that the function doesn't need to take additional configurations and extra setup logic.

<Tabs>
<TabItem value="python" label="Python" default>

The standalone function needs to be decorated by `@cocoindex.op.function()`, like this:

```python
@cocoindex.op.function(...)
def compute_something(arg1: str, arg2: int | None = None) -> str:
    """
    Documentation for the function.
    """
    ...
```

Notes:

*   The `cocoindex.op.function()` function decorator also takes optional parameters.
    See [Parameters for custom functions](#parameters-for-custom-functions) for details.
*   Types of arguments and the return value must be annotated, so that CocoIndex will have information about data types of the operation's output fields.
    See [Data Types](/docs/core/data_types) for supported types.

</TabItem>
</Tabs>

### Examples

The cocoindex repository contains the following examples of custom functions defined in this way:

*   In the [code_embedding](https://github.com/cocoindex-io/cocoindex/blob/main/examples/code_embedding/main.py) example,
    `extract_extension` is a custom function to extract the extension of a file name.
*   In the [manuals_llm_extraction](https://github.com/cocoindex-io/cocoindex/blob/main/examples/manuals_llm_extraction/main.py) example,
    `summarize_manuals` is a custom function to summarize structured information of a manual page.


## Option 2: By a function spec and an executor

This is more advanced and flexible way to define a custom function.
It allows a function to be configured with the function spec, and allow preparation logic before execution, e.g. initialize a model based on the spec.

### Function Spec

The function spec of a function configures behavior of a specific instance of the function.
When you use this function in a flow (typically by a [`transform()`](/docs/core/flow_def#transform)), you instantiate this function spec, with specific parameter values.

<Tabs>
<TabItem value="python" label="Python" default>

A function spec is defined as a class that inherits from `cocoindex.op.FunctionSpec`.

```python
class ComputeSomething(cocoindex.op.FunctionSpec):
    """
    Documentation for the function.
    """
    param1: str
    param2: int | None = None
    ...
```

Notes:
*   All fields of the spec must have a type serializable / deserializable by the `json` module.
*   All subclasses of `FunctionSpec` can be instantiated similar to a dataclass, i.e. `ClassName(param1=value1, param2=value2, ...)`.

</TabItem>
</Tabs>


### Function Executor

A function executor defines behavior of a function. It's instantiated for each operation that uses this function.

The function executor is responsible for:

*   *Prepare* for the function execution, based on the spec.
    It happens once and only once before execution.
    e.g. if the function calls a machine learning model, the model name can be a parameter as a field of the spec, and we may load the model in this phase.
*   *Run* the function, for each specific input arguments. This happens multiple times, for each specific row of data.

<Tabs>
<TabItem value="python" label="Python" default>

A function executor is defined as a class decorated by `@cocoindex.op.executor_class()`.


```python
@cocoindex.op.executor_class(...)
class ComputeSomethingExecutor:
    spec: ComputeSomething
    ...

    def prepare(self) -> None:
        ...

    def __call__(self, arg1: str, arg2: int | None = None) -> str:
        ...
```

Notes:

*   The `cocoindex.op.executor_class()` class decorator also takes optional parameters.
    See [Parameters for custom functions](#parameters-for-custom-functions) for details.

*   A `spec` field must be present in the class, and must be annotated with the spec class name.
*   The `prepare()` method is optional. It's executed once and only once before any `__call__` execution, to prepare the function execution.
*   The `__call__()` method is required. It's executed for each specific rows of data.
    Types of arugments and the return value must be decorated, so that CocoIndex will have information about data types of the operation's output fields.
    See [Data Types](/docs/core/data_types) for supported types.

</TabItem>
</Tabs>

### Examples

The cocoindex repository contains the following examples of custom functions defined in this way:

*   In the [pdf_embedding](https://github.com/cocoindex-io/cocoindex/blob/main/examples/pdf_embedding/main.py) example, we define a custom function `PdfToMarkdown`
*   The `SentenceTransformerEmbed` function shipped with the CocoIndex Python package is defined by Python SDK.
    Search for [`SentenceTransformerEmbedExecutor`](https://github.com/search?q=repo%3Acocoindex-io%2Fcocoindex+lang%3Apython+SentenceTransformerEmbedExecutor&type=code) to see the code.

## Parameters for custom functions

Custom functions take the following additional parameters:

*   `gpu: bool`: Whether the executor will use GPU. It will affect the way the function is scheduled.

*   `cache: bool`: Whether the executor will enable cache for this function.
     When `True`, the executor will cache the result of the function for reuse during reprocessing.
     We recommend to set this to `True` for any function that is computationally intensive.

*   `behavior_version: int`: The version of the behavior of the function.
    When the version is changed, the function will be re-executed even if cache is enabled.
    It's required to be set if `cache` is `True`.

*   `arg_relationship: tuple[ArgRelationship, str]`: It specifies the relationship between an input argument and the output,
    e.g. `(ArgRelationship.CHUNKS_BASE_TEXT, "content")` means the output is chunks for the text represented by the
    input argument with name `content`.
    This provides metadata for tools, e.g. CocoInsight.
    Currently the following attributes are supported:

    *   `ArgRelationship.CHUNKS_BASE_TEXT`:
        The output is chunks for the text represented by the input argument. In this case, the output is expected to be a *Table*, whose each row represents a text chunk, and the first column has type *Range*, representing the range of the text chunk.
    *   `ArgRelationship.EMBEDDING_ORIGIN_TEXT`: The output is embedding vector for the text represented by the input argument. The output is expected to be a *Vector*.
    *   `ArgRelationship.RECTS_BASE_IMAGE`: The output is rectangles for the image represented by the input argument. The output is expected to be a *Table*, whose each row represents a rectangle, and the first column has type *Struct*, with fields `min_x`, `min_y`, `max_x`, `max_y` to represent the coordinates of the rectangle.

For example:

<Tabs>
<TabItem value="python" label="Python" default>

This enables cache for a standalone function:

```python
@cocoindex.op.function(cache=True, behavior_version=1)
def compute_something(arg1: str, arg2: int | None = None) -> str:
    ...
```

This enables cache for a function defined by a spec and an executor:

```python
class ComputeSomething(cocoindex.op.FunctionSpec):
    ...

@cocoindex.op.executor_class(cache=True, behavior_version=1)
class ComputeSomethingExecutor:
    spec: ComputeSomething

    ...
```

</TabItem>
</Tabs>



================================================
FILE: docs/docs/core/data_types.mdx
================================================
---
title: Data Types
description: Data Types in CocoIndex
toc_max_heading_level: 4
---

# Data Types in CocoIndex

In CocoIndex, all data processed by the flow have a type determined when the flow is defined, before any actual data is processed at runtime.

This makes schema of data processed by CocoIndex clear, and easily determine the schema of your index.

## Data Types

As an engine written in Rust, designed to be used in different languages and data are always serializable, CocoIndex defines a type system independent of any specific programming language.

CocoIndex automatically infers data types of the output created by CocoIndex sources and functions.
You don't need to spell out any data type explicitly when you define the flow.
All you need to do is to make sure the data passed to functions and targets are compatible with them.

Each type in CocoIndex type system is mapped to one or multiple types in Python.
When you define a [custom function](/docs/core/custom_function), you need to annotate the data types of arguments and return values.

*   When you pass a Python value to the engine (e.g. return values of a custom function), a specific type annotation is required.
    The type annotation needs to be specific in describing the target data type, as it provides the ground truth of the data type in the flow.

*   When you use a Python variable to bind to an engine value (e.g. arguments of a custom function),
    the engine already knows the specific data type, so we don't require a specific type annotation, e.g. type annotations can be omitted, or you can use `Any` at any level.
    When a specific type annotation is provided, it's still used as a guidance to construct the Python value with compatible type.
    Otherwise, we will bind to a default Python type.

### Basic Types

#### Primitive Types

Primitive types are basic types that are not composed of other types.
This is the list of all primitive types supported by CocoIndex:

| CocoIndex Type | Python Types | Convertible to | Explanation |
|------|-------------|--------------|----------------|
| *Bytes* | `bytes` | | |
| *Str* | `str` | | |
| *Bool* | `bool` | | |
| *Int64* | `cocoindex.Int64`, `int`, `numpy.int64` | | |
| *Float32* | `cocoindex.Float32`, `numpy.float32` | *Float64* | |
| *Float64* |  `cocoindex.Float64`, `float`, `numpy.float64` | | |
| *Range* | `cocoindex.Range` | | |
| *Uuid* | `uuid.UUId` | | |
| *Date* | `datetime.date` | | |
| *Time* | `datetime.time` | | |
| *LocalDatetime* | `cocoindex.LocalDateTime` | *OffsetDatetime* | without timezone |
| *OffsetDatetime* | `cocoindex.OffsetDateTime`, `datetime.datetime` | | with timezone |
| *TimeDelta* | `datetime.timedelta` | | |

Notes:

*   For some CocoIndex types, we support multiple Python types. You can annotate with any of these Python types.
    The first one is the default type, i.e. CocoIndex will create a value with this type when a specific type annotation is not provided (e.g. for arguments of a custom function).

*   All Python types starting with `cocoindex.` are type aliases exported by CocoIndex. They're annotated types based on certain Python types:

    *   `cocoindex.Int64`: `int`
    *   `cocoindex.Float64`: `float`
    *   `cocoindex.Float32`: `float`
    *   `cocoindex.Range`: `tuple[int, int]`, i.e. a start offset (inclusive) and an end offset (exclusive)
    *   `cocoindex.OffsetDateTime`: `datetime.datetime`
    *   `cocoindex.LocalDateTime`: `datetime.datetime`

    These aliases provide a non-ambiguous way to represent a specific type in CocoIndex, given their base Python types can represent a superset of possible values.

*   When we say a CocoIndex type is *convertible to* another type, it means Python types for the second type can be also used to bind to a value of the first type.
    *   For example, *Float32* is convertible to *Float64*, so you can bind a value of *Float32* to a Python value of `float` or `np.float64` types.
    *   For *LocalDatetime*, when you use `cocoindex.OffsetDateTime` or `datetime.datetime` as the annotation to bind its value, the timezone will be set to UTC.


#### Json Type

*Json* type can hold any data convertible to JSON by `json` package.
In Python, it's represented by `cocoindex.Json`.
It's useful to hold data without fixed schema known at flow definition time.


#### Vector Types

A vector type is a collection of elements of the same basic type.
Optionally, it can have a fixed dimension. Noted as *Vector[Type]* or *Vector[Type, Dim]*, e.g. *Vector[Float32]* or *Vector[Float32, 384]*.

It supports the following Python types:

*   `cocoindex.Vector[T]` or `cocoindex.Vector[T, typing.Literal[Dim]]`, e.g. `cocoindex.Vector[cocoindex.Float32]` or `cocoindex.Vector[cocoindex.Float32, typing.Literal[384]]`
    *   The underlying Python type is `numpy.typing.NDArray[T]` where `T` is a numpy numeric type (`numpy.int64`, `numpy.float32` or `numpy.float64`), or `list[T]` otherwise
*   `numpy.typing.NDArray[T]` where `T` is a numpy numeric type
*   `list[T]`


#### Union Types

A union type is a type that can represent values in one of multiple basic types.
Noted as *Type1* | *Type2* | ..., e.g. *Int64* | *Float32* | *Float64*.

The Python type is `T1 | T2 | ...`, e.g. `cocoindex.Int64 | cocoindex.Float32 | cocoindex.Float64`, `int | float` (equivalent to `cocoindex.Int64 | cocoindex.Float64`)


### Struct Types

A *Struct* has a bunch of fields, each with a name and a type.

In Python, a *Struct* type is represented by either a [dataclass](https://docs.python.org/3/library/dataclasses.html)
or a [NamedTuple](https://docs.python.org/3/library/typing.html#typing.NamedTuple), with all fields annotated with a specific type.
Both options define a structured type with named fields, but they differ slightly:

- **Dataclass**: A flexible class-based structure, mutable by default, defined using the `@dataclass` decorator.
- **NamedTuple**: An immutable tuple-based structure, defined using `typing.NamedTuple`.

For example:

```python
from dataclasses import dataclass
from typing import NamedTuple
import datetime

# Using dataclass
@dataclass
class Person:
    first_name: str
    last_name: str
    dob: datetime.date

# Using NamedTuple
class PersonTuple(NamedTuple):
    first_name: str
    last_name: str
    dob: datetime.date
```

Both `Person` and `PersonTuple` are valid Struct types in CocoIndex, with identical schemas (three fields: `first_name` (Str), `last_name` (Str), `dob` (Date)).
Choose `dataclass` for mutable objects or when you need additional methods, and `NamedTuple` for immutable, lightweight structures.

Besides, for arguments of custom functions, CocoIndex also supports using dictionaries (`dict[str, Any]`) to represent a *Struct* type.
It's the default Python type if you don't annotate the function argument with a specific type.

### Table Types

A *Table* type models a collection of rows, each with multiple columns.
Each column of a table has a specific type.

We have two specific types of *Table* types: *KTable* and *LTable*.

#### KTable

*KTable* is a *Table* type whose first column serves as the key.
The row order of a *KTable* is not preserved.
Type of the first column (key column) must be a [key type](#key-types).

In Python, a *KTable* type is represented by `dict[K, V]`. 
The `K` should be the type binding to a key type,
and the `V` should be the type binding to a *Struct* type representing the value fields of each row.
When the specific type annotation is not provided,
the key type is bound to a tuple with its key parts when it's a *Struct* type, the value type is bound to `dict[str, Any]`.


For example, you can use `dict[str, Person]` or `dict[str, PersonTuple]` to represent a *KTable*, with 4 columns: key (*Str*), `first_name` (*Str*), `last_name` (*Str*), `dob` (*Date*).
It's bound to `dict[str, dict[str, Any]]` if you don't annotate the function argument with a specific type.

Note that if you want to use a *Struct* as the key, you need to ensure its value in Python is immutable. For `dataclass`, annotate it with `@dataclass(frozen=True)`. For `NamedTuple`, immutability is built-in. For example:

```python
@dataclass(frozen=True)
class PersonKey:
    id_kind: str
    id: str

class PersonKeyTuple(NamedTuple):
    id_kind: str
    id: str
```

Then you can use `dict[PersonKey, Person]` or `dict[PersonKeyTuple, PersonTuple]` to represent a KTable keyed by `PersonKey` or `PersonKeyTuple`.
It's bound to `dict[(str, str), dict[str, Any]]` if you don't annotate the function argument with a specific type.


#### LTable

*LTable* is a *Table* type whose row order is preserved. *LTable* has no key column.

In Python, a *LTable* type is represented by `list[R]`, where `R` is the type binding to the *Struct* type representing the value fields of each row.
For example, you can use `list[Person]` to represent a *LTable* with 3 columns: `first_name` (*Str*), `last_name` (*Str*), `dob` (*Date*).
It's bound to `list[dict[str, Any]]` if you don't annotate the function argument with a specific type.

## Key Types

Currently, the following types are key types

- *Bytes*
- *Str*
- *Bool*
- *Int64*
- *Range*
- *Uuid*
- *Date*
- *Struct* with all fields being key types (using `@dataclass(frozen=True)` or `NamedTuple`)



================================================
FILE: docs/docs/core/flow_def.mdx
================================================
---
title: Flow Definition
description: Define a CocoIndex flow, by specifying source, transformations and targets, and connect input/output data of them.
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# CocoIndex Flow Definition

In CocoIndex, to define an indexing flow, you provide a function to import source, transform data and put them into targets.
You connect input/output of these operations with fields of data scopes.

## Entry Point

A CocoIndex flow is defined by a function:

<Tabs>
<TabItem value="python" label="Python" default>

The easiest way is to use the `@cocoindex.flow_def` decorator:

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...
```

This `@cocoindex.flow_def` decorator declares this function as a CocoIndex flow definition.

It takes two arguments:

*   `flow_builder`: a `FlowBuilder` object to help build the flow.
*   `data_scope`: a `DataScope` object, representing the top-level data scope. Any data created by the flow should be added to it.

Alternatively, for more flexibility (e.g. you want to do this conditionally or generate dynamic name), you can explicitly call the `cocoindex.add_flow_def()` method:

```python
def demo_flow_def(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...

# Add the flow definition to the flow registry.
demo_flow = cocoindex.add_flow_def("DemoFlow", demo_flow_def)
```

In both cases, `demo_flow` will be an object with `cocoindex.Flow` class type.
See [Flow Running](/docs/core/flow_methods) for more details on it.

Sometimes you no longer want to keep states of the flow in memory. We provide a `cocoindex.remove_flow()` method for this purpose:

```python
cocoindex.remove_flow(demo_flow)
```

After it's called, `demo_flow` becomes an invalid object, and you should not call any methods of it.

:::note

This only removes states of the flow from the current process, and it won't affect the persistent states.
See [Setup / drop flow](/docs/core/flow_methods#setup--drop-flow) if you want to clean up the persistent states.

:::

</TabItem>
</Tabs>

## Data Scope

A **data scope** represents data for a certain unit, e.g. the top level scope (involving all data for a flow), for a document, or for a chunk.
A data scope has a bunch of fields and collectors, and users can add new fields and collectors to it.

### Get or Add a Field

You can get or add a field of a data scope (which is a data slice).

:::note

You cannot override an existing field.

:::

<Tabs>
<TabItem value="python" label="Python" default>

Getting and setting a field of a data scope is done by the `[]` operator with a field name:

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):

    # Add "documents" to the top-level data scope.
    data_scope["documents"] = flow_builder.add_source(DemoSourceSpec(...))

    # Each row of "documents" is a child scope.
    with data_scope["documents"].row() as document:

        # Get "content" from the document scope, transform, and add "summary" to scope.
        document["summary"] = field1_row["content"].transform(DemoFunctionSpec(...))
```

</TabItem>
</Tabs>

### Add a collector

See [Data Collector](#data-collector) below for more details.

## Data Slice

A **data slice** references a subset of data belonging to a data scope, e.g. a specific field from a data scope.
A data slice has a certain data type, and it's the input for most operations.

### Import from source

To get the initial data slice, we need to start from importing data from a source.
`FlowBuilder` provides a `add_source()` method to import data from external sources.
A *source spec* needs to be provided for any import operation, to describe the source and parameters related to the source.
Import must happen at the top level, and the field created by import must be in the top-level struct.

<Tabs>
<TabItem value="python" label="Python" default>

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    data_scope["documents"] = flow_builder.add_source(DemoSourceSpec(...))
    ......
```

</TabItem>
</Tabs>

:::note

The actual value of data is not available at the time when we define the flow: it's only available at runtime.
In a flow definition, you can use a data representation as input for operations, but not access the actual value.

:::

#### Refresh interval

You can provide a `refresh_interval` argument.
When present, in the [live update mode](/docs/core/flow_methods#live-update), the data source will be refreshed by specified interval.

<Tabs>
<TabItem value="python" label="Python" default>

The `refresh_interval` argument is of type `datetime.timedelta`. For example, this refreshes the data source every 1 minute:

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    data_scope["documents"] = flow_builder.add_source(
        DemoSourceSpec(...), refresh_interval=datetime.timedelta(minutes=1))
    ......
```

</TabItem>
</Tabs>

:::info

In live update mode, for each refresh, CocoIndex will list rows in the data source to figure out the changes based on metadata such as last modified time,
and only perform transformations on changed source keys.
If nothing changed during the last refresh cycle, only list operations will be performed, which is usually cheap for most data sources.

:::

### Transform

`transform()` method transforms the data slice by a function, which creates another data slice.
A *function spec* needs to be provided for any transform operation, to describe the function and parameters related to the function.

The function takes one or multiple data arguments.
The first argument is the data slice to be transformed, and the `transform()` method is applied from it.
Other arguments can be passed in as positional arguments or keyword arguments, after the function spec.

<Tabs>
<TabItem value="python" label="Python" default>

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...
    data_scope["field2"] = data_scope["field1"].transform(
                               DemoFunctionSpec(...),
                               arg1, arg2, ..., key0=kwarg0, key1=kwarg1, ...)
    ...
```

</TabItem>
</Tabs>

### For-each-row

If the data slice has [table type](/docs/core/data_types#table-types), you can call `row()` method to obtain a child scope representing each row, to apply operations on each row.

<Tabs>
<TabItem value="python" label="Python" default>

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
  ...
  with data_scope["table1"].row() as table1_row:
    # Children operations
    table1_row["field2"] = table1_row["field1"].transform(DemoFunctionSpec(...))
```

</TabItem>
</Tabs>


### Get a sub field

If the data slice has `Struct` type, you can obtain a data slice on a specific sub field of it, similar to getting a field of a data scope.

## Data Collector

A **data collector** can be added from a specific data scope, and it collects multiple entries of data from the same or children scope.

### Collect

Call its `collect()` method to collect a specific entry, which can have multiple fields.
Each field has a name as specified by the argument name, and a value in one of the following representations:

*   A `DataSlice`.
*   An enum `cocoindex.GeneratedField.UUID` indicating its value is an UUID automatically generated by the engine.
    The uuid will remain stable when other collected input values are unchanged.

    :::note

    An automatically generated UUID field is allowed to appear at most once.

    :::

For example,

<Tabs>
<TabItem value="python" label="Python" default>

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...
    demo_collector = data_scope.add_collector()
    with data_scope["documents"].row() as document:
        ...
        demo_collector.collect(id=cocoindex.GeneratedField.UUID,
                               filename=document["filename"],
                               summary=document["summary"])
    ...
```

</TabItem>
</Tabs>

Here the collector is in the top-level data scope.
It collects `filename` and `summary` fields from each row of `documents`,
and generates a `id` field with UUID and remains stable when `filename` and `summary` are unchanged.

### Export

The `export()` method exports the collected data to an external target.

A *target spec* needs to be provided for any export operation, to describe the target and parameters related to the target.

Export must happen at the top level of a flow, i.e. not within any child scopes created by "for each row". It takes the following arguments:

*   `name`: the name to identify the export target.
*   `target_spec`: the target spec as the export target.
*   `setup_by_user` (optional):
     whether the export target is setup by user.
     By default, CocoIndex is managing the target setup (see [Setup / drop flow](/docs/core/flow_methods#setup--drop-flow)), e.g. create related tables/collections/etc. with compatible schema, and update them upon change.
     If `True`, the export target will be managed by users, and users are responsible for creating the target and updating it upon change.
*   Fields to configure [storage indexes](#storage-indexes). `primary_key_fields` is required, and all others are optional.

<Tabs>
<TabItem value="python" label="Python" default>

```python
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...
    demo_collector = data_scope.add_collector()
    ...
    demo_collector.export(
        "demo_target", DemoTargetSpec(...),
        primary_key_fields=["field1"],
        vector_indexes=[cocoindex.VectorIndexDef("field2", cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
```

</TabItem>
</Tabs>

The target is managed by CocoIndex, i.e. it'll be created or dropped when [setup / drop flow](/docs/core/flow_methods#setup--drop-flow), and the data will be automatically updated (including stale data removal) when updating the index.
The `name` for the same target should remain stable across different runs.
If it changes, CocoIndex will treat it as an old target removed and a new one created, and perform setup changes and reindexing accordingly.

## Storage Indexes

Many targets are storage systems supporting indexes, to boost efficiency in retrieving data.
CocoIndex provides a common way to configure indexes for various targets.

### Primary Key

*Primary key* is specified by `primary_key_fields` (`Sequence[str]`).
Types of the fields must be key types. See [Key Types](data_types#key-types) for more details.

### Vector Index

*Vector index* is specified by `vector_indexes` (`Sequence[VectorIndexDef]`). `VectorIndexDef` has the following fields:

    *   `field_name`: the field to create vector index.
    *   `metric`: the similarity metric to use.

#### Similarity Metrics

Following metrics are supported:

| Metric Name | Description | Similarity Order |
|-------------|-------------|------------------|
| CosineSimilarity | [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | Larger is more similar |
| L2Distance | [L2 distance (a.k.a. Euclidean distance)](https://en.wikipedia.org/wiki/Euclidean_distance) | Smaller is more similar |
| InnerProduct | [Inner product](https://en.wikipedia.org/wiki/Inner_product_space) | Larger is more similar |

## Miscellaneous

### Getting App Namespace

You can use the [`app_namespace` setting](settings#app-namespace) or `COCOINDEX_APP_NAMESPACE` environment variable to specify the app namespace,
to organize flows across different environments (e.g., dev, staging, production), team members, etc.

In the code, You can call `flow.get_app_namespace()` to get the app namespace, and use it to name certain backends. It takes the following arguments:

*   `trailing_delimiter` (optional): a string to append to the app namespace when it's not empty.

e.g. when the current app namespace is `Staging`, `flow.get_app_namespace(trailing_delimiter='.')` will return `Staging.`.

For example,

<Tabs>
<TabItem value="python" label="Python" default>

```python
doc_embeddings.export(
    "doc_embeddings",
    cocoindex.targets.Qdrant(
        collection_name=cocoindex.get_app_namespace(trailing_delimiter='__') + "doc_embeddings",
        ...
    ),
    ...
)
```

</TabItem>
</Tabs>

It will use `Staging__doc_embeddings` as the collection name if the current app namespace is `Staging`, and use `doc_embeddings` if the app namespace is empty.

### Control Processing Concurrency

You can control the concurrency of the processing by setting the following options:

*   `max_inflight_rows`: the maximum number of concurrent inflight requests for the processing.
*   `max_inflight_bytes`: the maximum number of concurrent inflight bytes for the processing.

These options can be passed in to the following APIs:

*   [`FlowBuilder.add_source()`](#import-from-source): The options above control the processing concurrency of multiple rows from a source. New rows will not be loaded in memory if it'll be over the limit.

    Besides, global limits on overall processing concurrency of all sources from all flows can be specified by [`GlobalExecutionOptions`](/docs/core/settings#globalexecutionoptions) or corresponding [environment variables](/docs/core/settings#list-of-environment-variables).
    If both global and per-source limits are specified, both need to be satisfied to admit additional source rows.

*   [`DataSlice.row()`](#for-each-row):  The options above provides a finer-grained control, to limit the processing concurrency of multiple rows within a table at any level.

`max_inflight_bytes` only counts the number of bytes already existing in the current row before any further processing.

For example:

```py
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
  data_scope["documents"] = flow_builder.add_source(
      DemoSourceSpec(...), max_inflight_rows=10, max_inflight_bytes=100*1024*1024)

  with data_scope["documents"].row() as doc:
    doc["chunks"] = doc["content"].transform(SplitRecursively(...))
    with doc["chunks"].row(max_inflight_rows=100) as chunk:
      ......
```

### Target Declarations

Most time a target is created by calling `export()` method on a collector, and this `export()` call comes with configurations needed for the target, e.g. options for storage indexes.
Occasionally, you may need to specify some configurations for the target out of the context of any specific data collector.

For example, for graph database targets like `Neo4j` and `Kuzu`, you may have a data collector to export data to relationships, which will create nodes referenced by various relationships in turn.
These nodes don't directly come from any specific data collector (consider relationships from different data collectors may share the same nodes).
To specify configurations for these nodes, you can *declare* spec for related node labels.

`FlowBuilder` provides `declare()` method for this purpose, which takes the spec to declare, as provided by various target types.

<Tabs>
<TabItem value="python" label="Python" default>

```python
flow_builder.declare(
    cocoindex.targets.Neo4jDeclarations(...)
)
```

</TabItem>
</Tabs>

### Auth Registry

CocoIndex manages an auth registry. It's an in-memory key-value store, mainly to store authentication information for a backend.
It's usually used for targets, where key stability is important for backend cleanup.

Operation spec is the default way to configure sources, functions and targets. But it has the following limitations:

*   The spec isn't supposed to contain secret information, and it's frequently shown in various places, e.g. `cocoindex show`.
*   For targets, once an operation is removed after flow definition code change, the spec is also gone.
    But we still need to be able to drop the persistent backend (e.g. a table) when [setup / drop flow](/docs/core/flow_methods#setup--drop-flow).

Auth registry is introduced to solve the problems above.


#### Auth Entry

An auth entry is an entry in the auth registry with an explicit key.

*   You can create new *auth entry* by a key and a value.
*   You can reference the entry by the key, and pass it as part of spec for certain operations. e.g. `Neo4j` takes `connection` field in the form of auth entry reference.

<Tabs>
<TabItem value="python" label="Python" default>

You can add an auth entry by `cocoindex.add_auth_entry()` function, which returns a `cocoindex.AuthEntryReference[T]`:

```python
my_graph_conn = cocoindex.add_auth_entry(
    "my_graph_conn",
    cocoindex.targets.Neo4jConnectionSpec(
            uri="bolt://localhost:7687",
            user="neo4j",
            password="cocoindex",
    ))
```

Then reference it when building a spec that takes an auth entry:

*   You can either reference by the `AuthEntryReference[T]` object directly:

    ```python
    demo_collector.export(
        "MyGraph",
        cocoindex.targets.Neo4jRelationship(connection=my_graph_conn, ...)
    )
    ```

*   You can also reference it by the key string, using `cocoindex.ref_auth_entry()` function:

    ```python
    demo_collector.export(
        "MyGraph",
        cocoindex.targets.Neo4jRelationship(connection=cocoindex.ref_auth_entry("my_graph_conn"), ...))
    ```

</TabItem>
</Tabs>

Note that CocoIndex backends use the key of an auth entry to identify the backend.

*   Keep the key stable.
    If the key doesn't change, it's considered to be the same backend (even if the underlying way to connect/authenticate changes).

*   If a key is no longer referenced in any operation spec, keep it until the next flow setup / drop action,
    so that CocoIndex will be able to clean up the backends.

#### Transient Auth Entry

A transient auth entry is an entry in the auth registry with an automatically generated key.
It's usually used for sources and functions, where key stability is not important.

<Tabs>
<TabItem value="python" label="Python" default>

You can create a new *transient auth entry* by `cocoindex.add_transient_auth_entry()` function, which returns a `cocoindex.TransientAuthEntryReference[T]`, and pass it to a source or function spec that takes it, e.g.

```python
flow_builder.add_source(
    cocoindex.sources.AzureBlob(
        ...
        sas_token=cocoindex.add_transient_auth_entry("...")
    )
)
```


</TabItem>
</Tabs>

Whenever a `TransientAuthEntryReference[T]` is expected, you can also pass a `AuthEntryReference[T]` instead, as `AuthEntryReference[T]` is a subtype of `TransientAuthEntryReference[T]`.



================================================
FILE: docs/docs/core/flow_methods.mdx
================================================
---
title: Run a Flow
toc_max_heading_level: 4
description: Run a CocoIndex Flow, including build / update data in the target and evaluate the flow without changing the target.
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Run a CocoIndex Flow

After a flow is defined as discussed in [Flow Definition](/docs/core/flow_def), you can start to transform data with it.

It can be achieved in two ways:

*   Use [CocoIndex CLI](/docs/core/cli).

*   Use APIs provided by the library.
    You have a `cocoindex.Flow` object after defining the flow in your code, and you can interact with it later.

The following sections assume you have a flow `demo_flow`:

<Tabs>
<TabItem value="python" label="Python" default>

```python title="main.py"
@cocoindex.flow_def(name="DemoFlow")
def demo_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    ...
```

It creates a `demo_flow` object in `cocoindex.Flow` type.

</TabItem>
</Tabs>

## Setup / drop flow

For a flow, its persistent backends need to be ready before it can run, including:

*   [Internal storage](/docs/core/basics#internal-storage) for CocoIndex.
*   Backend entities for targets exported by the flow, e.g. a table (in relational databases), a collection (in some vector databases), etc.

The desired state of the backends for a flow is derived based on the flow definition itself.
CocoIndex supports two types of actions to manage the persistent backends automatically:

*   *Setup* a flow, which will change the backends owned by the flow to the desired state, e.g. create new tables for new flow, drop an existing table if the corresponding target is gone, add new column to a target table if a new field is collected, etc. It's no-op if the backend states are already in the desired state.

*   *Drop* a flow, which will drop all backends owned by the flow. It's no-op if there are no existing backends owned by the flow (e.g. never setup or already dropped).

### CLI

`cocoindex setup` subcommand will setup all flows.
`cocoindex update` and `cocoindex server` also provide a `--setup` option to setup the flow if needed before performing the main action of updating or starting the server.

`cocoindex drop` subcommand will drop all flows.

### Library API

<Tabs>
<TabItem value="python" label="Python">

`Flow` provides the following APIs to setup / drop individual flows:

*   `setup(report_to_stdout: bool = False)`: Setup the flow.
*   `drop(report_to_stdout: bool = False)`: Drop the flow.

For example:

```python
demo_flow.setup(report_to_stdout=True)
demo_flow.drop(report_to_stdout=True)
```

We also provide the following asynchronous versions of the APIs:

*   `setup_async(report_to_stdout: bool = False)`: Setup the flow asynchronously.
*   `drop_async(report_to_stdout: bool = False)`: Drop the flow asynchronously.

For example:

```python
await demo_flow.setup_async(report_to_stdout=True)
await demo_flow.drop_async(report_to_stdout=True)
```


Besides, CocoIndex also provides APIs to setup / drop all flows at once:

*   `setup_all_flows(report_to_stdout: bool = False)`: Setup all flows.
*   `drop_all_flows(report_to_stdout: bool = False)`: Drop all flows.

For example:

```python
cocoindex.setup_all_flows(report_to_stdout=True)
cocoindex.drop_all_flows(report_to_stdout=True)
```

</TabItem>
</Tabs>

:::note

After dropping the flow, the in-memory `cocoindex.Flow` instance is still valid, and you can call setup methods on it again.

If you want to remove the flow from the current process, you can call `cocoindex.remove_flow(demo_flow)` to do so (see [related doc](/docs/core/flow_def#entry-point)).

:::

## Build / update target data

The major goal of a flow is to perform the transformations on source data and build / update data in the target.
This action has two modes:

*   **One time update.**
    It builds/update the target data based on source data up to the current moment.
    After the target data is at least as fresh as the source data when update starts, it's done.
    It fits into situations that you need to access the fresh target data at certain time points.

*   **Live update.**
    It continuously captures changes from the source data and updates the target data accordingly.
    It's long-running and only stops when being aborted explicitly.
    It fits into situations that you need to access the fresh target data continuously in most of the time.

:::info

For both modes, CocoIndex is performing *incremental processing*,
i.e. we only perform computations and target mutations on source data that are changed, or the flow has changed.
This is to achieve best efficiency.

:::


### One time update

#### CLI

The `cocoindex update` subcommand creates/updates data in the target.

Once it's done, the target data is fresh up to the moment when the command is called.

```sh
cocoindex update main.py
```

With a `--setup` option, it will also setup the flow first if needed.

```sh
cocoindex update --setup main.py
```

#### Library API

<Tabs>
<TabItem value="python" label="Python">

The `Flow.update()` method creates/updates data in the target.


```python
stats = demo_flow.update()
print(stats)
```

`update_async()` is the asynchronous version of `update()`.

```python
stats = await demo_flow.update_async()
print(stats)
```

Once the function finishes, the target data is fresh up to the moment when the function is called.

:::info

`update()` and `update_async()` can be called simultaneously, even if a previous call is not finished yet.
It's quite cheap to do so, as CocoIndex will automatically combine multiple calls into a single update, as long as we hold the guarantee that the target data is fresh up to the moment when the last call is initiated.

:::


</TabItem>
</Tabs>

### Live update

A data source may enable one or multiple *change capture mechanisms*:

*   Configured with a [refresh interval](flow_def#refresh-interval), which is generally applicable to all data sources.

*   Specific data sources also provide their specific change capture mechanisms.
    For example, [`AmazonS3` source](../ops/sources/#amazons3) watches S3 bucket's change events, and [`GoogleDrive` source](../ops/sources#googledrive) allows polling recent modified files.
    See documentations for specific data sources.

Change capture mechanisms enable CocoIndex to continuously capture changes from the source data and update the target data accordingly, under live update mode.

#### CLI

To perform live update, run the `cocoindex update` subcommand with `-L` option:

```sh
cocoindex update main.py -L
```

If there's at least one data source with change capture mechanism enabled, it will keep running until aborted (e.g. by `Ctrl-C`).
Otherwise, it falls back to the same behavior as one time update, and will finish after a one-time update is done.

With a `--setup` option, it will also setup the flow first if needed.

```sh
cocoindex update main.py -L --setup
```

#### Library API

<Tabs>
<TabItem value="python" label="Python">

To perform live update, you need to create a `cocoindex.FlowLiveUpdater` object using the `cocoindex.Flow` object.
It takes an optional `cocoindex.FlowLiveUpdaterOptions` option, with the following fields:

*   `live_mode` (type: `bool`, default: `True`):
     Whether to perform live update for data sources with change capture mechanisms.
     It has no effect for data sources without any change capture mechanism.

*   `print_stats` (type: `bool`, default: `False`): Whether to print stats during update.

Note that `cocoindex.FlowLiveUpdater` provides a unified interface for both one-time update and live update.
It only performs live update when `live_mode` is `True`, and only for sources with change capture mechanisms enabled.
If a source has multiple change capture mechanisms enabled, all will take effect to trigger updates.

This creates a `cocoindex.FlowLiveUpdater` object, with an optional `cocoindex.FlowLiveUpdaterOptions` option:

```python
my_updater = cocoindex.FlowLiveUpdater(
    demo_flow, cocoindex.FlowLiveUpdaterOptions(print_stats=True))
```

A `FlowLiveUpdater` object supports the following methods:

*   `start()`: Start the updater.
    CocoIndex will continuously capture changes from the source data and update the target data accordingly in background threads managed by the engine.

*   `abort()`: Abort the updater.

*   `wait()`: Wait for the updater to finish. It only unblocks in one of the following cases:
    *   The updater was aborted.
    *   A one time update is done, and live update is not enabled:
        either `live_mode` is `False`, or all data sources have no change capture mechanisms enabled.

*   `next_status_updates()`: Get the next status updates.
    It blocks until there's a new status updates, including the processing finishes for a bunch of source updates, and live updater stops (aborted, or no more sources to process).
    You can continuously call this method in a loop to get the latest status updates and react accordingly.

    It returns a `cocoindex.FlowUpdaterStatusUpdates` object, with the following properties:
    *   `active_sources`: Names of sources that are still active, i.e. not stopped processing. If it's empty, it means the updater is stopped.
    *   `updated_sources`: Names of sources with updates since last time.
        You can check this to see which sources have recent updates and get processed.

*   `update_stats()`: It returns the stats of the updater.

This snippets shows the lifecycle of a live updater:

```python
my_updater = cocoindex.FlowLiveUpdater(demo_flow)
# Start the updater.
my_updater.start()

# Perform your own logic (e.g. a query loop).
...

...
# Wait for the updater to finish.
my_updater.wait()
# Print the update stats.
print(my_updater.update_stats())
```

Somewhere (in the same or other threads) you can also continuously call `next_status_updates()` to get the latest status updates and react accordingly, e.g.

```python
while True:
    updates = my_updater.next_status_updates()

    for source in updates.updated_sources:
        # Perform downstream operations on the target of the source.
        run_your_downstream_operations_for(source)

    # Break the loop if there's no more active sources.
    if not updates.active_sources:
        break
```

:::info

`next_status_updates()` automatically combines multiple status updates if more than one arrives between two calls,
e.g. your downstream operations may take more time, or you don't need to process too frequently (in which case you can explicitly sleep for a while).

So you don't need to worry about the status updates piling up.

:::

Python SDK also allows you to use the updater as a context manager.
It will automatically start the updater during the context entry, and abort and wait for the updater to finish automatically when the context is exited.
The following code is equivalent to the code above (if no early return happens):

```python
with cocoindex.FlowLiveUpdater(demo_flow) as my_updater:
    # Perform your own logic (e.g. a query loop).
    ...
```

CocoIndex also provides asynchronous versions of APIs for blocking operations, including:

*   `start_async()` and `wait_async()`, e.g.

    ```python
    my_updater = cocoindex.FlowLiveUpdater(demo_flow)
    # Start the updater.
    await my_updater.start_async()

    # Perform your own logic (e.g. a query loop).
    ...

    # Wait for the updater to finish.
    await my_updater.wait_async()
    # Print the update stats.
    print(my_updater.update_stats())
    ```

*   `next_status_updates_async()`, e.g.

    ```python
    while True:
        updates = await my_updater.next_status_updates_async()

        ...
    ```

*   Async context manager, e.g.

    ```python
    async with cocoindex.FlowLiveUpdater(demo_flow) as my_updater:
        # Perform your own logic (e.g. a query loop).
        ...
    ```

</TabItem>
</Tabs>

## Evaluate the flow

CocoIndex allows you to run the transformations defined by the flow without updating the target.

### CLI

The `cocoindex evaluate` subcommand runs the transformation and dumps flow outputs.
It takes the following options:

*   `--output-dir` (optional): The directory to dump the result to. If not provided, it will use `eval_{flow_name}_{timestamp}`.
*   `--no-cache` (optional): By default, we use already-cached intermediate data if available.
    This flag will turn it off.
    Note that we only read existing cached data without updating the cache, even if it's turned on.

Example:

```sh
cocoindex evaluate main.py --output-dir ./eval_output
```

### Library API

<Tabs>
<TabItem value="python" label="Python">

The `evaluate_and_dump()` method runs the transformation and dumps flow outputs to files.

It takes a `EvaluateAndDumpOptions` dataclass as input to configure, with the following fields:

*   `output_dir` (type: `str`, required): The directory to dump the result to.
*   `use_cache` (type: `bool`, default: `True`): Use already-cached intermediate data if available.
    Note that we only read existing cached data without updating the cache, even if it's turned on.

Example:

```python
demo_flow.evaluate_and_dump(EvaluateAndDumpOptions(output_dir="./eval_output"))
```

</TabItem>
</Tabs>



================================================
FILE: docs/docs/core/settings.mdx
================================================
---
title: CocoIndex Settings
description: Provide settings for CocoIndex, e.g. database connection, app namespace, etc.
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# CocoIndex Setting

Certain settings need to be provided for CocoIndex to work, e.g. database connections, app namespace, etc.

## Launch CocoIndex

You have two ways to launch CocoIndex:

*   Use [Cocoindex CLI](cli). It's handy for most routine indexing building and management tasks.
    It will load settings from environment variables, either already set in your environment, or specified in `.env` file.
    See [CLI](cli#environment-variables) for more details.

*   Call CocoIndex functionality from your own Python application or library.
    It's needed when you want to leverage CocoIndex support for query, or have your custom logic to trigger indexing, etc.

    <Tabs>
    <TabItem value="python" label="Python" default>

    You need to explicitly call `cocoindex.init()` before doing anything with CocoIndex, and settings will be loaded during the call.

    *   If it's called without any argument, it will load settings from environment variables.
        Only existing environment variables already set in your environment will be used.
        If you want to load environment variables from a specific `.env` file, consider call `load_dotenv()` provided by the [`python-dotenv`](https://github.com/theskumar/python-dotenv) package.

        ```py
        from dotenv import load_dotenv
        import cocoindex

        load_dotenv()
        cocoindex.init()
        ```

    *   It takes an optional `cocoindex.Settings` dataclass object as argument, so you can also construct settings explicitly and pass to it:

        ```py
        import cocoindex

        cocoindex.init(
            cocoindex.Settings(
                database=cocoindex.DatabaseConnectionSpec(
                    url="postgres://cocoindex:cocoindex@localhost/cocoindex"
                )
            )
        )
        ```
    </TabItem>
    </Tabs>

## List of Settings

`cocoindex.Settings` is a dataclass that contains the following fields:

*   `app_namespace` (type: `str`, required): The namespace of the application.
*   `database` (type: `DatabaseConnectionSpec`, required): The connection to the Postgres database.
*   `global_execution_options` (type: `GlobalExecutionOptions`, optional): The global execution options shared by all  flows.

### App Namespace

The `app_namespace` field helps organize flows across different environments (e.g., dev, staging, production), team members, etc. When set, it prefixes flow names with the namespace.

For example, if the namespace is `Staging`, for a flow with name specified as `Flow1` in code, the full name of the flow will be `Staging.Flow1`.
You can also get the current app namespace by calling `cocoindex.get_app_namespace()` (see [Getting App Namespace](flow_def#getting-app-namespace) for more details).

If not set, all flows are in a default unnamed namespace.

*Environment variable*: `COCOINDEX_APP_NAMESPACE`

### DatabaseConnectionSpec

`DatabaseConnectionSpec` configures the connection to a database. Only Postgres is supported for now. It has the following fields:

*   `url` (type: `str`, required): The URL of the Postgres database to use as the internal storage, e.g. `postgres://cocoindex:cocoindex@localhost/cocoindex`.

    *Environment variable* for `Settings.database.url`: `COCOINDEX_DATABASE_URL`

*   `user` (type: `str`, optional): The username for the Postgres database. If not provided, username will come from `url`.

    *Environment variable* for `Settings.database.user`: `COCOINDEX_DATABASE_USER`

*   `password` (type: `str`, optional): The password for the Postgres database. If not provided, password will come from `url`.

    *Environment variable* for `Settings.database.password`: `COCOINDEX_DATABASE_PASSWORD`

:::tip

Please be careful that all values in `url` needs to be url-encoded if they contain special characters.
For this reason, prefer to use the separated `user` and `password` fields for username and password.

:::

:::info

If you use the Postgres database hosted by [Supabase](https://supabase.com/), please click **Connect** on your project dashboard and find the following URL:

*    If you're on a IPv6 network, use the URL under **Direct connection**. You can visit [IPv6 test](https://test-ipv6.com/) to see if you have IPv6 Internet connection.
*    Otherwise, use the URL under **Session pooler**.

:::

### GlobalExecutionOptions

`GlobalExecutionOptions` is used to configure the global execution options shared by all flows. It has the following fields:

*   `source_max_inflight_rows` (type: `int`, optional): The maximum number of concurrent inflight requests for all source operations.
*   `source_max_inflight_bytes` (type: `int`, optional): The maximum number of concurrent inflight bytes for all source operations.

See also [flow definition docs](/docs/core/flow_def#control-processing-concurrency) to control processing concurrency on per-source basis.
If both global and per-source limits are specified, both need to be satisfied to admit additional source rows.

## List of Environment Variables

This is the list of environment variables, each of which has a corresponding field in `Settings`:

| environment variable | corresponding field in `Settings` | required? |
|---------------------|-------------------|----------|
| `COCOINDEX_APP_NAMESPACE` | `app_namespace` | No |
| `COCOINDEX_DATABASE_URL` | `database.url` | Yes |
| `COCOINDEX_DATABASE_USER` | `database.user` | No |
| `COCOINDEX_DATABASE_PASSWORD` | `database.password` | No |
| `COCOINDEX_SOURCE_MAX_INFLIGHT_ROWS` | `global_execution_options.source_max_inflight_rows` | No |
| `COCOINDEX_SOURCE_MAX_INFLIGHT_BYTES` | `global_execution_options.source_max_inflight_bytes` | No |



================================================
FILE: docs/docs/getting_started/installation.md
================================================
---
title: Installation
description: Setup the CocoIndex environment in 0-3 min
---

## ğŸ Python and Pip
To follow the steps in this guide, you'll need:

1. Install [Python](https://wiki.python.org/moin/BeginnersGuide/Download/). We support Python 3.11 to 3.13.
2. Install [pip](https://pip.pypa.io/en/stable/installation/) - a Python package installer


## ğŸŒ´ Install CocoIndex
```bash
pip install -U cocoindex
```

## ğŸ“¦ Install Postgres

You can skip this step if you already have a Postgres database with pgvector extension installed.

If you don't have a Postgres database:

1. Install [Docker Compose](https://docs.docker.com/compose/install/) ğŸ³.
2. Start a Postgres SQL database for cocoindex using our docker compose config:

```bash
docker compose -f <(curl -L https://raw.githubusercontent.com/cocoindex-io/cocoindex/refs/heads/main/dev/postgres.yaml) up -d
```

## ğŸ‰ All set!

You can now start using CocoIndex.



================================================
FILE: docs/docs/getting_started/markdown_files.zip
================================================
[Binary file]


================================================
FILE: docs/docs/getting_started/overview.md
================================================
---
title: Overview
slug: /
---

# Welcome to CocoIndex

CocoIndex is an ultra-performant real-time data transformation framework for AI, with incremental processing.

As a data framework, CocoIndex takes it to the next level on data freshness. **Incremental processing** is one of the core values provided by CocoIndex.

![Incremental Processing](/img/incremental-etl.gif)

## Programming Model
CocoIndex follows the idea of [Dataflow programming](https://en.wikipedia.org/wiki/Dataflow_programming) model. Each transformation creates a new field solely based on input fields, without hidden states and value mutation. All data before/after each transformation is observable, with lineage out of the box.

The gist of an example data transformation:
```python
# import
data['content'] = flow_builder.add_source(...)

# transform
data['out'] = data['content']
    .transform(...)
    .transform(...)

# collect data
collector.collect(...)

# export to db, vector db, graph db ...
collector.export(...)
```

Get Started:
- [Quick Start](https://cocoindex.io/docs/getting_started/quickstart)



================================================
FILE: docs/docs/getting_started/quickstart.md
================================================
---
title: Quickstart
description: Get started with CocoIndex in 10 minutes
---

import ReactPlayer from 'react-player'

# Build your first CocoIndex project

This guide will help you get up and running with CocoIndex in just a few minutes. We'll build a project that does:
*   Read files from a directory
*   Perform basic chunking and embedding
*   Load the data into a vector store (PG Vector)

<ReactPlayer controls url='https://www.youtube.com/watch?v=gv5R8nOXsWU' />

## Prerequisite: Install CocoIndex environment

We'll need to install a bunch of dependencies for this project.

1.  Install CocoIndex:

    ```bash
    pip install -U 'cocoindex[embeddings]'
    ```

2.  You can skip this step if you already have a Postgres database with pgvector extension installed.
    If not, the easiest way is to bring up a Postgres database using docker compose:

    - Make sure Docker Compose is installed: [docs](https://docs.docker.com/compose/install/)
    - Start a Postgres SQL database for cocoindex using our docker compose config:

    ```bash
    docker compose -f <(curl -L https://raw.githubusercontent.com/cocoindex-io/cocoindex/refs/heads/main/dev/postgres.yaml) up -d
    ```

## Step 1: Prepare directory for your project

1.  Open the terminal and create a new directory for your project:

    ```bash
    mkdir cocoindex-quickstart
    cd cocoindex-quickstart
    ```

2.  Prepare input files for the index. Put them in a directory, e.g. `markdown_files`.
    If you don't have any files at hand, you may download the example [markdown_files.zip](markdown_files.zip) and unzip it in the current directory.

## Step 2: Define the indexing flow

Create a new file `quickstart.py` and import the `cocoindex` library:

```python title="quickstart.py"
import cocoindex
```

Then we'll create the indexing flow as follows.

```python title="quickstart.py"
@cocoindex.flow_def(name="TextEmbedding")
def text_embedding_flow(flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope):
    # Add a data source to read files from a directory
    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.LocalFile(path="markdown_files"))

    # Add a collector for data to be exported to the vector index
    doc_embeddings = data_scope.add_collector()

    # Transform data of each document
    with data_scope["documents"].row() as doc:
        # Split the document into chunks, put into `chunks` field
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown", chunk_size=2000, chunk_overlap=500)

        # Transform data of each chunk
        with doc["chunks"].row() as chunk:
            # Embed the chunk, put into `embedding` field
            chunk["embedding"] = chunk["text"].transform(
                cocoindex.functions.SentenceTransformerEmbed(
                    model="sentence-transformers/all-MiniLM-L6-v2"))

            # Collect the chunk into the collector.
            doc_embeddings.collect(filename=doc["filename"], location=chunk["location"],
                                   text=chunk["text"], embedding=chunk["embedding"])

    # Export collected data to a vector index.
    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY)])
```

Notes:

1.  The `@cocoindex.flow_def` declares a function to be a CocoIndex flow.

2.  In CocoIndex, data is organized in different *data scopes*.
    *   `data_scope`, representing all data.
    *   `doc`, representing each row of `documents`.
    *   `chunk`, representing each row of `chunks`.

3.  A *data source* extracts data from an external source.
    In this example, the `LocalFile` data source imports local files as a KTable (table with a key field, see [KTable](../core/data_types#ktable) for details), each row has `"filename"` and `"content"` fields.

4. After defining the KTable, we extend a new field `"chunks"` to each row by *transforming* the `"content"` field using `SplitRecursively`. The output of the `SplitRecursively` is also a KTable representing each chunk of the document, with `"location"` and `"text"` fields.

5. After defining the KTable, we extend a new field `"embedding"` to each row by *transforming* the `"text"` field using `SentenceTransformerEmbed`.

6. In CocoIndex, a *collector* collects multiple entries of data together. In this example, the `doc_embeddings` collector collects data from all `chunk`s across all `doc`s, and uses the collected data to build a vector index `"doc_embeddings"`, using `Postgres`.

## Step 3: Run the indexing pipeline and queries

Specify the database URL by environment variable:

```bash
export COCOINDEX_DATABASE_URL="postgresql://cocoindex:cocoindex@localhost:5432/cocoindex"
```

Now we're ready to build the index:

```bash
cocoindex update --setup quickstart.py
```

If you run it the first time for this flow, CocoIndex will automatically create its persistent backends (tables in the database).
CocoIndex will ask you to confirm the action, enter `yes` to proceed.

CocoIndex will run for a few seconds and populate the target table with data as declared by the flow. It will output the following statistics:

```
documents: 3 added, 0 removed, 0 updated
```

## Step 4 (optional): Run queries against the index

CocoIndex excels at transforming your data and storing it (a.k.a. indexing).
The goal of transforming your data is usually to query against it.
Once you already have your index built, you can directly access the transformed data in the target database.
CocoIndex also provides utilities for you to do this more seamlessly.

In this example, we'll use the [`psycopg` library](https://www.psycopg.org/) along with pgvector to connect to the database and run queries on vector data.
Please make sure the required packages are installed:

```bash
pip install numpy "psycopg[binary,pool]" pgvector
```

### Step 4.1: Extract common transformations

Between your indexing flow and the query logic, one piece of transformation is shared: compute the embedding of a text.
i.e. they should use exactly the same embedding model and parameters.

Let's extract that into a function:

```python title="quickstart.py"
from numpy.typing import NDArray
import numpy as np

@cocoindex.transform_flow()
def text_to_embedding(text: cocoindex.DataSlice[str]) -> cocoindex.DataSlice[NDArray[np.float32]]:
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"))
```

`cocoindex.DataSlice[str]` represents certain data in the flow (e.g. a field in a data scope), with type `str` at runtime.
Similar to the `text_embedding_flow()` above, the `text_to_embedding()` is also to constructing the flow instead of directly doing computation,
so the type it takes is `cocoindex.DataSlice[str]` instead of `str`.
See [Data Slice](../core/flow_def#data-slice) for more details.


Then the corresponding code in the indexing flow can be simplified by calling this function:

```python title="quickstart.py"
...
# Transform data of each chunk
with doc["chunks"].row() as chunk:
    # Embed the chunk, put into `embedding` field
    chunk["embedding"] = text_to_embedding(chunk["text"])

    # Collect the chunk into the collector.
    doc_embeddings.collect(filename=doc["filename"], location=chunk["location"],
                            text=chunk["text"], embedding=chunk["embedding"])
...
```

The function decorator `@cocoindex.transform_flow()` is used to declare a function as a CocoIndex transform flow,
i.e., a sub flow only performing transformations, without importing data from sources or exporting data to targets.
The decorator is needed for evaluating the flow with specific input data in Step 4.2 below.

### Step 4.2: Provide the query logic

Now we can create a function to query the index upon a given input query:

```python title="quickstart.py"
from psycopg_pool import ConnectionPool
from pgvector.psycopg import register_vector

def search(pool: ConnectionPool, query: str, top_k: int = 5):
    # Get the table name, for the export target in the text_embedding_flow above.
    table_name = cocoindex.utils.get_target_default_name(text_embedding_flow, "doc_embeddings")
    # Evaluate the transform flow defined above with the input query, to get the embedding.
    query_vector = text_to_embedding.eval(query)
    # Run the query and get the results.
    with pool.connection() as conn:
        register_vector(conn)
        with conn.cursor() as cur:
            cur.execute(f"""
                SELECT filename, text, embedding <=> %s AS distance
                FROM {table_name} ORDER BY distance LIMIT %s
            """, (query_vector, top_k))
            return [
                {"filename": row[0], "text": row[1], "score": 1.0 - row[2]}
                for row in cur.fetchall()
            ]
```

In the function above, most parts are standard query logic - you can use any libraries you like.
There're two CocoIndex-specific logic:

1.  Get the table name from the export target in the `text_embedding_flow` above.
    Since the table name for the `Postgres` target is not explicitly specified in the `export()` call,
    CocoIndex uses a default name.
    `cocoindex.utils.get_target_default_name()` is a utility function to get the default table name for this case.

2.  Evaluate the transform flow defined above with the input query, to get the embedding.
    It's done by the `eval()` method of the transform flow `text_to_embedding`.
    The return type of this method is `NDArray[np.float32]` as declared in the `text_to_embedding()` function (`cocoindex.DataSlice[NDArray[np.float32]]`).

### Step 4.3: Add the main script logic

Now we can add the main logic to the program. It uses the query function we just defined:

```python title="quickstart.py"
if __name__ == "__main__":
    # Initialize CocoIndex library states
    cocoindex.init()

    # Initialize the database connection pool.
    pool = ConnectionPool(os.getenv("COCOINDEX_DATABASE_URL"))
    # Run queries in a loop to demonstrate the query capabilities.
    while True:
        try:
            query = input("Enter search query (or Enter to quit): ")
            if query == '':
                break
            # Run the query function with the database connection pool and the query.
            results = search(pool, query)
            print("\nSearch results:")
            for result in results:
                print(f"[{result['score']:.3f}] {result['filename']}")
                print(f"    {result['text']}")
                print("---")
            print()
        except KeyboardInterrupt:
            break
```

It interacts with users and search the database by calling the `search()` method created in Step 4.2.

### Step 4.4: Run queries against the index

Now we can run the same Python file, which will run the new added main logic:

```bash
python quickstart.py
```

It will ask you to enter a query and it will return the top 5 results.

## Next Steps

Next, you may want to:

*   Learn about [CocoIndex Basics](../core/basics.md).
*   Learn about other examples in the [examples](https://github.com/cocoindex-io/cocoindex/tree/main/examples) directory.
    *    The `text_embedding` example is this quickstart.
    *    Pick other examples to learn upon your interest.



================================================
FILE: docs/docs/ops/functions.md
================================================
---
title: Functions
description: CocoIndex Built-in Functions
---

# CocoIndex Built-in Functions

## ParseJson

`ParseJson` parses a given text to JSON.

The spec takes the following fields:

*   `text` (`str`): The source text to parse.
*   `language` (`str`, optional): The language of the source text.  Only `json` is supported now.  Default to `json`.

Return: *Json*

## SplitRecursively

`SplitRecursively` splits a document into chunks of a given size.
It tries to split at higher-level boundaries. If each chunk is still too large, it tries at the next level of boundaries.
For example, for a Markdown file, it identifies boundaries in this order: level-1 sections, level-2 sections, level-3 sections, paragraphs, sentences, etc.

The spec takes the following fields:

*   `custom_languages` (`list[CustomLanguageSpec]`, optional): This allows you to customize the way to chunking specific languages using regular expressions. Each `CustomLanguageSpec` is a dict with the following fields:
    *   `language_name` (`str`): Name of the language.
    *   `aliases` (`list[str]`, optional): A list of aliases for the language.
        It's an error if any language name or alias is duplicated.

    *   `separators_regex` (`list[str]`): A list of regex patterns to split the text.
        Higher-level boundaries should come first, and lower-level should be listed later. e.g. `[r"\n# ", r"\n## ", r"\n\n", r"\. "]`.
        See [regex syntax](https://docs.rs/regex/latest/regex/#syntax) for supported regular expression syntax.

Input data:

*   `text` (*Str*): The text to split.
*   `chunk_size` (*Int64*): The maximum size of each chunk, in bytes.
*   `min_chunk_size` (*Int64*, optional): The minimum size of each chunk, in bytes. If not provided, default to `chunk_size / 2`.

    :::note

    `SplitRecursively` will do its best to make the output chunks sized between `min_chunk_size` and `chunk_size`.
    However, it's possible that some chunks are smaller than `min_chunk_size` or larger than `chunk_size` in rare cases, e.g. too short input text, or non-splittable large text.

    Please avoid setting `min_chunk_size` to a value too close to `chunk_size`, to leave more rooms for the function to plan the optimal chunking.

    :::

*   `chunk_overlap` (*Int64*, optional): The maximum overlap size between adjacent chunks, in bytes.
*   `language` (*Str*, optional): The language of the document.
    Can be a language name (e.g. `Python`, `Javascript`, `Markdown`) or a file extension (e.g. `.py`, `.js`, `.md`).


    :::note

    We use the `language` field to determine how to split the input text, following these rules:

    *   We match the input `language` field against the following registries in the following order:
        *   `custom_languages` in the spec, against the `language_name` or `aliases` field of each entry.
        *   Builtin languages (see [Supported Languages](#supported-languages) section below), against the language, aliases or file extensions of each entry.

        All matches are in a case-insensitive manner. If the value of `language` is null, it'll be treated as empty string.

    *   If no match is found, the input will be treated as plain text.

    :::

Return: [*KTable*](/docs/core/data_types#ktable), each row represents a chunk, with the following sub fields:

*   `location` (*Range*): The location of the chunk.
*   `text` (*Str*): The text of the chunk.
*   `start` / `end` (*Struct*): Details about the start position (inclusive) and end position (exclusive) of the chunk. They have the following sub fields:
    *   `offset` (*Int64*): The byte offset of the position.
    *   `line` (*Int64*): The line number of the position. Starting from 1.
    *   `column` (*Int64*): The column number of the position. Starting from 1.

### Supported Languages

Currently, `SplitRecursively` supports the following languages:

| Language | Aliases | File Extensions |
|----------|---------|-----------------|
| C | | `.c` |
| C++ | CPP | `.cpp`, `.cc`, `.cxx`, `.h`, `.hpp` |
| C# | CSharp, CS | `.cs` |
| CSS | | `.css`, `.scss` |
| DTD | | `.dtd` |
| Fortran | F, F90, F95, F03 | `.f`, `.f90`, `.f95`, `.f03` |
| Go | Golang | `.go` |
| HTML | | `.html`, `.htm` |
| Java | | `.java` |
| JavaScript | JS | `.js` |
| JSON | | `.json` |
| Kotlin | | `.kt`, `.kts` |
| Markdown | MD | `.md`, `.mdx` |
| Pascal | PAS, DPR, Delphi | `.pas`, `.dpr` |
| PHP | | `.php` |
| Python | | `.py` |
| R | | `.r` |
| Ruby | | `.rb` |
| Rust | RS | `.rs` |
| Scala | | `.scala` |
| SQL | | `.sql` |
| Swift | | `.swift` |
| TOML | | `.toml` |
| TSX | | `.tsx` |
| TypeScript | TS | `.ts` |
| XML | | `.xml` |
| YAML | | `.yaml`, `.yml` |



## SentenceTransformerEmbed

`SentenceTransformerEmbed` embeds a text into a vector space using the [SentenceTransformer](https://huggingface.co/sentence-transformers) library.

:::note Optional Dependency Required

This function requires the 'sentence-transformers' library, which is an optional dependency. Install CocoIndex with:

```bash
pip install 'cocoindex[embeddings]'
```
:::

The spec takes the following fields:

*   `model` (`str`): The name of the SentenceTransformer model to use.
*   `args` (`dict[str, Any]`, optional): Additional arguments to pass to the SentenceTransformer constructor. e.g. `{"trust_remote_code": True}`

Input data:

*   `text` (*Str*): The text to embed.

Return: *Vector[Float32, N]*, where *N* is determined by the model

## ExtractByLlm

`ExtractByLlm` extracts structured information from a text using specified LLM. The spec takes the following fields:

*   `llm_spec` (`cocoindex.LlmSpec`): The specification of the LLM to use. See [LLM Spec](/docs/ai/llm#llm-spec) for more details.
*   `output_type` (`type`): The type of the output. e.g. a dataclass type name. See [Data Types](/docs/core/data_types) for all supported data types. The LLM will output values that match the schema of the type.
*   `instruction` (`str`, optional): Additional instruction for the LLM.

:::tip Clear type definitions

Definitions of the `output_type` is fed into LLM as guidance to generate the output.
To improve the quality of the extracted information, giving clear definitions for your dataclasses is especially important, e.g.

*   Provide readable field names for your dataclasses.
*   Provide reasonable docstrings for your dataclasses.
*   For any optional fields, clearly annotate that they are optional, by `SomeType | None` or `typing.Optional[SomeType]`.

:::

Input data:

*   `text` (*Str*): The text to extract information from.

Return: As specified by the `output_type` field in the spec. The extracted information from the input text.

## EmbedText

`EmbedText` embeds a text into a vector space using various LLM APIs that support text embedding.

The spec takes the following fields:

*   `api_type` ([`cocoindex.LlmApiType`](/docs/ai/llm#llm-api-types)): The type of LLM API to use for embedding.
*   `model` (`str`): The name of the embedding model to use.
*   `address` (`str`, optional): The address of the LLM API. If not specified, uses the default address for the API type.
*   `output_dimension` (`int`, optional): The expected dimension of the output embedding vector. If not specified, use the default dimension of the model.

    For most API types, the function internally keeps a registry for the default output dimension of known model.
    You need to explicitly specify the `output_dimension` if you want to use a new model that is not in the registry yet.

*   `task_type` (`str`, optional): The task type for embedding, used by some embedding models to optimize the embedding for specific use cases.

:::note Supported APIs for Text Embedding

Not all LLM APIs support text embedding. See the [LLM API Types table](/docs/ai/llm#llm-api-types) for which APIs support text embedding functionality.

:::

Input data:

*   `text` (*Str*, required): The text to embed.

Return: *Vector[Float32, N]*, where *N* is the dimension of the embedding vector determined by the model.



================================================
FILE: docs/docs/ops/sources.md
================================================
---
title: Sources
toc_max_heading_level: 4
description: CocoIndex Built-in Sources
---

# CocoIndex Built-in Sources

## LocalFile

The `LocalFile` source imports files from a local file system.

### Spec

The spec takes the following fields:
*   `path` (`str`): full path of the root directory to import files from
*   `binary` (`bool`, optional): whether reading files as binary (instead of text)
*   `included_patterns` (`list[str]`, optional): a list of glob patterns to include files, e.g. `["*.txt", "docs/**/*.md"]`.
    If not specified, all files will be included.
*   `excluded_patterns` (`list[str]`, optional): a list of glob patterns to exclude files, e.g. `["tmp", "**/node_modules"]`.
    Any file or directory matching these patterns will be excluded even if they match `included_patterns`.
    If not specified, no files will be excluded.

    :::info

    `included_patterns` and `excluded_patterns` are using Unix-style glob syntax. See [globset syntax](https://docs.rs/globset/latest/globset/index.html#syntax) for the details.

    :::

### Schema

The output is a [*KTable*](/docs/core/data_types#ktable) with the following sub fields:
*   `filename` (*Str*, key): the filename of the file, including the path, relative to the root directory, e.g. `"dir1/file1.md"`
*   `content` (*Str* if `binary` is `False`, *Bytes* otherwise): the content of the file

## AmazonS3

### Setup for Amazon S3

#### Setup AWS accounts

You need to setup AWS accounts to own and access Amazon S3. In particular,

*   Setup an AWS account from [AWS homepage](https://aws.amazon.com/) or login with an existing account.
*   AWS recommends all programming access to AWS should be done using [IAM users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html) instead of root account. You can create an IAM user at [AWS IAM Console](https://console.aws.amazon.com/iam/home).
*   Make sure your IAM user at least have the following permissions in the IAM console:
    *   Attach permission policy `AmazonS3ReadOnlyAccess` for read-only access to Amazon S3.
    *   (optional) Attach permission policy `AmazonSQSFullAccess` to receive notifications from Amazon SQS, if you want to enable change event notifications.
        Note that `AmazonSQSReadOnlyAccess` is not enough, as we need to be able to delete messages from the queue after they're processed.


#### Setup Credentials for AWS SDK

AWS SDK needs to access credentials to access Amazon S3.
The easiest way to setup credentials is to run:

```sh
aws configure
```

It will create a credentials file at `~/.aws/credentials` and config at `~/.aws/config`.

See the following documents if you need more control:

*   [`aws configure`](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html)
*   [Globally configuring AWS SDKs and tools](https://docs.aws.amazon.com/sdkref/latest/guide/creds-config-files.html)


#### Create Amazon S3 buckets

You can create a Amazon S3 bucket in the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/home), and upload your files to it.

It's also doable by using the AWS CLI `aws s3 mb` (to create buckets) and `aws s3 cp` (to upload files).
When doing so, make sure your current user also has permission policy `AmazonS3FullAccess`.

#### (Optional) Setup SQS queue for event notifications

You can setup an Amazon Simple Queue Service (Amazon SQS) queue to receive change event notifications from Amazon S3.
It provides a change capture mechanism for your AmazonS3 data source, to trigger reprocessing of your AWS S3 files on any creation, update or deletion.  Please use a dedicated SQS queue for each of your S3 data source.

This is how to setup:

*   Create a SQS queue with proper access policy.
    *   In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/home), create a queue.
    *   Add access policy statements, to make sure Amazon S3 can send messages to the queue.
        ```json
        {
          ...
          "Statement": [
            ...
            {
              "Sid": "__publish_statement",
              "Effect": "Allow",
              "Principal": {
                "Service": "s3.amazonaws.com"
              },
              "Resource": "${SQS_QUEUE_ARN}",
              "Action": "SQS:SendMessage",
              "Condition": {
                "ArnLike": {
                  "aws:SourceArn": "${S3_BUCKET_ARN}"
                }
              }
            }
          ]
        }
        ```

        Here, you need to replace `${SQS_QUEUE_ARN}` and `${S3_BUCKET_ARN}` with the actual ARN of your SQS queue and S3 bucket.
        You can find the ARN of your SQS queue in the existing policy statement (it starts with `arn:aws:sqs:`), and the ARN of your S3 bucket in the S3 console (it starts with `arn:aws:s3:`).

*   In the [Amazon S3 Console](https://s3.console.aws.amazon.com/s3/home), open your S3 bucket. Under *Properties* tab, click *Create event notification*.
    *   Fill in an arbitrary event name, e.g. `S3ChangeNotifications`.
    *   If you want your AmazonS3 data source to expose a subset of files sharing a prefix, set the same prefix here. Otherwise, leave it empty.
    *   Select the following event types: *All object create events*, *All object removal events*.
    *   Select *SQS queue* as the destination, and specify the SQS queue you created above.

AWS's [Guide of Configuring a Bucket for Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html#step1-create-sqs-queue-for-notification) provides more details.

### Spec

The spec takes the following fields:
*   `bucket_name` (`str`): Amazon S3 bucket name.
*   `prefix` (`str`, optional): if provided, only files with path starting with this prefix will be imported.
*   `binary` (`bool`, optional): whether reading files as binary (instead of text).
*   `included_patterns` (`list[str]`, optional): a list of glob patterns to include files, e.g. `["*.txt", "docs/**/*.md"]`.
    If not specified, all files will be included.
*   `excluded_patterns` (`list[str]`, optional): a list of glob patterns to exclude files, e.g. `["*.tmp", "**/*.log"]`.
    Any file or directory matching these patterns will be excluded even if they match `included_patterns`.
    If not specified, no files will be excluded.

    :::info

    `included_patterns` and `excluded_patterns` are using Unix-style glob syntax. See [globset syntax](https://docs.rs/globset/latest/globset/index.html#syntax) for the details.

    :::

*   `sqs_queue_url` (`str`, optional): if provided, the source will receive change event notifications from Amazon S3 via this SQS queue.

    :::info

    We will delete messages from the queue after they're processed.
    If there are unrelated messages in the queue (e.g. test messages that SQS will send automatically on queue creation, messages for a different bucket, for non-included files, etc.), we will delete the message upon receiving it, to avoid repeatedly receiving irrelevant messages after they're redelivered.

    :::

### Schema

The output is a [*KTable*](/docs/core/data_types#ktable) with the following sub fields:

*   `filename` (*Str*, key): the filename of the file, including the path, relative to the root directory, e.g. `"dir1/file1.md"`.
*   `content` (*Str* if `binary` is `False`, otherwise *Bytes*): the content of the file.


## AzureBlob

The `AzureBlob` source imports files from Azure Blob Storage.

### Setup for Azure Blob Storage

#### Get Started

If you didn't have experience with Azure Blob Storage, you can refer to the [quickstart](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal).
These are actions you need to take:

*   Create a storage account in the [Azure Portal](https://portal.azure.com/).
*   Create a container in the storage account.
*   Upload your files to the container.
*   Grant the user / identity / service principal (depends on your authentication method, see below) access to the storage account. At minimum, a **Storage Blob Data Reader** role is needed. See [this doc](https://learn.microsoft.com/en-us/azure/storage/blobs/authorize-data-operations-portal) for reference.

#### Authentication

We support the following authentication methods:

*   Shared access signature (SAS) tokens.
    You can generate it from the Azure Portal in the settings for a specific container.
    You need to provide at least *List* and *Read* permissions when generating the SAS token.
    It's a query string in the form of
    `sp=rl&st=2025-07-20T09:33:00Z&se=2025-07-19T09:48:53Z&sv=2024-11-04&sr=c&sig=i3FDjsadfklj3%23adsfkk`.

*   Storage account access key. You can find it in the Azure Portal in the settings for a specific storage account.

*   Default credential. When none of the above is provided, it will use the default credential.

    This allows you to connect to Azure services without putting any secrets in the code or flow spec.
    It automatically chooses the best authentication method based on your environment:

    *   On your local machine: uses your Azure CLI login (`az login`) or environment variables.

        ```sh
        az login
        # Optional: Set a default subscription if you have more than one
        az account set --subscription "<YOUR_SUBSCRIPTION_NAME_OR_ID>"
        ```
    *   In Azure (VM, App Service, AKS, etc.): uses the resourceâ€™s Managed Identity.
    *   In automated environments: supports Service Principals via environment variables
        *   `AZURE_CLIENT_ID`
        *   `AZURE_TENANT_ID`
        *   `AZURE_CLIENT_SECRET`

You can refer to [this doc](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication/overview) for more details.

### Spec

The spec takes the following fields:

*   `account_name` (`str`): the name of the storage account.
*   `container_name` (`str`): the name of the container.
*   `prefix` (`str`, optional): if provided, only files with path starting with this prefix will be imported.
*   `binary` (`bool`, optional): whether reading files as binary (instead of text).
*   `included_patterns` (`list[str]`, optional): a list of glob patterns to include files, e.g. `["*.txt", "docs/**/*.md"]`.
    If not specified, all files will be included.
*   `excluded_patterns` (`list[str]`, optional): a list of glob patterns to exclude files, e.g. `["*.tmp", "**/*.log"]`.
    Any file or directory matching these patterns will be excluded even if they match `included_patterns`.
    If not specified, no files will be excluded.
*   `sas_token` (`cocoindex.TransientAuthEntryReference[str]`, optional): a SAS token for authentication.
*   `account_access_key` (`cocoindex.TransientAuthEntryReference[str]`, optional): an account access key for authentication.

    :::info

    `included_patterns` and `excluded_patterns` are using Unix-style glob syntax. See [globset syntax](https://docs.rs/globset/latest/globset/index.html#syntax) for the details.

    :::

### Schema

The output is a [*KTable*](/docs/core/data_types#ktable) with the following sub fields:

*   `filename` (*Str*, key): the filename of the file, including the path, relative to the root directory, e.g. `"dir1/file1.md"`.
*   `content` (*Str* if `binary` is `False`, otherwise *Bytes*): the content of the file.


## GoogleDrive

The `GoogleDrive` source imports files from Google Drive.

### Setup for Google Drive

To access files in Google Drive, the `GoogleDrive` source will need to authenticate by service accounts.

1.  Register / login in **Google Cloud**.
2.  In [**Google Cloud Console**](https://console.cloud.google.com/), search for *Service Accounts*, to enter the *IAM & Admin / Service Accounts* page.
    -   **Create a new service account**: Click *+ Create Service Account*. Follow the instructions to finish service account creation.
    -   **Add a key and download the credential**: Under "Actions" for this new service account, click *Manage keys* â†’ *Add key* â†’ *Create new key* â†’ *JSON*.
      Download the key file to a safe place.
3.  In **Google Cloud Console**, search for *Google Drive API*. Enable this API.
4.  In **Google Drive**, share the folders containing files that need to be imported through your source with the service account's email address.
    **Viewer permission** is sufficient.
    -   The email address can be found under the *IAM & Admin / Service Accounts* page (in Step 2), in the format of `{service-account-id}@{gcp-project-id}.iam.gserviceaccount.com`.
    -   Copy the folder ID. Folder ID can be found from the last part of the folder's URL, e.g. `https://drive.google.com/drive/u/0/folders/{folder-id}` or `https://drive.google.com/drive/folders/{folder-id}?usp=drive_link`.


### Spec

The spec takes the following fields:

*   `service_account_credential_path` (`str`): full path to the service account credential file in JSON format.
*   `root_folder_ids` (`list[str]`): a list of Google Drive folder IDs to import files from.
*   `binary` (`bool`, optional): whether reading files as binary (instead of text).
*   `recent_changes_poll_interval` (`datetime.timedelta`, optional): when set, this source provides a change capture mechanism by polling Google Drive for recent modified files periodically.

    :::info

    Since it only retrieves metadata for recent modified files (up to the previous poll) during polling,
    it's typically cheaper than a full refresh by setting the [refresh interval](../core/flow_def#refresh-interval) especially when the folder contains a large number of files.
    So you can usually set it with a smaller value compared to the `refresh_interval`.

    On the other hand, this only detects changes for files that still exist.
    If the file is deleted (or the current account no longer has access to it), this change will not be detected by this change stream.

    So when a `GoogleDrive` source has `recent_changes_poll_interval` enabled, it's still recommended to set a `refresh_interval`, with a larger value.
    So that most changes can be covered by polling recent changes (with low latency, like 10 seconds), and remaining changes (files no longer exist or accessible) will still be covered (with a higher latency, like 5 minutes, and should be larger if you have a huge number of files like 1M).
    In reality, configure them based on your requirement: how fresh do you need the target index to be?

    :::

### Schema

The output is a [*KTable*](/docs/core/data_types#ktable) with the following sub fields:

*   `file_id` (*Str*, key): the ID of the file in Google Drive.
*   `filename` (*Str*): the filename of the file, without the path, e.g. `"file1.md"`
*   `mime_type` (*Str*): the MIME type of the file.
*   `content` (*Str* if `binary` is `False`, otherwise *Bytes*): the content of the file.



================================================
FILE: docs/docs/ops/targets.md
================================================
---
title: Targets
description: CocoIndex Built-in Targets
toc_max_heading_level: 4
---

# CocoIndex Built-in Targets

For each target, data are exported from a data collector, containing data of multiple entries, each with multiple fields.
The way to map data from a data collector to a target depends on data model of the target.

## Entry-Oriented Targets

An entry-oriented target organizes data into independent entries, such as rows, key-value pairs, or documents.
Each entry is self-contained and does not explicitly link to others.
There is usually a straightforward mapping from data collector rows to entries.

### Postgres

Exports data to Postgres database (with pgvector extension).

#### Data Mapping

Here's how CocoIndex data elements map to Postgres elements during export:

| CocoIndex Element | Postgres Element |
|-------------------|------------------|
| an export target | a unique table |
| a collected row | a row |
| a field | a column |

For example, if you have a data collector that collects rows with fields `id`, `title`, and `embedding`, it will be exported to a Postgres table with corresponding columns.
It should be a unique table, meaning that no other export target should export to the same table.

:::warning vector type mapping to Postgres

Since vectors in pgvector must have fixed dimension, we only map vectors of number types with fixed dimension (i.e. *Vector[cocoindex.Float32, N]*, *Vector[cocoindex.Float64, N]*, and *Vector[cocoindex.Int64, N]*) to `vector(N)` columns.
For all other vector types, we map them to `jsonb` columns.

:::

#### Spec

The spec takes the following fields:

*   `database` ([auth reference](../core/flow_def#auth-registry) to `DatabaseConnectionSpec`, optional): The connection to the Postgres database.
    See [DatabaseConnectionSpec](../core/settings#databaseconnectionspec) for its specific fields.
    If not provided, will use the same database as the [internal storage](/docs/core/basics#internal-storage).

*   `table_name` (`str`, optional): The name of the table to store to. If unspecified, will use the table name `[${AppNamespace}__]${FlowName}__${TargetName}`, e.g. `DemoFlow__doc_embeddings` or `Staging__DemoFlow__doc_embeddings`.

### Qdrant

Exports data to a [Qdrant](https://qdrant.tech/) collection.

#### Data Mapping

Here's how CocoIndex data elements map to Qdrant elements during export:

| CocoIndex Element | Qdrant Element |
|-------------------|------------------|
| an export target  | a unique collection |
| a collected row   | a point |
| a field           | a named vector, if fits into Qdrant vector; or a field within payload otherwise |

*Vector[Float32, N]*, *Vector[Float64, N]* and *Vector[Int64, N]* types fit into Qdrant vector.

:::warning vector type mapping to Qdrant

Since vectors in Qdrant must have fixed dimension, we only map vectors of number types with fixed dimension (i.e. *Vector[cocoindex.Float32, N]*, *Vector[cocoindex.Float64, N]*, and *Vector[cocoindex.Int64, N]*) to Qdrant vectors.
For all other vector types, we map to Qdrant payload as JSON arrays.

:::

#### Spec

The spec takes the following fields:

*   `connection` ([auth reference](../core/flow_def#auth-registry) to `QdrantConnection`, optional): The connection to the Qdrant instance. `QdrantConnection` has the following fields:
    *   `grpc_url` (`str`): The [gRPC URL](https://qdrant.tech/documentation/interfaces/#grpc-interface) of the Qdrant instance, e.g. `http://localhost:6334/`.
    *   `api_key` (`str`, optional). API key to authenticate requests with.

    If `connection` is not provided, will use local Qdrant instance at `http://localhost:6334/` by default.

*   `collection_name` (`str`, required): The name of the collection to export the data to.

You can find an end-to-end example [here](https://github.com/cocoindex-io/cocoindex/tree/main/examples/text_embedding_qdrant).

## Property Graph Targets

Property graph is a widely-adopted model for knowledge graphs, where both nodes and relationships can have properties.
[Graph database concepts](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/) has a good introduction to basic concepts of property graphs.

The following concepts will be used in the following sections:
* [Node](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/#graphdb-node)
    * [Node label](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/#graphdb-labels), which represents a type of nodes.
* [Relationship](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/#graphdb-relationship), which describes a connection between two nodes.
    * [Relationship type](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/#graphdb-relationship-type)
* [Properties](https://neo4j.com/docs/getting-started/appendix/graphdb-concepts/#graphdb-properties), which are key-value pairs associated with nodes and relationships.

### Data Mapping

Data from collectors are mapped to graph elements in various types:

1.  Rows from collectors â†’ Nodes in the graph
2.  Rows from collectors â†’ Relationships in the graph (including source and target nodes of the relationship)

This is what you need to provide to define these mappings:

*   Specify [nodes to export](#nodes-to-export).
*   [Declare extra node labels](#declare-extra-node-labels), for labels to appear as source/target nodes of relationships but not exported as nodes.
*   Specify [relationships to export](#relationships-to-export).

In addition, the same node may appear multiple times, from exported nodes and various relationships.
They should appear as the same node in the target graph database.
CocoIndex automatically [matches and deduplicates nodes](#nodes-matching-and-deduplicating) based on their primary key values.

#### Nodes to Export

Here's how CocoIndex data elements map to nodes in the graph:

| CocoIndex Element | Graph Element |
|-------------------|------------------|
| an export target  | nodes with a unique label |
| a collected row   | a node |
| a field           | a property of node |

Note that the label used in different `Nodes`s should be unique.

`cocoindex.targets.Nodes` is to describe mapping to nodes. It has the following fields:

*   `label` (`str`): The label of the node.

For example, consider we have collected the following rows:

<small>

| filename | summary |
|----------|---------|
| chapter1.md | At the beginning, ... |
| chapter2.md | In the second day, ... |

</small>

We can export them to nodes under label `Document` like this:

```python
document_collector.export(
    ...
    cocoindex.targets.Neo4j(
        ...
        mapping=cocoindex.targets.Nodes(label="Document"),
    ),
    primary_key_fields=["filename"],
)
```

The collected rows will be mapped to nodes in knowledge database like this:

```mermaid
graph TD
  Doc_Chapter1@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter1.md**
            summary: At the beginning, ..."
    classDef: node
  }

  Doc_Chapter2@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter2.md**
            summary: In the second day, ..."
    classDef: node
  }

  classDef node font-size:8pt,text-align:left,stroke-width:2;
```

#### Declare Extra Node Labels

If a node label needs to appear as source or target of a relationship, but not exported as a node, you need to [declare](../core/flow_def#target-declarations) the label with necessary configuration.

The dataclass to describe the declaration is specific to each target (e.g. `cocoindex.targets.Neo4jDeclarations`),
while they share the following common fields:

*   `nodes_label` (required): The label of the node.
*   Options for [storage indexes](../core/flow_def#storage-indexes).
    *   `primary_key_fields` (required)
    *   `vector_indexes` (optional)

Continuing the same example above.
Considering we want to extract relationships from `Document` to `Place` later (i.e. a document mentions a place), but the `Place` label isn't exported as a node, we need to declare it:

```python
flow_builder.declare(
    cocoindex.targets.Neo4jDeclarations(
        connection = ...,
        nodes_label="Place",
        primary_key_fields=["name"],
    ),
)
```

#### Relationships to Export

Here's how CocoIndex data elements map to relationships in the graph:

| CocoIndex Element | Graph Element |
|-------------------|------------------|
| an export target  | relationships with a unique type |
| a collected row   | a relationship |
| a field           | a property of relationship, or a property of source/target node, based on configuration |

Note that the type used in different `Relationships`s should be unique.

`cocoindex.targets.Relationships` is to describe mapping to relationships. It has the following fields:

*   `rel_type` (`str`): The type of the relationship.
*   `source`/`target` (`cocoindex.targets.NodeFromFields`): Specify how to extract source/target node information from specific fields in the collected row. It has the following fields:
    *   `label` (`str`): The label of the node.
    *   `fields` (`Sequence[cocoindex.targets.TargetFieldMapping]`): Specify field mappings from the collected rows to node properties, with the following fields:
        *   `source` (`str`): The name of the field in the collected row.
        *   `target` (`str`, optional): The name of the field to use as the node field. If unspecified, will use the same as `source`.

        :::note Map necessary fields for nodes of relationships

        You need to map the following fields for nodes of each relationship:

        *   Make sure all primary key fields for the label are mapped.
        *   Optionally, you can also map non-key fields. If you do so, please make sure all value fields are mapped.

        :::

All fields in the collector that are not used in mappings for source or target node fields will be mapped to relationship properties.

For example, consider we have collected the following rows, to describe places mentioned in each file, along with embeddings of the places:

<small>

| doc_filename | place_name | place_embedding | location |
|----------|-------|-----------------|-----------------|
| chapter1.md | Crystal Palace | [0.1, 0.5, ...] | 12 |
| chapter2.md | Magic Forest | [0.4, 0.2, ...] | 23 |
| chapter2.md | Crystal Palace | [0.1, 0.5, ...] | 56 |

</small>

We can export them to relationships under type `MENTION` like this:

```python
doc_place_collector.export(
    ...
    cocoindex.targets.Neo4j(
        ...
        mapping=cocoindex.targets.Relationships(
            rel_type="MENTION",
            source=cocoindex.targets.NodeFromFields(
                label="Document",
                fields=[cocoindex.targets.TargetFieldMapping(source="doc_filename", target="filename")],
            ),
            target=cocoindex.targets.NodeFromFields(
                label="Place",
                fields=[
                    cocoindex.targets.TargetFieldMapping(source="place_name", target="name"),
                    cocoindex.targets.TargetFieldMapping(source="place_embedding", target="embedding"),
                ],
            ),
        ),
    ),
    ...
)
```

The `doc_filename` field is mapped to `Document.filename` property for the source node, while `place_name` and `place_embedding` are mapped to `Place.name` and `Place.embedding` properties for the target node.
The remaining field `location` becomes a property of the relationship.
For the data above, we get a bunch of relationships like this:

```mermaid
graph TD
  Doc_Chapter1@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter1.md**"
    classDef: nodeRef
  }

  Doc_Chapter2_a@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter2.md**"
    classDef: nodeRef
  }

  Doc_Chapter2_b@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter2.md**"
    classDef: nodeRef
  }

  Place_CrystalPalace_a@{
    shape: rounded
    label: "**[Place]**
            **name\\*: Crystal Palace**
            embedding: [0.1, 0.5, ...]"
    classDef: node
  }

  Place_MagicForest@{
    shape: rounded
    label: "**[Place]**
            **name\\*: Magic Forest**
            embedding: [0.4, 0.2, ...]"
    classDef: node
  }

  Place_CrystalPalace_b@{
    shape: rounded
    label: "**[Place]**
            **name\\*: Crystal Palace**
            embedding: [0.1, 0.5, ...]"
    classDef: node
  }


  Doc_Chapter1:::nodeRef -- **:MENTION** (location:12) --> Place_CrystalPalace_a:::node
  Doc_Chapter2_a:::nodeRef -- **:MENTION** (location:23) --> Place_MagicForest:::node
  Doc_Chapter2_b:::nodeRef -- **:MENTION** (location:56) --> Place_CrystalPalace_b:::node

  classDef nodeRef font-size:8pt,text-align:left,fill:transparent,stroke-width:1,stroke-dasharray:5 5;
  classDef node font-size:8pt,text-align:left,stroke-width:2;

```

#### Nodes Matching and Deduplicating

The nodes and relationships we got above are discrete elements.
To fit them into a connected property graph, CocoIndex will match and deduplicate nodes automatically:

*   Match nodes based on their primary key values. Nodes with the same primary key values are considered as the same node.
*   For non-primary key fields (a.k.a. value fields), CocoIndex will pick the values from an arbitrary one.
    If multiple nodes (before deduplication) with the same primary key provide value fields, an arbitrary one will be picked.

:::note

The best practice is to make the value fields consistent across different appearances of the same node, to avoid non-determinism in the exported graph.

:::

After matching and deduplication, we get the final graph:

```mermaid
graph TD
  Doc_Chapter1@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter1.md**
            summary: At the beginning, ..."
    classDef: node
  }

  Doc_Chapter2@{
    shape: rounded
    label: "**[Document]**
            **filename\\*: chapter2.md**
            summary: In the second day, ..."
    classDef: node
  }

  Place_CrystalPalace@{
    shape: rounded
    label: "**[Place]**
            **name\\*: Crystal Palace**
            embedding: [0.1, 0.5, ...]"
    classDef: node
  }

  Place_MagicForest@{
    shape: rounded
    label: "**[Place]**
            **name\\*: Magic Forest**
            embedding: [0.4, 0.2, ...]"
    classDef: node
  }

  Doc_Chapter1:::node -- **:MENTION** (location:12) --> Place_CrystalPalace:::node
  Doc_Chapter2:::node -- **:MENTION** (location:23) --> Place_MagicForest:::node
  Doc_Chapter2:::node -- **:MENTION** (location:56) --> Place_CrystalPalace:::node

  classDef node font-size:8pt,text-align:left,stroke-width:2;
```

#### Examples

You can find end-to-end examples fitting into any of supported property graphs in the following directories:
*   [examples/docs_to_knowledge_graph](https://github.com/cocoindex-io/cocoindex/tree/main/examples/docs_to_knowledge_graph)
*   [examples/product_recommendation](https://github.com/cocoindex-io/cocoindex/tree/main/examples/product_recommendation)

### Neo4j

#### Spec

The `Neo4j` target spec takes the following fields:

*   `connection` ([auth reference](../core/flow_def#auth-registry) to `Neo4jConnectionSpec`): The connection to the Neo4j database. `Neo4jConnectionSpec` has the following fields:
    *   `url` (`str`): The URI of the Neo4j database to use as the internal storage, e.g. `bolt://localhost:7687`.
    *   `user` (`str`): Username for the Neo4j database.
    *   `password` (`str`): Password for the Neo4j database.
    *   `db` (`str`, optional): The name of the Neo4j database to use as the internal storage, e.g. `neo4j`.
*   `mapping` (`Nodes | Relationships`): The mapping from collected row to nodes or relationships of the graph. For either [nodes to export](#nodes-to-export) or [relationships to export](#relationships-to-export).

Neo4j also provides a declaration spec `Neo4jDeclaration`, to configure indexing options for nodes only referenced by relationships. It has the following fields:

*   `connection` (auth reference to `Neo4jConnectionSpec`)
*   Fields for [nodes to declare](#declare-extra-node-labels), including
    *   `nodes_label` (required)
    *   `primary_key_fields` (required)
    *   `vector_indexes` (optional)

#### Neo4j dev instance

If you don't have a Neo4j database, you can start a Neo4j database using our docker compose config:

```bash
docker compose -f <(curl -L https://raw.githubusercontent.com/cocoindex-io/cocoindex/refs/heads/main/dev/neo4j.yaml) up -d
```

This will bring up a Neo4j instance, which can be accessed by username `neo4j` and password `cocoindex`.
You can access the Neo4j browser at [http://localhost:7474](http://localhost:7474).

:::warning

The docker compose config above will start a Neo4j Enterprise instance under the [Evaluation License](https://neo4j.com/terms/enterprise_us/),
with 30 days trial period.
Please read and agree the license before starting the instance.

:::


### Kuzu

#### Spec

CocoIndex supports talking to Kuzu through its [API server](https://github.com/kuzudb/api-server).

The `Kuzu` target spec takes the following fields:

*   `connection` ([auth reference](../core/flow_def#auth-registry) to `KuzuConnectionSpec`): The connection to the Kuzu database. `KuzuConnectionSpec` has the following fields:
    *   `api_server_url` (`str`): The URL of the Kuzu API server, e.g. `http://localhost:8123`.
*   `mapping` (`Nodes | Relationships`): The mapping from collected row to nodes or relationships of the graph. For either [nodes to export](#nodes-to-export) or [relationships to export](#relationships-to-export).

Kuzu also provides a declaration spec `KuzuDeclaration`, to configure indexing options for nodes only referenced by relationships. It has the following fields:

*   `connection` (auth reference to `KuzuConnectionSpec`)
*   Fields for [nodes to declare](#declare-extra-node-labels), including
    *   `nodes_label` (required)
    *   `primary_key_fields` (required)

#### Kuzu dev instance

If you don't have a Kuzu instance yet, you can bring up a Kuzu API server locally by running:

```bash
KUZU_DB_DIR=$HOME/.kuzudb
KUZU_PORT=8123
docker run -d --name kuzu -p ${KUZU_PORT}:8000 -v ${KUZU_DB_DIR}:/database kuzudb/api-server:latest
```

To explore the graph you built with Kuzu, you can use the [Kuzu Explorer](https://github.com/kuzudb/explorer).
Currently Kuzu API server and the explorer cannot be up at the same time. So you need to stop the API server before running the explorer.

To start the instance of the explorer, run:

```bash
KUZU_EXPLORER_PORT=8124
docker run -d --name kuzu-explorer -p ${KUZU_EXPLORER_PORT}:8000  -v ${KUZU_DB_DIR}:/database -e MODE=READ_ONLY  kuzudb/explorer:latest
```

You can then access the explorer at [http://localhost:8124](http://localhost:8124).



================================================
FILE: docs/src/components/HomepageFeatures/index.tsx
================================================
import type {ReactNode} from 'react';
import clsx from 'clsx';
import Heading from '@theme/Heading';
import styles from './styles.module.css';

type FeatureItem = {
  title: string;
  Svg: React.ComponentType<React.ComponentProps<'svg'>>;
  description: ReactNode;
};

const FeatureList: FeatureItem[] = [
  {
    title: 'Easy to Use',
    Svg: require('@site/static/img/undraw_docusaurus_mountain.svg').default,
    description: (
      <>
        Docusaurus was designed from the ground up to be easily installed and
        used to get your website up and running quickly.
      </>
    ),
  },
  {
    title: 'Focus on What Matters',
    Svg: require('@site/static/img/undraw_docusaurus_tree.svg').default,
    description: (
      <>
        Docusaurus lets you focus on your docs, and we&apos;ll do the chores. Go
        ahead and move your docs into the <code>docs</code> directory.
      </>
    ),
  },
  {
    title: 'Powered by React',
    Svg: require('@site/static/img/undraw_docusaurus_react.svg').default,
    description: (
      <>
        Extend or customize your website layout by reusing React. Docusaurus can
        be extended while reusing the same header and footer.
      </>
    ),
  },
];

function Feature({title, Svg, description}: FeatureItem) {
  return (
    <div className={clsx('col col--4')}>
      <div className="text--center">
        <Svg className={styles.featureSvg} role="img" />
      </div>
      <div className="text--center padding-horiz--md">
        <Heading as="h3">{title}</Heading>
        <p>{description}</p>
      </div>
    </div>
  );
}

export default function HomepageFeatures(): ReactNode {
  return (
    <section className={styles.features}>
      <div className="container">
        <div className="row">
          {FeatureList.map((props, idx) => (
            <Feature key={idx} {...props} />
          ))}
        </div>
      </div>
    </section>
  );
}



================================================
FILE: docs/src/components/HomepageFeatures/styles.module.css
================================================
.features {
  display: flex;
  align-items: center;
  padding: 2rem 0;
  width: 100%;
}

.featureSvg {
  height: 200px;
  width: 200px;
}



================================================
FILE: docs/src/css/custom.css
================================================
/**
 * Any CSS included here will be global. The classic template
 * bundles Infima by default. Infima is a CSS framework designed to
 * work well for content-centric websites.
 */

/* Import Questrial font from Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Questrial&display=swap');

/* You can override the default Infima variables here. */
:root {
  /* Iris color scheme from Radix UI */
  --ifm-color-primary: #5B5BD6;
  --ifm-color-primary-dark: #4D4DCC;
  --ifm-color-primary-darker: #4040C8;
  --ifm-color-primary-darkest: #3333B3;
  --ifm-color-primary-light: #7373DE;
  --ifm-color-primary-lighter: #8F8FE6;
  --ifm-color-primary-lightest: #ABABEF;


  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.1);
  --my-color-text-black: #111827;

  /* Additional colors */
  --ifm-navbar-background-color: var(--ifm-background-color);
  --ifm-background-color: #ffffff;
  --ifm-footer-background-color: #ffffff;
  --ifm-menu-color: #374151;
  --ifm-toc-link-color: #374151;

  /* Theme colors for breadcrumbs */
  --theme-color-text-light: #6b7280;
  --theme-color-text-default: #111827;
  --theme-color-keyline: #e5e7eb;
}

/* For readability concerns, you should choose a lighter palette in dark mode. */
[data-theme='dark'] {
  --ifm-color-primary: #8F8FE6;
  --ifm-color-primary-dark: #7373DE;
  --ifm-color-primary-darker: #5B5BD6;
  --ifm-color-primary-darkest: #4D4DCC;
  --ifm-color-primary-light: #ABABEF;
  --ifm-color-primary-lighter: #C4C4F5;
  --ifm-color-primary-lightest: #E1E1FF;

  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.3);
  --my-color-text-black: #f9fafb;

  /* Dark mode specific colors */
  --ifm-navbar-background-color: var(--ifm-background-color);
  --ifm-background-color: #111827;
  --ifm-footer-background-color: #111827;
  --ifm-menu-color: #f3f4f6;
  --ifm-toc-link-color: #f3f4f6;

  /* Dark mode theme colors for breadcrumbs */
  --theme-color-text-light: #9ca3af;
  --theme-color-text-default: #f9fafb;
  --theme-color-keyline: #374151;
}

.markdown {
  line-height: 150%;

  code {
    font-size: var(--ifm-code-font-size);
    border: none;
  }

  a {
    font-weight: var(--ifm-font-weight-semibold);
  }

  h1,
  h1:first-child,
  h2,
  h3,
  h4,
  h5,
  h6 {
    --ifm-h1-font-size: 1.8rem;
    --ifm-h1-vertical-rhythm-bottom: 0.5;
    --ifm-h2-font-size: 1.5rem;
    --ifm-heading-vertical-rhythm-bottom: 1;
    --ifm-h3-font-size: 1.2rem;
    --ifm-h4-font-size: 1rem;
    --ifm-h5-font-size: 0.8rem;
  }
}

.navbar__title {
  font-family: 'Questrial', sans-serif;
  font-size: 1.125rem; /* 18px */
  color: var(--my-color-text-black);
}

.navbar {
  box-shadow: none;
  font-size: 0.875rem; /* 14px */
  border-bottom: 1px solid var(--ifm-color-emphasis-200);
}

.sidebarItemTitle_pO2u,
.navbar__items {
  font-family: 'Questrial', sans-serif;
}

.footer {
  padding: 4rem 2rem;
  border-top: 1px solid var(--ifm-color-emphasis-100);
}

.footer__title {
  font-family: 'Questrial', sans-serif;
  font-size: 1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  color: var(--my-color-text-black);
}

.footer__item {
  font-family: 'Questrial', sans-serif;
  font-size: 0.9rem;
  padding: 0.25rem 0;
}

.footer__copyright {
  font-family: 'Questrial', sans-serif;
  font-size: 0.9rem;
  margin-top: 3rem;
  text-align: left;
  color: var(--ifm-color-emphasis-600);
}

.footer__link-item {
  color: var(--ifm-color-emphasis-700);
  line-height: 2;
}

.footer__link-item:hover {
  color: var(--ifm-color-primary);
  text-decoration: none;
}

.theme-doc-sidebar-menu {
  font-size: 0.875rem; /* 14px */
}

.table-of-contents {
  font-size: 0.8125rem; /* 13px */
}

.breadcrumbs {
  display: flex;
  flex-wrap: wrap;
  align-items: center;
  color: var(--theme-color-text-light);

  .breadcrumbs__item:first-child {
    display: none;
  }
  .breadcrumbs__link {
    padding: 0;
    background: none;
  }
}

/* Sidebar caret styles */
.menu__caret:before,
.menu__link--sublist-caret:after {
  background: var(--ifm-menu-link-sublist-icon) 50% / 1rem 1rem;
}

.navbar__item.navbar-github-link{
  display: block;
  width: 120px;
}



================================================
FILE: docs/src/theme/Root.js
================================================
import React from 'react';
import mixpanel from 'mixpanel-browser';

// Default implementation, that you can customize
export default function Root({ children }) {
  React.useEffect(() => {
    const mixpanelApiKey = process.env.COCOINDEX_DOCS_MIXPANEL_API_KEY;
    if (typeof window !== 'undefined' && !!mixpanelApiKey) {
      // Initialize Mixpanel with the token
      mixpanel.init(mixpanelApiKey, {
        track_pageview: true,
        debug: process.env.NODE_ENV === 'development'
      });
    }
  }, []);

  return <>{children}</>;
}



================================================
FILE: docs/static/robots.txt
================================================
User-agent: *
Disallow:



================================================
FILE: docs/static/.nojekyll
================================================
[Empty file]


================================================
FILE: examples/amazon_s3_embedding/README.md
================================================
This example builds an embedding index based on files stored in an Amazon S3 bucket.
It continuously updates the index as files are added / updated / deleted in the source bucket:
it keeps the index in sync with the Amazon S3 bucket effortlessly.

## Prerequisite

Before running the example, you need to:

1.  [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one.

2.  Prepare for Amazon S3.
    See [Setup for AWS S3](https://cocoindex.io/docs/ops/sources#setup-for-amazon-s3) for more details.

3.  Create a `.env` file with your Amazon S3 bucket name and (optionally) prefix.
    Start from copying the `.env.example`, and then edit it to fill in your bucket name and prefix.

    ```bash
    cp .env.example .env
    $EDITOR .env
    ```

    Example `.env` file:
    ```
    # Database Configuration
    DATABASE_URL=postgresql://localhost:5432/cocoindex

    # Amazon S3 Configuration
    AMAZON_S3_BUCKET_NAME=your-bucket-name
    AMAZON_S3-SQS_QUEUE_URL=https://sqs.us-west-2.amazonaws.com/123456789/S3ChangeNotifications
    ```

## Run

Install dependencies:

```sh
pip install -e .
```

Run:

```sh
python main.py
```

During running, it will keep observing changes in the Amazon S3 bucket and update the index automatically.
At the same time, it accepts queries from the terminal, and performs search on top of the up-to-date index.


## CocoInsight
CocoInsight is in Early Access now (Free) ğŸ˜Š You found us! A quick 3 minute video tutorial about CocoInsight: [Watch on YouTube](https://youtu.be/ZnmyoHslBSc?si=pPLXWALztkA710r9).

Run CocoInsight to understand your RAG data pipeline:

```sh
cocoindex server -ci main.py
```

You can also add a `-L` flag to make the server keep updating the index to reflect source changes at the same time:

```sh
cocoindex server -ci -L main.py
```

Then open the CocoInsight UI at [https://cocoindex.io/cocoinsight](https://cocoindex.io/cocoinsight).



================================================
FILE: examples/amazon_s3_embedding/main.py
================================================
from dotenv import load_dotenv
from psycopg_pool import ConnectionPool
import cocoindex
import os
from typing import Any


@cocoindex.transform_flow()
def text_to_embedding(
    text: cocoindex.DataSlice[str],
) -> cocoindex.DataSlice[list[float]]:
    """
    Embed the text using a SentenceTransformer model.
    This is a shared logic between indexing and querying, so extract it as a function.
    """
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )


@cocoindex.flow_def(name="AmazonS3TextEmbedding")
def amazon_s3_text_embedding_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
) -> None:
    """
    Define an example flow that embeds text from Amazon S3 into a vector database.
    """
    bucket_name = os.environ["AMAZON_S3_BUCKET_NAME"]
    prefix = os.environ.get("AMAZON_S3_PREFIX", None)
    sqs_queue_url = os.environ.get("AMAZON_S3_SQS_QUEUE_URL", None)

    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.AmazonS3(
            bucket_name=bucket_name,
            prefix=prefix,
            included_patterns=["*.md", "*.mdx", "*.txt", "*.docx"],
            binary=False,
            sqs_queue_url=sqs_queue_url,
        )
    )

    doc_embeddings = data_scope.add_collector()

    with data_scope["documents"].row() as doc:
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown",
            chunk_size=2000,
            chunk_overlap=500,
        )

        with doc["chunks"].row() as chunk:
            chunk["embedding"] = text_to_embedding(chunk["text"])
            doc_embeddings.collect(
                filename=doc["filename"],
                location=chunk["location"],
                text=chunk["text"],
                embedding=chunk["embedding"],
            )

    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY,
            )
        ],
    )


def search(pool: ConnectionPool, query: str, top_k: int = 5) -> list[dict[str, Any]]:
    # Get the table name, for the export target in the amazon_s3_text_embedding_flow above.
    table_name = cocoindex.utils.get_target_default_name(
        amazon_s3_text_embedding_flow, "doc_embeddings"
    )
    # Evaluate the transform flow defined above with the input query, to get the embedding.
    query_vector = text_to_embedding.eval(query)
    # Run the query and get the results.
    with pool.connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT filename, text, embedding <=> %s::vector AS distance
                FROM {table_name} ORDER BY distance LIMIT %s
            """,
                (query_vector, top_k),
            )
            return [
                {"filename": row[0], "text": row[1], "score": 1.0 - row[2]}
                for row in cur.fetchall()
            ]


def _main() -> None:
    # Initialize the database connection pool.
    pool = ConnectionPool(os.getenv("COCOINDEX_DATABASE_URL"))

    amazon_s3_text_embedding_flow.setup()
    with cocoindex.FlowLiveUpdater(amazon_s3_text_embedding_flow) as updater:
        # Run queries in a loop to demonstrate the query capabilities.
        while True:
            query = input("Enter search query (or Enter to quit): ")
            if query == "":
                break
            # Run the query function with the database connection pool and the query.
            results = search(pool, query)
            print("\nSearch results:")
            for result in results:
                print(f"[{result['score']:.3f}] {result['filename']}")
                print(f"    {result['text']}")
                print("---")
            print()


if __name__ == "__main__":
    load_dotenv()
    cocoindex.init()
    _main()



================================================
FILE: examples/amazon_s3_embedding/pyproject.toml
================================================
[project]
name = "amazon-s3-text-embedding"
version = "0.1.0"
description = "Simple example for cocoindex: build embedding index based on Amazon S3 files."
requires-python = ">=3.11"
dependencies = ["cocoindex[embeddings]>=0.1.67", "python-dotenv>=1.0.1"]

[tool.setuptools]
packages = []



================================================
FILE: examples/amazon_s3_embedding/.env.example
================================================
# Postgres database address for cocoindex
COCOINDEX_DATABASE_URL=postgres://cocoindex:cocoindex@localhost/cocoindex

# Amazon S3 Configuration
AMAZON_S3_BUCKET_NAME=your-bucket-name

# Optional
# AMAZON_S3_PREFIX=

# Optional
# AMAZON_S3_SQS_QUEUE_URL=



================================================
FILE: examples/azure_blob_embedding/README.md
================================================
This example builds an embedding index based on files stored in an Azure Blob Storage container.
It continuously updates the index as files are added / updated / deleted in the source container:
it keeps the index in sync with the Azure Blob Storage container effortlessly.

## Prerequisite

Before running the example, you need to:

1.  [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one.

2.  Prepare for Azure Blob Storage.
    See [Setup for Azure Blob Storage](https://cocoindex.io/docs/ops/sources#setup-for-azure-blob-storage) for more details.

3.  Create a `.env` file with your Azure Blob Storage container name and (optionally) prefix.
    Start from copying the `.env.example`, and then edit it to fill in your bucket name and prefix.

    ```bash
    cp .env.example .env
    $EDITOR .env
    ```

    Example `.env` file:
    ```
    # Database Configuration
    DATABASE_URL=postgresql://localhost:5432/cocoindex

    # Azure Blob Storage Configuration
    AZURE_BLOB_STORAGE_ACCOUNT_NAME=your-account-name
    AZURE_BLOB_STORAGE_CONTAINER_NAME=your-container-name
    ```

## Run

Install dependencies:

```sh
pip install -e .
```

Run:

```sh
python main.py
```

During running, it will keep observing changes in the Amazon S3 bucket and update the index automatically.
At the same time, it accepts queries from the terminal, and performs search on top of the up-to-date index.


## CocoInsight
CocoInsight is in Early Access now (Free) ğŸ˜Š You found us! A quick 3 minute video tutorial about CocoInsight: [Watch on YouTube](https://youtu.be/ZnmyoHslBSc?si=pPLXWALztkA710r9).

Run CocoInsight to understand your RAG data pipeline:

```sh
cocoindex server -ci main.py
```

You can also add a `-L` flag to make the server keep updating the index to reflect source changes at the same time:

```sh
cocoindex server -ci -L main.py
```

Then open the CocoInsight UI at [https://cocoindex.io/cocoinsight](https://cocoindex.io/cocoinsight).



================================================
FILE: examples/azure_blob_embedding/main.py
================================================
from dotenv import load_dotenv
from psycopg_pool import ConnectionPool
import cocoindex
import os
from typing import Any


@cocoindex.transform_flow()
def text_to_embedding(
    text: cocoindex.DataSlice[str],
) -> cocoindex.DataSlice[list[float]]:
    """
    Embed the text using a SentenceTransformer model.
    This is a shared logic between indexing and querying, so extract it as a function.
    """
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )


@cocoindex.flow_def(name="AzureBlobTextEmbedding")
def azure_blob_text_embedding_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
) -> None:
    """
    Define an example flow that embeds text from Azure Blob Storage into a vector database.
    """
    account_name = os.environ["AZURE_STORAGE_ACCOUNT_NAME"]
    container_name = os.environ["AZURE_BLOB_CONTAINER_NAME"]
    prefix = os.environ.get("AZURE_BLOB_PREFIX", None)

    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.AzureBlob(
            account_name=account_name,
            container_name=container_name,
            prefix=prefix,
            included_patterns=["*.md", "*.mdx", "*.txt", "*.docx"],
            binary=False,
        )
    )

    doc_embeddings = data_scope.add_collector()

    with data_scope["documents"].row() as doc:
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown",
            chunk_size=2000,
            chunk_overlap=500,
        )

        with doc["chunks"].row() as chunk:
            chunk["embedding"] = text_to_embedding(chunk["text"])
            doc_embeddings.collect(
                filename=doc["filename"],
                location=chunk["location"],
                text=chunk["text"],
                embedding=chunk["embedding"],
            )

    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY,
            )
        ],
    )


def search(pool: ConnectionPool, query: str, top_k: int = 5) -> list[dict[str, Any]]:
    # Get the table name, for the export target in the azure_blob_text_embedding_flow above.
    table_name = cocoindex.utils.get_target_default_name(
        azure_blob_text_embedding_flow, "doc_embeddings"
    )
    # Evaluate the transform flow defined above with the input query, to get the embedding.
    query_vector = text_to_embedding.eval(query)
    # Run the query and get the results.
    with pool.connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT filename, text, embedding <=> %s::vector AS distance
                FROM {table_name} ORDER BY distance LIMIT %s
            """,
                (query_vector, top_k),
            )
            return [
                {"filename": row[0], "text": row[1], "score": 1.0 - row[2]}
                for row in cur.fetchall()
            ]


def _main() -> None:
    # Initialize the database connection pool.
    pool = ConnectionPool(os.getenv("COCOINDEX_DATABASE_URL"))

    azure_blob_text_embedding_flow.setup()
    update_stats = azure_blob_text_embedding_flow.update()
    print(update_stats)

    # Run queries in a loop to demonstrate the query capabilities.
    while True:
        query = input("Enter search query (or Enter to quit): ")
        if query == "":
            break
        # Run the query function with the database connection pool and the query.
        results = search(pool, query)
        print("\nSearch results:")
        for result in results:
            print(f"[{result['score']:.3f}] {result['filename']}")
            print(f"    {result['text']}")
            print("---")
        print()


if __name__ == "__main__":
    load_dotenv()
    cocoindex.init()
    _main()



================================================
FILE: examples/azure_blob_embedding/pyproject.toml
================================================
[project]
name = "azure-blob-text-embedding"
version = "0.1.0"
description = "Simple example for cocoindex: build embedding index based on Azure Blob Storage files."
requires-python = ">=3.11"
dependencies = ["cocoindex[embeddings]>=0.1.67", "python-dotenv>=1.0.1"]

[tool.setuptools]
packages = []



================================================
FILE: examples/azure_blob_embedding/.env.example
================================================
# Database Configuration
COCOINDEX_DATABASE_URL=postgres://cocoindex:cocoindex@localhost/cocoindex

# Azure Blob Storage Configuration (Public test container - ready to use!)
AZURE_STORAGE_ACCOUNT_NAME=testnamecocoindex1
AZURE_BLOB_CONTAINER_NAME=testpublic1
AZURE_BLOB_PREFIX=



================================================
FILE: examples/code_embedding/README.md
================================================
# Build real-time index for codebase
[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)

CocoIndex provides built-in support for code base chunking, using Tree-sitter to keep syntax boundary. In this example, we will build real-time index for codebase using CocoIndex.

We appreciate a star â­ at [CocoIndex Github](https://github.com/cocoindex-io/cocoindex) if this is helpful.

![Build embedding index for codebase](https://github.com/user-attachments/assets/6dc5ce89-c949-41d4-852f-ad95af163dbd)

[Tree-sitter](https://en.wikipedia.org/wiki/Tree-sitter_%28parser_generator%29) is a parser generator tool and an incremental parsing library. It is available in Rust ğŸ¦€ - [GitHub](https://github.com/tree-sitter/tree-sitter). CocoIndex has built-in Rust integration with Tree-sitter to efficiently parse code and extract syntax trees for various programming languages. Check out the list of supported languages [here](https://cocoindex.io/docs/ops/functions#splitrecursively) - in the `language` section.


## Tutorials
- Step by step tutorial - Check out the [blog](https://cocoindex.io/blogs/index-code-base-for-rag).
- Video tutorial - [Youtube](https://youtu.be/G3WstvhHO24?si=Bnxu67Ax5Lv8b-J2).

## Steps

### Indexing Flow
<p align='center'>
  <img width="434" alt="Screenshot 2025-05-19 at 10 14 36â€¯PM" src="https://github.com/user-attachments/assets/3a506034-698f-480a-b653-22184dae4e14" />
</p>

1. We will ingest CocoIndex codebase.
2. For each file, perform chunking (Tree-sitter) and then embedding.
3. We will save the embeddings and the metadata in Postgres with PGVector.

### Query:
We will match against user-provided text by a SQL query, reusing the embedding operation in the indexing flow.


## Prerequisite
[Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one.

## Run

- Install dependencies:
  ```bash
  pip install -e .
  ```

- Setup:

  ```bash
  cocoindex setup main.py
  ```

- Update index:

  ```bash
  cocoindex update main.py
  ```

- Run:

  ```bash
  python main.py
  ```

## CocoInsight
I used CocoInsight (Free beta now) to troubleshoot the index generation and understand the data lineage of the pipeline.
It just connects to your local CocoIndex server, with Zero pipeline data retention. Run the following command to start CocoInsight:

```
cocoindex server -ci main.py
```

Then open the CocoInsight UI at [https://cocoindex.io/cocoinsight](https://cocoindex.io/cocoinsight).

<img width="1305" alt="Chunking Visualization" src="https://github.com/user-attachments/assets/8e83b9a4-2bed-456b-83e5-b5381b28b84a" />



================================================
FILE: examples/code_embedding/main.py
================================================
from dotenv import load_dotenv
from psycopg_pool import ConnectionPool
from pgvector.psycopg import register_vector
from typing import Any
import cocoindex
import os
from numpy.typing import NDArray
import numpy as np


@cocoindex.op.function()
def extract_extension(filename: str) -> str:
    """Extract the extension of a filename."""
    return os.path.splitext(filename)[1]


@cocoindex.transform_flow()
def code_to_embedding(
    text: cocoindex.DataSlice[str],
) -> cocoindex.DataSlice[NDArray[np.float32]]:
    """
    Embed the text using a SentenceTransformer model.
    """
    # You can also switch to Voyage embedding model:
    #    return text.transform(
    #        cocoindex.functions.EmbedText(
    #            api_type=cocoindex.LlmApiType.VOYAGE,
    #            model="voyage-code-3",
    #        )
    #    )
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )


@cocoindex.flow_def(name="CodeEmbedding")
def code_embedding_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
) -> None:
    """
    Define an example flow that embeds files into a vector database.
    """
    data_scope["files"] = flow_builder.add_source(
        cocoindex.sources.LocalFile(
            path="../..",
            included_patterns=["*.py", "*.rs", "*.toml", "*.md", "*.mdx"],
            excluded_patterns=["**/.*", "target", "**/node_modules"],
        )
    )
    code_embeddings = data_scope.add_collector()

    with data_scope["files"].row() as file:
        file["extension"] = file["filename"].transform(extract_extension)
        file["chunks"] = file["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language=file["extension"],
            chunk_size=1000,
            min_chunk_size=300,
            chunk_overlap=300,
        )
        with file["chunks"].row() as chunk:
            chunk["embedding"] = chunk["text"].call(code_to_embedding)
            code_embeddings.collect(
                filename=file["filename"],
                location=chunk["location"],
                code=chunk["text"],
                embedding=chunk["embedding"],
                start=chunk["start"],
                end=chunk["end"],
            )

    code_embeddings.export(
        "code_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY,
            )
        ],
    )


def search(pool: ConnectionPool, query: str, top_k: int = 5) -> list[dict[str, Any]]:
    # Get the table name, for the export target in the code_embedding_flow above.
    table_name = cocoindex.utils.get_target_default_name(
        code_embedding_flow, "code_embeddings"
    )
    # Evaluate the transform flow defined above with the input query, to get the embedding.
    query_vector = code_to_embedding.eval(query)
    # Run the query and get the results.
    with pool.connection() as conn:
        register_vector(conn)
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT filename, code, embedding <=> %s AS distance, start, "end"
                FROM {table_name} ORDER BY distance LIMIT %s
            """,
                (query_vector, top_k),
            )
            return [
                {
                    "filename": row[0],
                    "code": row[1],
                    "score": 1.0 - row[2],
                    "start": row[3],
                    "end": row[4],
                }
                for row in cur.fetchall()
            ]


def _main() -> None:
    # Make sure the flow is built and up-to-date.
    stats = code_embedding_flow.update()
    print("Updated index: ", stats)

    # Initialize the database connection pool.
    pool = ConnectionPool(os.getenv("COCOINDEX_DATABASE_URL"))
    # Run queries in a loop to demonstrate the query capabilities.
    while True:
        query = input("Enter search query (or Enter to quit): ")
        if query == "":
            break
        # Run the query function with the database connection pool and the query.
        results = search(pool, query)
        print("\nSearch results:")
        for result in results:
            print(
                f"[{result['score']:.3f}] {result['filename']} (L{result['start']['line']}-L{result['end']['line']})"
            )
            print(f"    {result['code']}")
            print("---")
        print()


if __name__ == "__main__":
    load_dotenv()
    cocoindex.init()
    _main()



================================================
FILE: examples/code_embedding/pyproject.toml
================================================
[project]
name = "code-embedding"
version = "0.1.0"
description = "Simple example for cocoindex: build embedding index based on source code."
requires-python = ">=3.11"
dependencies = ["cocoindex[embeddings]>=0.1.67", "python-dotenv>=1.0.1"]

[tool.setuptools]
packages = []



================================================
FILE: examples/docs_to_knowledge_graph/README.md
================================================
# Build Real-Time Knowledge Graph For Documents with LLM

We will process a list of documents and use LLM to extract relationships between the concepts in each document.
We will generate two kinds of relationships:

1. Relationships between subjects and objects. E.g., "CocoIndex supports Incremental Processing"
2. Mentions of entities in a document. E.g., "core/basics.mdx" mentions `CocoIndex` and `Incremental Processing`.

You can find a step by step blog for this project [here](https://cocoindex.io/blogs/knowledge-graph-for-docs)

Please drop [Cocoindex on Github](https://github.com/cocoindex-io/cocoindex) a star to support us if you like our work. Thank you so much with a warm coconut hug ğŸ¥¥ğŸ¤—. [![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)

![example-explanation](https://github.com/user-attachments/assets/07ddbd60-106f-427f-b7cc-16b73b142d27)

## Prerequisite
*   [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one.
*   Install [Neo4j](https://cocoindex.io/docs/ops/targets#neo4j-dev-instance) or [Kuzu](https://cocoindex.io/docs/ops/targets#kuzu-dev-instance) if you don't have one.
    *   The example uses Neo4j by default for now. If you want to use Kuzu, find out the "SELECT ONE GRAPH DATABASE TO USE" section and switch the active branch.
*   [Configure your OpenAI API key](https://cocoindex.io/docs/ai/llm#openai).

## Documentation
You can read the official CocoIndex Documentation for Property Graph Targets [here](https://cocoindex.io/docs/ops/targets#property-graph-targets).

## Run

### Build the index

Install dependencies:

```bash
pip install -e .
```

Setup:

```bash
cocoindex setup main.py
```

Update index:

```bash
cocoindex update main.py
```

### Browse the knowledge graph

After the knowledge graph is built, you can explore the knowledge graph.

* If you're using Neo4j, you can open the explorer at [http://localhost:7474](http://localhost:7474), with username `neo4j` and password `cocoindex`.
* If you're using Kuzu, you can start a Kuzu explorer locally. See [Kuzu dev instance](https://cocoindex.io/docs/ops/targets#kuzu-dev-instance) for more details.

You can run the following Cypher query to get all relationships:

```cypher
MATCH p=()-->() RETURN p
```

<img width="1366" alt="neo4j-for-coco-docs" src="https://github.com/user-attachments/assets/3c8b6329-6fee-4533-9480-571399b57e57" />

## CocoInsight
I used CocoInsight (Free beta now) to troubleshoot the index generation and understand the data lineage of the pipeline.
It just connects to your local CocoIndex server, with Zero pipeline data retention. Run following command to start CocoInsight:

```bash
cocoindex server -ci main.py
```

And then open the url https://cocoindex.io/cocoinsight.

<img width="1430" alt="cocoinsight" src="https://github.com/user-attachments/assets/d5ada581-cceb-42bf-a949-132df674f3dd" />



================================================
FILE: examples/docs_to_knowledge_graph/main.py
================================================
"""
This example shows how to extract relationships from documents and build a knowledge graph.
"""

import dataclasses
import cocoindex

neo4j_conn_spec = cocoindex.add_auth_entry(
    "Neo4jConnection",
    cocoindex.targets.Neo4jConnection(
        uri="bolt://localhost:7687",
        user="neo4j",
        password="cocoindex",
    ),
)
kuzu_conn_spec = cocoindex.add_auth_entry(
    "KuzuConnection",
    cocoindex.targets.KuzuConnection(
        api_server_url="http://localhost:8123",
    ),
)

# SELECT ONE GRAPH DATABASE TO USE
# This example can use either Neo4j or Kuzu as the graph database.
# Please make sure only one branch is live and others are commented out.

# Use Neo4j
GraphDbSpec = cocoindex.targets.Neo4j
GraphDbConnection = cocoindex.targets.Neo4jConnection
GraphDbDeclaration = cocoindex.targets.Neo4jDeclaration
conn_spec = neo4j_conn_spec

# Use Kuzu
#  GraphDbSpec = cocoindex.targets.Kuzu
#  GraphDbConnection = cocoindex.targets.KuzuConnection
#  GraphDbDeclaration = cocoindex.targets.KuzuDeclaration
#  conn_spec = kuzu_conn_spec


@dataclasses.dataclass
class DocumentSummary:
    """Describe a summary of a document."""

    title: str
    summary: str


@dataclasses.dataclass
class Relationship:
    """
    Describe a relationship between two entities.
    Subject and object should be Core CocoIndex concepts only, should be nouns. For example, `CocoIndex`, `Incremental Processing`, `ETL`,  `Data` etc.
    """

    subject: str
    predicate: str
    object: str


@cocoindex.flow_def(name="DocsToKG")
def docs_to_kg_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
) -> None:
    """
    Define an example flow that extracts relationship from files and build knowledge graph.
    """
    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.LocalFile(
            path="../../docs/docs/core", included_patterns=["*.md", "*.mdx"]
        )
    )

    document_node = data_scope.add_collector()
    entity_relationship = data_scope.add_collector()
    entity_mention = data_scope.add_collector()

    with data_scope["documents"].row() as doc:
        # extract summary from document
        doc["summary"] = doc["content"].transform(
            cocoindex.functions.ExtractByLlm(
                llm_spec=cocoindex.LlmSpec(
                    # Supported LLM: https://cocoindex.io/docs/ai/llm
                    api_type=cocoindex.LlmApiType.OPENAI,
                    model="gpt-4o",
                ),
                output_type=DocumentSummary,
                instruction="Please summarize the content of the document.",
            )
        )
        document_node.collect(
            filename=doc["filename"],
            title=doc["summary"]["title"],
            summary=doc["summary"]["summary"],
        )

        # extract relationships from document
        doc["relationships"] = doc["content"].transform(
            cocoindex.functions.ExtractByLlm(
                llm_spec=cocoindex.LlmSpec(
                    # Supported LLM: https://cocoindex.io/docs/ai/llm
                    api_type=cocoindex.LlmApiType.OPENAI,
                    model="gpt-4o",
                ),
                output_type=list[Relationship],
                instruction=(
                    "Please extract relationships from CocoIndex documents. "
                    "Focus on concepts and ignore examples and code. "
                ),
            )
        )

        with doc["relationships"].row() as relationship:
            # relationship between two entities
            entity_relationship.collect(
                id=cocoindex.GeneratedField.UUID,
                subject=relationship["subject"],
                object=relationship["object"],
                predicate=relationship["predicate"],
            )
            # mention of an entity in a document, for subject
            entity_mention.collect(
                id=cocoindex.GeneratedField.UUID,
                entity=relationship["subject"],
                filename=doc["filename"],
            )
            # mention of an entity in a document, for object
            entity_mention.collect(
                id=cocoindex.GeneratedField.UUID,
                entity=relationship["object"],
                filename=doc["filename"],
            )

    # export to neo4j
    document_node.export(
        "document_node",
        GraphDbSpec(
            connection=conn_spec, mapping=cocoindex.targets.Nodes(label="Document")
        ),
        primary_key_fields=["filename"],
    )
    # Declare reference Node to reference entity node in a relationship
    flow_builder.declare(
        GraphDbDeclaration(
            connection=conn_spec,
            nodes_label="Entity",
            primary_key_fields=["value"],
        )
    )
    entity_relationship.export(
        "entity_relationship",
        GraphDbSpec(
            connection=conn_spec,
            mapping=cocoindex.targets.Relationships(
                rel_type="RELATIONSHIP",
                source=cocoindex.targets.NodeFromFields(
                    label="Entity",
                    fields=[
                        cocoindex.targets.TargetFieldMapping(
                            source="subject", target="value"
                        ),
                    ],
                ),
                target=cocoindex.targets.NodeFromFields(
                    label="Entity",
                    fields=[
                        cocoindex.targets.TargetFieldMapping(
                            source="object", target="value"
                        ),
                    ],
                ),
            ),
        ),
        primary_key_fields=["id"],
    )
    entity_mention.export(
        "entity_mention",
        GraphDbSpec(
            connection=conn_spec,
            mapping=cocoindex.targets.Relationships(
                rel_type="MENTION",
                source=cocoindex.targets.NodeFromFields(
                    label="Document",
                    fields=[cocoindex.targets.TargetFieldMapping("filename")],
                ),
                target=cocoindex.targets.NodeFromFields(
                    label="Entity",
                    fields=[
                        cocoindex.targets.TargetFieldMapping(
                            source="entity", target="value"
                        )
                    ],
                ),
            ),
        ),
        primary_key_fields=["id"],
    )



================================================
FILE: examples/docs_to_knowledge_graph/pyproject.toml
================================================
[project]
name = "manuals-to-kg"
version = "0.1.0"
description = "Simple example for cocoindex: extract triples from files and build knowledge graph."
requires-python = ">=3.11"
dependencies = ["cocoindex>=0.1.67"]

[tool.setuptools]
packages = []



================================================
FILE: examples/face_recognition/README.md
================================================
# Recognize faces in images and build embedding index
[![GitHub](https://img.shields.io/github/stars/cocoindex-io/cocoindex?color=5B5BD6)](https://github.com/cocoindex-io/cocoindex)


In this example, we will recognize faces in images and build embedding index.

We appreciate a star â­ at [CocoIndex Github](https://github.com/cocoindex-io/cocoindex) if this is helpful.

## Steps
### Indexing Flow

1. We will ingest a list of images.
2. For each image, we:
   - Extract faces from the image.
   - Compute embeddings for each face.
3. We will export to the following tables in Postgres with PGVector:
   - Filename, rect, embedding for each face.


## Prerequisite

1.  [Install Postgres](https://cocoindex.io/docs/getting_started/installation#-install-postgres) if you don't have one.

2.  dependencies:

    ```bash
    pip install -e .
    ```

## Run

Update index, which will also setup the tables at the first time:

```bash
cocoindex update --setup main.py
```

You can also run the command with `-L`, which will watch for file changes and update the index automatically.

```bash
cocoindex update --setup -L main.py
```

## CocoInsight
I used CocoInsight (Free beta now) to troubleshoot the index generation and understand the data lineage of the pipeline. It just connects to your local CocoIndex server, with zero pipeline data retention. Run following command to start CocoInsight:

```
cocoindex server -ci main.py
```

Then open the CocoInsight UI at [https://cocoindex.io/cocoinsight](https://cocoindex.io/cocoinsight).



================================================
FILE: examples/face_recognition/main.py
================================================
import cocoindex
import io
import dataclasses
import datetime
import typing

import face_recognition
from PIL import Image
import numpy as np


@dataclasses.dataclass
class ImageRect:
    min_x: int
    min_y: int
    max_x: int
    max_y: int


@dataclasses.dataclass
class FaceBase:
    """A face in an image."""

    rect: ImageRect
    image: bytes


MAX_IMAGE_WIDTH = 1280


@cocoindex.op.function(
    cache=True,
    behavior_version=1,
    gpu=True,
    arg_relationship=(cocoindex.op.ArgRelationship.RECTS_BASE_IMAGE, "content"),
)
def extract_faces(content: bytes) -> list[FaceBase]:
    """Extract the first pages of a PDF."""
    orig_img = Image.open(io.BytesIO(content)).convert("RGB")

    # The model is too slow on large images, so we resize them if too large.
    if orig_img.width > MAX_IMAGE_WIDTH:
        ratio = orig_img.width * 1.0 / MAX_IMAGE_WIDTH
        img = orig_img.resize(
            (MAX_IMAGE_WIDTH, int(orig_img.height / ratio)),
            resample=Image.Resampling.BICUBIC,
        )
    else:
        ratio = 1.0
        img = orig_img

    # Extract face locations.
    locs = face_recognition.face_locations(np.array(img), model="cnn")

    faces: list[FaceBase] = []
    for min_y, max_x, max_y, min_x in locs:
        rect = ImageRect(
            min_x=int(min_x * ratio),
            min_y=int(min_y * ratio),
            max_x=int(max_x * ratio),
            max_y=int(max_y * ratio),
        )

        # Crop the face and save it as a PNG.
        buf = io.BytesIO()
        orig_img.crop((rect.min_x, rect.min_y, rect.max_x, rect.max_y)).save(
            buf, format="PNG"
        )
        face = buf.getvalue()
        faces.append(FaceBase(rect, face))

    return faces


@cocoindex.op.function(cache=True, behavior_version=1, gpu=True)
def extract_face_embedding(
    face: bytes,
) -> cocoindex.Vector[cocoindex.Float32, typing.Literal[128]]:
    """Extract the embedding of a face."""
    img = Image.open(io.BytesIO(face)).convert("RGB")
    embedding = face_recognition.face_encodings(
        np.array(img),
        known_face_locations=[(0, img.width - 1, img.height - 1, 0)],
    )[0]
    return embedding


@cocoindex.flow_def(name="FaceRecognition")
def face_recognition_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
) -> None:
    """
    Define an example flow that embeds files into a vector database.
    """
    data_scope["images"] = flow_builder.add_source(
        cocoindex.sources.LocalFile(path="images", binary=True),
        refresh_interval=datetime.timedelta(seconds=10),
    )

    face_embeddings = data_scope.add_collector()

    with data_scope["images"].row() as image:
        # Extract faces
        image["faces"] = image["content"].transform(extract_faces)

        with image["faces"].row() as face:
            face["embedding"] = face["image"].transform(extract_face_embedding)

            # Collect embeddings
            face_embeddings.collect(
                filename=image["filename"],
                rect=face["rect"],
                embedding=face["embedding"],
            )

    face_embeddings.export(
        "face_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "rect"],
    )



================================================
FILE: examples/face_recognition/pyproject.toml
================================================
[project]
name = "cocoindex-face-recognition-example"
version = "0.1.0"
description = "Build index for papers with both metadata and content embeddings"
requires-python = ">=3.11"
dependencies = [
    "cocoindex>=0.1.71",
    "face-recognition>=1.3.0",
    "pillow>=10.0.0",
    "numpy>=1.26.0",
]

[tool.setuptools]
packages = []



================================================
FILE: examples/fastapi_server_docker/README.md
================================================
## Run docker container with a simple query endpoint via fastapi

In this example, we will build index for text embedding from local markdown files, and provide a simple query endpoint via fastapi.
We provide a simple docker container using docker compose to build pgvector17 along with a simple python fastapi script

We appreciate a star â­ at [CocoIndex Github](https://github.com/cocoindex-io/cocoindex) if this is helpful.


## Run locally without docker

In the `.env` file, use local Postgres URL

```
# For local testing
COCOINDEX_DATABASE_URL=postgres://cocoindex:cocoindex@localhost/cocoindex
```

- Install dependencies:

    ```bash
    pip install -e .
    ```

- Setup:

    ```bash
    cocoindex setup main.py
    ```

- Update index:

    ```bash
    cocoindex update main.py
    ```

- Run:

    ```bash
    uvicorn main:fastapi_app --reload --host 0.0.0.0 --port 8000
    ```

 ## Query the endpoint

    ```bash
    curl "http://localhost:8000/search?q=model&limit=3"
    ```


## Run Docker

In the `.env` file, use Docker Postgres URL

```
COCOINDEX_DATABASE_URL=postgres://cocoindex:cocoindex@coco_db:5436/cocoindex
```

Build the docker container via:
```bash
docker compose up --build
```

Test the endpoint:
```bash
curl "http://0.0.0.0:8080/search?q=model&limit=3"
```



================================================
FILE: examples/fastapi_server_docker/compose.yaml
================================================
services:
  coco_db:
    image: pgvector/pgvector:pg17
    restart: always
    environment:
      POSTGRES_USER: cocoindex
      POSTGRES_PASSWORD: cocoindex
      POSTGRES_DB: cocoindex
      POSTGRES_PORT: 5436
    ports:
      - "5436:5436"
    command: postgres -p 5436

  coco_api:
    build:
      context: .
    ports:
      - 8080:8080
    depends_on:
      - coco_db



================================================
FILE: examples/fastapi_server_docker/dockerfile
================================================
FROM python:3.11-slim

WORKDIR /app

# Install PostgreSQL client libraries
RUN apt-get update && apt-get install -y \
    libpq-dev \
    gcc \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .

RUN pip install -r requirements.txt

COPY . .

RUN cat .env

CMD ["sh", "-c", "echo yes | cocoindex setup main.py && cocoindex update main.py && python main.py"]



================================================
FILE: examples/fastapi_server_docker/main.py
================================================
import cocoindex
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Query
from fastapi import Request
from psycopg_pool import ConnectionPool
from contextlib import asynccontextmanager
import os


@cocoindex.transform_flow()
def text_to_embedding(
    text: cocoindex.DataSlice[str],
) -> cocoindex.DataSlice[list[float]]:
    """
    Embed the text using a SentenceTransformer model.
    This is a shared logic between indexing and querying.
    """
    return text.transform(
        cocoindex.functions.SentenceTransformerEmbed(
            model="sentence-transformers/all-MiniLM-L6-v2"
        )
    )


@cocoindex.flow_def(name="MarkdownEmbeddingFastApiExample")
def markdown_embedding_flow(
    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope
):
    """
    Define an example flow that embeds markdown files into a vector database.
    """
    data_scope["documents"] = flow_builder.add_source(
        cocoindex.sources.LocalFile(path="files")
    )
    doc_embeddings = data_scope.add_collector()

    with data_scope["documents"].row() as doc:
        doc["chunks"] = doc["content"].transform(
            cocoindex.functions.SplitRecursively(),
            language="markdown",
            chunk_size=2000,
            chunk_overlap=500,
        )

        with doc["chunks"].row() as chunk:
            chunk["embedding"] = text_to_embedding(chunk["text"])
            doc_embeddings.collect(
                filename=doc["filename"],
                location=chunk["location"],
                text=chunk["text"],
                embedding=chunk["embedding"],
            )

    doc_embeddings.export(
        "doc_embeddings",
        cocoindex.targets.Postgres(),
        primary_key_fields=["filename", "location"],
        vector_indexes=[
            cocoindex.VectorIndexDef(
                field_name="embedding",
                metric=cocoindex.VectorSimilarityMetric.COSINE_SIMILARITY,
            )
        ],
    )


def search(pool: ConnectionPool, query: str, top_k: int = 5):
    # Get the table name, for the export target in the text_embedding_flow above.
    table_name = cocoindex.utils.get_target_default_name(
        markdown_embedding_flow, "doc_embeddings"
    )
    # Evaluate the transform flow defined above with the input query, to get the embedding.
    query_vector = text_to_embedding.eval(query)
    # Run the query and get the results.
    with pool.connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT filename, text, embedding <=> %s::vector AS distance
                FROM {table_name} ORDER BY distance LIMIT %s
            """,
                (query_vector, top_k),
            )
            return [
                {"filename": row[0], "text": row[1], "score": 1.0 - row[2]}
                for row in cur.fetchall()
            ]


@asynccontextmanager
def lifespan(app: FastAPI):
    load_dotenv()
    cocoindex.init()
    pool = ConnectionPool(os.getenv("COCOINDEX_DATABASE_URL"))
    app.state.pool = pool
    try:
        yield
    finally:
        pool.close()


fastapi_app = FastAPI(lifespan=lifespan)


@fastapi_app.get("/search")
def search_endpoint(
    request: Request,
    q: str = Query(..., description="Search query"),
    limit: int = Query(5, description="Number of results"),
):
    pool = request.app.state.pool
    results = search(pool, q, limit)
    return {"results": results}


if __name__ == "__main__":
    uvicorn.run(fastapi_app, host="0.0.0.0", port=8080)



================================================
FILE: examples/fastapi_server_docker/requirements.txt
================================================
cocoindex[embeddings]>=0.1.63
python-dotenv>=1.0.1
fastapi==0.115.12
fastapi-cli==0.0.7
uvicorn==0.34.2
psycopg[binary]==3.2.6
psycopg_pool==3.2.6



================================================
FILE: examples/fastapi_server_docker/.dockerignore
================================================
.venv
__pycache__



================================================
FILE: examples/fastapi_server_docker/files/1810.04805v2.md
================================================
# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova

Google AI Language

{jacobdevlin,mingweichang,kentonl,kristout}@google.com

### Abstract

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models [(Peters et al.,](#page-10-0) [2018a;](#page-10-0) [Rad](#page-10-1)[ford et al.,](#page-10-1) [2018)](#page-10-1), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.

BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

### 1 Introduction

Language model pre-training has been shown to be effective for improving many natural language processing tasks [(Dai and Le,](#page-9-0) [2015;](#page-9-0) [Peters et al.,](#page-10-0) [2018a;](#page-10-0) [Radford et al.,](#page-10-1) [2018;](#page-10-1) [Howard and Ruder,](#page-9-1) [2018)](#page-9-1). These include sentence-level tasks such as natural language inference [(Bowman et al.,](#page-9-2) [2015;](#page-9-2) [Williams et al.,](#page-11-0) [2018)](#page-11-0) and paraphrasing [(Dolan](#page-9-3) [and Brockett,](#page-9-3) [2005)](#page-9-3), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level [(Tjong Kim Sang and](#page-10-2) [De Meulder,](#page-10-2) [2003;](#page-10-2) [Rajpurkar et al.,](#page-10-3) [2016)](#page-10-3).

There are two existing strategies for applying pre-trained language representations to downstream tasks: *feature-based* and *fine-tuning*. The feature-based approach, such as ELMo [(Peters](#page-10-0) [et al.,](#page-10-0) [2018a)](#page-10-0), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) [(Radford et al.,](#page-10-1) [2018)](#page-10-1), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning *all* pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.

We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer [(Vaswani et al.,](#page-10-4) [2017)](#page-10-4). Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.

In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a "masked language model" (MLM) pre-training objective, inspired by the Cloze task [(Taylor,](#page-10-5) [1953)](#page-10-5). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a "next sentence prediction" task that jointly pretrains text-pair representations. The contributions of our paper are as follows:

- We demonstrate the importance of bidirectional pre-training for language representations. Unlike [Radford et al.](#page-10-1) [(2018)](#page-10-1), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to [Peters et al.](#page-10-0) [(2018a)](#page-10-0), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.
- We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level *and* token-level tasks, outperforming many task-specific architectures.
- BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at [https://github.com/](https://github.com/google-research/bert) [google-research/bert](https://github.com/google-research/bert).

### 2 Related Work

There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.

#### 2.1 Unsupervised Feature-based Approaches

Learning widely applicable representations of words has been an active area of research for decades, including non-neural [(Brown et al.,](#page-9-4) [1992;](#page-9-4) [Ando and Zhang,](#page-9-5) [2005;](#page-9-5) [Blitzer et al.,](#page-9-6) [2006)](#page-9-6) and neural [(Mikolov et al.,](#page-10-6) [2013;](#page-10-6) [Pennington et al.,](#page-10-7) [2014)](#page-10-7) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch [(Turian et al.,](#page-10-8) [2010)](#page-10-8). To pretrain word embedding vectors, left-to-right language modeling objectives have been used [(Mnih](#page-10-9) [and Hinton,](#page-10-9) [2009)](#page-10-9), as well as objectives to discriminate correct from incorrect words in left and right context [(Mikolov et al.,](#page-10-6) [2013)](#page-10-6).

These approaches have been generalized to coarser granularities, such as sentence embeddings [(Kiros et al.,](#page-10-10) [2015;](#page-10-10) [Logeswaran and Lee,](#page-10-11) [2018)](#page-10-11) or paragraph embeddings [(Le and Mikolov,](#page-10-12) [2014)](#page-10-12). To train sentence representations, prior work has used objectives to rank candidate next sentences [(Jernite et al.,](#page-9-7) [2017;](#page-9-7) [Logeswaran and](#page-10-11) [Lee,](#page-10-11) [2018)](#page-10-11), left-to-right generation of next sentence words given a representation of the previous sentence [(Kiros et al.,](#page-10-10) [2015)](#page-10-10), or denoising autoencoder derived objectives [(Hill et al.,](#page-9-8) [2016)](#page-9-8).

ELMo and its predecessor [(Peters et al.,](#page-10-13) [2017,](#page-10-13) [2018a)](#page-10-0) generalize traditional word embedding research along a different dimension. They extract *context-sensitive* features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks [(Peters et al.,](#page-10-0) [2018a)](#page-10-0) including question answering [(Rajpurkar et al.,](#page-10-3) [2016)](#page-10-3), sentiment analysis [(Socher et al.,](#page-10-14) [2013)](#page-10-14), and named entity recognition [(Tjong Kim Sang and De Meulder,](#page-10-2) [2003)](#page-10-2). [Melamud et al.](#page-10-15) [(2016)](#page-10-15) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. [Fedus](#page-9-9) [et al.](#page-9-9) [(2018)](#page-9-9) shows that the cloze task can be used to improve the robustness of text generation models.

#### 2.2 Unsupervised Fine-tuning Approaches

As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text [(Col](#page-9-10)[lobert and Weston,](#page-9-10) [2008)](#page-9-10).

More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task [(Dai](#page-9-0) [and Le,](#page-9-0) [2015;](#page-9-0) [Howard and Ruder,](#page-9-1) [2018;](#page-9-1) [Radford](#page-10-1) [et al.,](#page-10-1) [2018)](#page-10-1). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT [(Radford et al.,](#page-10-1) [2018)](#page-10-1) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark [(Wang](#page-10-16) [et al.,](#page-10-16) [2018a)](#page-10-16). Left-to-right language model-

<span id="page-2-0"></span>Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).

ing and auto-encoder objectives have been used for pre-training such models [(Howard and Ruder,](#page-9-1) [2018;](#page-9-1) [Radford et al.,](#page-10-1) [2018;](#page-10-1) [Dai and Le,](#page-9-0) [2015)](#page-9-0).

#### 2.3 Transfer Learning from Supervised Data

There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference [(Conneau et al.,](#page-9-11) [2017)](#page-9-11) and machine translation [(McCann et al.,](#page-10-17) [2017)](#page-10-17). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet [(Deng et al.,](#page-9-12) [2009;](#page-9-12) [Yosinski et al.,](#page-11-1) [2014)](#page-11-1).

### <span id="page-2-5"></span>3 BERT

We introduce BERT and its detailed implementation in this section. There are two steps in our framework: *pre-training* and *fine-tuning*. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure [1](#page-2-0) will serve as a running example for this section.

A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.

Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in [Vaswani et al.](#page-10-4) [(2017)](#page-10-4) and released in the tensor2tensor library.[1](#page-2-1) Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to [Vaswani et al.](#page-10-4) [(2017)](#page-10-4) as well as excellent guides such as "The Annotated Transformer."[2](#page-2-2)

In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. [3](#page-2-3) We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).

BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.[4](#page-2-4)

<span id="page-2-3"></span><span id="page-2-2"></span><span id="page-2-1"></span><sup>1</sup> https://github.com/tensorflow/tensor2tensor 2 http://nlp.seas.harvard.edu/2018/04/03/attention.html 3 In all cases we set the feed-forward/filter size to be 4H,

<span id="page-2-4"></span>i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-

Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answeri) in one token sequence. Throughout this work, a "sentence" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A "sequence" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.

We use WordPiece embeddings [(Wu et al.,](#page-11-2) [2016)](#page-11-2) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure [1,](#page-2-0) we denote input embedding as E, the final hidden vector of the special [CLS] token as C âˆˆ R H, and the final hidden vector for the i th input token as Ti âˆˆ R H.

For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure [2.](#page-4-0)

### <span id="page-3-2"></span>3.1 Pre-training BERT

Unlike [Peters et al.](#page-10-0) [(2018a)](#page-10-0) and [Radford et al.](#page-10-1) [(2018)](#page-10-1), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure [1.](#page-2-0)

Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right *or* right-to-left, since bidirectional conditioning would allow each word to indirectly "see itself", and the model could trivially predict the target word in a multi-layered context.

In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a "masked LM" (MLM), although it is often referred to as a *Cloze* task in the literature [(Taylor,](#page-10-5) [1953)](#page-10-5). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders [(Vincent et al.,](#page-10-18) [2008)](#page-10-18), we only predict the masked words rather than reconstructing the entire input.

Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace "masked" words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix [C.2.](#page-15-0)

Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the *relationship* between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized *next sentence prediction* task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure [1,](#page-2-0) C is used for next sentence prediction (NSP).[5](#page-3-0) Despite its simplicity, we demonstrate in Section [5.1](#page-7-0) that pre-training towards this task is very beneficial to both QA and NLI. [6](#page-3-1)

former is often referred to as a "Transformer encoder" while the left-context-only version is referred to as a "Transformer decoder" since it can be used for text generation.

<span id="page-3-1"></span><span id="page-3-0"></span><sup>5</sup>The final model achieves 97%-98% accuracy on NSP.

<sup>6</sup>The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.

<span id="page-4-0"></span>Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.

The NSP task is closely related to representationlearning objectives used in [Jernite et al.](#page-9-7) [(2017)](#page-9-7) and [Logeswaran and Lee](#page-10-11) [(2018)](#page-10-11). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.

Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) [(Zhu et al.,](#page-11-3) [2015)](#page-11-3) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark [(Chelba et al.,](#page-9-13) [2013)](#page-9-13) in order to extract long contiguous sequences.

#### 3.2 Fine-tuning BERT

Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairsâ€”by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as [Parikh et al.](#page-10-19) [(2016)](#page-10-19); [Seo et al.](#page-10-20) [(2017)](#page-10-20). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes *bidirectional* cross attention between two sentences.

For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-âˆ… pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.

Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.[7](#page-4-1) We describe the task-specific details in the corresponding subsections of Section [4.](#page-4-2) More details can be found in Appendix [A.5.](#page-13-0)

### <span id="page-4-2"></span>4 Experiments

In this section, we present BERT fine-tuning results on 11 NLP tasks.

#### 4.1 GLUE

The General Language Understanding Evaluation (GLUE) benchmark [(Wang et al.,](#page-10-16) [2018a)](#page-10-16) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix [B.1.](#page-13-1)

To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section [3,](#page-2-5) and use the final hidden vector C âˆˆ R H corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights W âˆˆ R KÃ—H, where K is the number of labels. We compute a standard classification loss with C and W, i.e., log(softmax(CWT )).

- <span id="page-4-3"></span>8 See (10) in <https://gluebenchmark.com/faq>.
<span id="page-4-1"></span><sup>7</sup> For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.

<span id="page-5-1"></span>

| System           | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |
|------------------|-------------|------|------|-------|------|-------|------|------|---------|
|                  | 392k        | 363k | 108k | 67k   | 8.5k | 5.7k  | 3.5k | 2.5k | -       |
| Pre-OpenAI SOTA  | 80.6/80.1   | 66.1 | 82.3 | 93.2  | 35.0 | 81.0  | 86.0 | 61.7 | 74.0    |
| BiLSTM+ELMo+Attn | 76.4/76.1   | 64.8 | 79.8 | 90.4  | 36.0 | 73.3  | 84.9 | 56.8 | 71.0    |
| OpenAI GPT       | 82.1/81.4   | 70.3 | 87.4 | 91.3  | 45.4 | 80.0  | 82.3 | 56.0 | 75.1    |
| BERTBASE         | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |
| BERTLARGE        | 86.7/85.9   | 72.1 | 92.7 | 94.9  | 60.5 | 86.5  | 89.3 | 70.1 | 82.1    |

Table 1: GLUE Test results, scored by the evaluation server (<https://gluebenchmark.com/leaderboard>). The number below each task denotes the number of training examples. The "Average" column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.[8](#page-4-3) BERT and OpenAI GPT are singlemodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.

We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.[9](#page-5-0)

Results are presented in Table [1.](#page-5-1) Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard[10](#page-5-2), BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.

We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section [5.2.](#page-7-1)

#### 4.2 SQuAD v1.1

The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs [(Rajpurkar et al.,](#page-10-3) [2016)](#page-10-3). Given a question and a passage from Wikipedia containing the answer, the task is to predict the answer text span in the passage.

As shown in Figure [1,](#page-2-0) in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S âˆˆ R H and an end vector E âˆˆ R H during fine-tuning. The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph: Pi = e SÂ·Ti P j e SÂ·Tj . The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as SÂ·Ti + EÂ·Tj , and the maximum scoring span where j â‰¥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.

Table [2](#page-6-0) shows top leaderboard entries as well as results from top published systems [(Seo et al.,](#page-10-20) [2017;](#page-10-20) [Clark and Gardner,](#page-9-14) [2018;](#page-9-14) [Peters et al.,](#page-10-0) [2018a;](#page-10-0) [Hu et al.,](#page-9-15) [2018)](#page-9-15). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,[11](#page-5-3) and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA [(Joshi](#page-10-21) [et al.,](#page-10-21) [2017)](#page-10-21) befor fine-tuning on SQuAD.

Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine-

<span id="page-5-0"></span><sup>9</sup>The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.

<span id="page-5-2"></span><sup>10</sup>https://gluebenchmark.com/leaderboard

<span id="page-5-3"></span><sup>11</sup>QANet is described in [Yu et al.](#page-11-4) [(2018)](#page-11-4), but the system has improved substantially after publication.

<span id="page-6-0"></span>

| System                                   | Dev  |      | Test |      |  |  |  |  |
|------------------------------------------|------|------|------|------|--|--|--|--|
|                                          | EM   | F1   | EM   | F1   |  |  |  |  |
| Top Leaderboard Systems (Dec 10th, 2018) |      |      |      |      |  |  |  |  |
| Human                                    | -    | -    | 82.3 | 91.2 |  |  |  |  |
| #1 Ensemble - nlnet                      | -    | -    | 86.0 | 91.7 |  |  |  |  |
| #2 Ensemble - QANet                      | -    | -    | 84.5 | 90.5 |  |  |  |  |
| Published                                |      |      |      |      |  |  |  |  |
| BiDAF+ELMo (Single)                      | -    | 85.6 | -    | 85.8 |  |  |  |  |
| R.M. Reader (Ensemble)                   | 81.2 | 87.9 | 82.3 | 88.5 |  |  |  |  |
| Ours                                     |      |      |      |      |  |  |  |  |
| BERTBASE (Single)                        | 80.8 | 88.5 | -    | -    |  |  |  |  |
| BERTLARGE (Single)                       | 84.1 | 90.9 | -    | -    |  |  |  |  |
| BERTLARGE (Ensemble)                     | 85.8 | 91.8 | -    | -    |  |  |  |  |
| BERTLARGE (Sgl.+TriviaQA)                | 84.2 | 91.1 | 85.1 | 91.8 |  |  |  |  |
| BERTLARGE (Ens.+TriviaQA)                | 86.2 | 92.2 | 87.4 | 93.2 |  |  |  |  |

Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.

<span id="page-6-2"></span>

| System                                   | Dev  |      | Test |      |  |  |  |  |
|------------------------------------------|------|------|------|------|--|--|--|--|
|                                          | EM   | F1   | EM   | F1   |  |  |  |  |
| Top Leaderboard Systems (Dec 10th, 2018) |      |      |      |      |  |  |  |  |
| Human                                    | 86.3 | 89.0 | 86.9 | 89.5 |  |  |  |  |
| #1 Single - MIR-MRC (F-Net)              | -    | -    | 74.8 | 78.0 |  |  |  |  |
| #2 Single - nlnet                        | -    | -    | 74.2 | 77.1 |  |  |  |  |
| Published                                |      |      |      |      |  |  |  |  |
| unet (Ensemble)                          | -    | -    | 71.4 | 74.9 |  |  |  |  |
| SLQA+ (Single)                           | -    |      | 71.4 | 74.4 |  |  |  |  |
| Ours                                     |      |      |      |      |  |  |  |  |
| BERTLARGE (Single)                       | 78.7 | 81.9 | 80.0 | 83.1 |  |  |  |  |

Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components.

tuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin.[12](#page-6-1)

#### 4.3 SQuAD v2.0

The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.

We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: snull = SÂ·C + EÂ·C to the score of the best non-null span

<span id="page-6-3"></span>

| System                 | Dev  | Test |
|------------------------|------|------|
| ESIM+GloVe             | 51.9 | 52.7 |
| ESIM+ELMo              | 59.1 | 59.2 |
| OpenAI GPT             | -    | 78.0 |
| BERTBASE               | 81.6 | -    |
| BERTLARGE              | 86.6 | 86.3 |
| Human (expert)â€         | -    | 85.0 |
| Human (5 annotations)â€  | -    | 88.0 |

Table 4: SWAG Dev and Test accuracies. â€ Human performance is measured with 100 samples, as reported in the SWAG paper.

sË†i,j = maxjâ‰¥iSÂ·Ti + EÂ·Tj . We predict a non-null answer when sË†i,j > snull + Ï„ , where the threshold Ï„ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.

The results compared to prior leaderboard entries and top published work [(Sun et al.,](#page-10-22) [2018;](#page-10-22) [Wang et al.,](#page-11-5) [2018b)](#page-11-5) are shown in Table [3,](#page-6-2) excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.

#### 4.4 SWAG

The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference [(Zellers et al.,](#page-11-6) [2018)](#page-11-6). Given a sentence, the task is to choose the most plausible continuation among four choices.

When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer.

We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table [4.](#page-6-3) BERTLARGE outperforms the authors' baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.

### 5 Ablation Studies

In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional

<span id="page-6-1"></span><sup>12</sup>The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.

<span id="page-7-2"></span>

|              | Dev Set |       |       |       |       |  |  |
|--------------|---------|-------|-------|-------|-------|--|--|
| Tasks        | MNLI-m  | QNLI  | MRPC  | SST-2 | SQuAD |  |  |
|              | (Acc)   | (Acc) | (Acc) | (Acc) | (F1)  |  |  |
| BERTBASE     | 84.4    | 88.4  | 86.7  | 92.7  | 88.5  |  |  |
| No NSP       | 83.9    | 84.9  | 86.5  | 92.6  | 87.9  |  |  |
| LTR & No NSP | 82.1    | 84.3  | 77.5  | 92.1  | 77.8  |  |  |
| + BiLSTM     | 82.1    | 84.1  | 75.7  | 91.6  | 84.9  |  |  |

Table 5: Ablation over the pre-training tasks using the BERTBASE architecture. "No NSP" is trained without the next sentence prediction task. "LTR & No NSP" is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. "+ BiLSTM" adds a randomly initialized BiLSTM on top of the "LTR + No NSP" model during fine-tuning.

ablation studies can be found in Appendix [C.](#page-15-1)

### <span id="page-7-0"></span>5.1 Effect of Pre-training Tasks

We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERTBASE:

No NSP: A bidirectional model which is trained using the "masked LM" (MLM) but without the "next sentence prediction" (NSP) task.

LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.

We first examine the impact brought by the NSP task. In Table [5,](#page-7-2) we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing "No NSP" to "LTR & No NSP". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.

For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.

We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.

### <span id="page-7-1"></span>5.2 Effect of Model Size

In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.

Results on selected GLUE tasks are shown in Table [6.](#page-8-0) In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in [Vaswani et al.](#page-10-4) [(2017)](#page-10-4) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters [(Al-Rfou et al.,](#page-9-16) [2018)](#page-9-16). By contrast, BERTBASE contains 110M parameters and BERTLARGE contains 340M parameters.

It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table [6.](#page-8-0) However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. [Peters et al.](#page-10-23) [(2018b)](#page-10-23) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and [Melamud et al.](#page-10-15) [(2016)](#page-10-15) mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a featurebased approach â€” we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.

#### <span id="page-8-2"></span>5.3 Feature-based Approach with BERT

All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.

In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task [(Tjong Kim Sang](#page-10-2) [and De Meulder,](#page-10-2) [2003)](#page-10-2). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF

<span id="page-8-0"></span>

|                         | Hyperparams                      |                           |                                      | Dev Set Accuracy                     |                                      |                                      |  |  |
|-------------------------|----------------------------------|---------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--|--|
| #L                      | #H                               | #A                        | LM (ppl)                             | MNLI-m                               | MRPC                                 | SST-2                                |  |  |
| 3<br>6<br>6<br>12<br>12 | 768<br>768<br>768<br>768<br>1024 | 12<br>3<br>12<br>12<br>16 | 5.84<br>5.24<br>4.68<br>3.99<br>3.54 | 77.9<br>80.6<br>81.9<br>84.4<br>85.7 | 79.8<br>82.2<br>84.8<br>86.7<br>86.9 | 88.4<br>90.7<br>91.3<br>92.9<br>93.3 |  |  |
| 24                      | 1024                             | 16                        | 3.23                                 | 86.6                                 | 87.8                                 | 93.7                                 |  |  |

Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of attention heads. "LM (ppl)" is the masked LM perplexity of held-out training data.

<span id="page-8-1"></span>

| System                            | Dev F1 | Test F1 |
|-----------------------------------|--------|---------|
| ELMo (Peters et al., 2018a)       | 95.7   | 92.2    |
| CVT (Clark et al., 2018)          | -      | 92.6    |
| CSE (Akbik et al., 2018)          | -      | 93.1    |
| Fine-tuning approach              |        |         |
| BERTLARGE                         | 96.6   | 92.8    |
| BERTBASE                          | 96.4   | 92.4    |
| Feature-based approach (BERTBASE) |        |         |
| Embeddings                        | 91.0   | -       |
| Second-to-Last Hidden             | 95.6   | -       |
| Last Hidden                       | 94.9   | -       |
| Weighted Sum Last Four Hidden     | 95.9   | -       |
| Concat Last Four Hidden           | 96.1   | -       |
| Weighted Sum All 12 Layers        | 95.5   | -       |

Table 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.

layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.

To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers *without* fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.

Results are presented in Table [7.](#page-8-1) BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.

### 6 Conclusion

Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep *bidirectional* architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.

### References

- <span id="page-9-18"></span>Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In *Proceedings of the 27th International Conference on Computational Linguistics*, pages 1638â€“1649.
- <span id="page-9-16"></span>Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. *arXiv preprint arXiv:1808.04444*.
- <span id="page-9-5"></span>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. *Journal of Machine Learning Research*, 6(Nov):1817â€“1853.
- <span id="page-9-22"></span>Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In *TAC*. NIST.
- <span id="page-9-6"></span>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In *Proceedings of the 2006 conference on empirical methods in natural language processing*, pages 120â€“128. Association for Computational Linguistics.
- <span id="page-9-2"></span>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In *EMNLP*. Association for Computational Linguistics.
- <span id="page-9-4"></span>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. *Computational linguistics*, 18(4):467â€“479.
- <span id="page-9-21"></span>Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. [Semeval-2017](https://doi.org/10.18653/v1/S17-2001) [task 1: Semantic textual similarity multilingual and](https://doi.org/10.18653/v1/S17-2001) [crosslingual focused evaluation.](https://doi.org/10.18653/v1/S17-2001) In *Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)*, pages 1â€“14, Vancouver, Canada. Association for Computational Linguistics.
- <span id="page-9-13"></span>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. *arXiv preprint arXiv:1312.3005*.
- <span id="page-9-20"></span>Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. [Quora question pairs.](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs)
- <span id="page-9-14"></span>Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In *ACL*.
- <span id="page-9-17"></span>Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1914â€“ 1925.
- <span id="page-9-10"></span>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In *Proceedings of the 25th international conference on Machine learning*, pages 160â€“167. ACM.
- <span id="page-9-11"></span>Alexis Conneau, Douwe Kiela, Holger Schwenk, LoÂ¨Ä±c Barrault, and Antoine Bordes. 2017. [Supervised](https://www.aclweb.org/anthology/D17-1070) [learning of universal sentence representations from](https://www.aclweb.org/anthology/D17-1070) [natural language inference data.](https://www.aclweb.org/anthology/D17-1070) In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pages 670â€“680, Copenhagen, Denmark. Association for Computational Linguistics.
- <span id="page-9-0"></span>Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In *Advances in neural information processing systems*, pages 3079â€“3087.
- <span id="page-9-12"></span>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In *CVPR09*.
- <span id="page-9-3"></span>William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In *Proceedings of the Third International Workshop on Paraphrasing (IWP2005)*.
- <span id="page-9-9"></span>William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the . *arXiv preprint arXiv:1801.07736*.
- <span id="page-9-19"></span>Dan Hendrycks and Kevin Gimpel. 2016. [Bridging](http://arxiv.org/abs/1606.08415) [nonlinearities and stochastic regularizers with gaus](http://arxiv.org/abs/1606.08415)[sian error linear units.](http://arxiv.org/abs/1606.08415) *CoRR*, abs/1606.08415.
- <span id="page-9-8"></span>Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*. Association for Computational Linguistics.
- <span id="page-9-1"></span>Jeremy Howard and Sebastian Ruder. 2018. [Universal](http://arxiv.org/abs/1801.06146) [language model fine-tuning for text classification.](http://arxiv.org/abs/1801.06146) In *ACL*. Association for Computational Linguistics.
- <span id="page-9-15"></span>Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In *IJCAI*.
- <span id="page-9-7"></span>Yacine Jernite, Samuel R. Bowman, and David Sontag. 2017. [Discourse-based objectives for fast un](http://arxiv.org/abs/1705.00557)[supervised sentence representation learning.](http://arxiv.org/abs/1705.00557) *CoRR*, abs/1705.00557.
- <span id="page-10-21"></span>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In *ACL*.
- <span id="page-10-10"></span>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In *Advances in neural information processing systems*, pages 3294â€“3302.
- <span id="page-10-12"></span>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In *International Conference on Machine Learning*, pages 1188â€“1196.
- <span id="page-10-24"></span>Hector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In *Aaai spring symposium: Logical formalizations of commonsense reasoning*, volume 46, page 47.
- <span id="page-10-11"></span>Lajanugen Logeswaran and Honglak Lee. 2018. [An](https://openreview.net/forum?id=rJvJXZb0W) [efficient framework for learning sentence represen](https://openreview.net/forum?id=rJvJXZb0W)[tations.](https://openreview.net/forum?id=rJvJXZb0W) In *International Conference on Learning Representations*.
- <span id="page-10-17"></span>Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In *NIPS*.
- <span id="page-10-15"></span>Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In *CoNLL*.
- <span id="page-10-6"></span>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In *Advances in Neural Information Processing Systems 26*, pages 3111â€“3119. Curran Associates, Inc.
- <span id="page-10-9"></span>Andriy Mnih and Geoffrey E Hinton. 2009. [A scal](http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf)[able hierarchical distributed language model.](http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf) In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, *Advances in Neural Information Processing Systems 21*, pages 1081â€“1088. Curran Associates, Inc.
- <span id="page-10-19"></span>Ankur P Parikh, Oscar Tackstr Â¨ om, Dipanjan Das, and Â¨ Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In *EMNLP*.
- <span id="page-10-7"></span>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [Glove: Global vectors for](http://www.aclweb.org/anthology/D14-1162) [word representation.](http://www.aclweb.org/anthology/D14-1162) In *Empirical Methods in Natural Language Processing (EMNLP)*, pages 1532â€“ 1543.
- <span id="page-10-13"></span>Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In *ACL*.
- <span id="page-10-0"></span>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In *NAACL*.
- <span id="page-10-23"></span>Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1499â€“1509.
- <span id="page-10-1"></span>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.
- <span id="page-10-3"></span>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 2383â€“2392.
- <span id="page-10-20"></span>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In *ICLR*.
- <span id="page-10-14"></span>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In *Proceedings of the 2013 conference on empirical methods in natural language processing*, pages 1631â€“1642.
- <span id="page-10-22"></span>Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. *arXiv preprint arXiv:1810.06638*.
- <span id="page-10-5"></span>Wilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. *Journalism Bulletin*, 30(4):415â€“433.
- <span id="page-10-2"></span>Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In *CoNLL*.
- <span id="page-10-8"></span>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In *Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics*, ACL '10, pages 384â€“394.
- <span id="page-10-4"></span>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Advances in Neural Information Processing Systems*, pages 6000â€“6010.
- <span id="page-10-18"></span>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In *Proceedings of the 25th international conference on Machine learning*, pages 1096â€“1103. ACM.
- <span id="page-10-16"></span>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform

for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353â€“355.

- <span id="page-11-5"></span>Wei Wang, Ming Yan, and Chen Wu. 2018b. Multigranularity hierarchical attention fusion networks for reading comp