Directory structure:
└── muvon-octocode/
    ├── README.md
    ├── Cargo.toml
    ├── CHANGELOG.md
    ├── Dockerfile
    ├── INSTALL.md
    ├── install.sh
    ├── INSTRUCTIONS.md
    ├── LICENSE
    ├── Makefile
    ├── rust-toolchain.toml
    ├── rustfmt.toml
    ├── .dockerignore
    ├── .editorconfig
    ├── .noindex
    ├── .pre-commit-config.yaml
    ├── config-templates/
    │   └── default.toml
    ├── doc/
    │   ├── ADVANCED_USAGE.md
    │   ├── API_KEYS.md
    │   ├── ARCHITECTURE.md
    │   ├── COMMANDS.md
    │   ├── CONFIGURATION.md
    │   ├── CONTRIBUTING.md
    │   ├── GETTING_STARTED.md
    │   ├── LSP_INTEGRATION.md
    │   ├── MCP_INTEGRATION.md
    │   ├── MEMORY_SYSTEM.md
    │   ├── PERFORMANCE.md
    │   └── RELEASE_MANAGEMENT.md
    ├── scripts/
    │   ├── format.sh
    │   ├── install-completions.sh
    │   └── test-completions.sh
    ├── src/
    │   ├── config.rs
    │   ├── constants.rs
    │   ├── lib.rs
    │   ├── main.rs
    │   ├── reranker.rs
    │   ├── state.rs
    │   ├── storage.rs
    │   ├── watcher_config.rs
    │   ├── commands/
    │   │   ├── clear.rs
    │   │   ├── commit.rs
    │   │   ├── config.rs
    │   │   ├── format.rs
    │   │   ├── graphrag.rs
    │   │   ├── index.rs
    │   │   ├── logs.rs
    │   │   ├── mcp.rs
    │   │   ├── mcp_proxy.rs
    │   │   ├── memory.rs
    │   │   ├── mod.rs
    │   │   ├── models.rs
    │   │   ├── output_format.rs
    │   │   ├── release.rs
    │   │   ├── review.rs
    │   │   ├── search.rs
    │   │   ├── view.rs
    │   │   ├── watch.rs
    │   │   └── format/
    │   │       └── utils.rs
    │   ├── embedding/
    │   │   ├── mod.rs
    │   │   ├── tests.rs
    │   │   ├── types.rs
    │   │   └── provider/
    │   │       ├── fastembed.rs
    │   │       ├── google.rs
    │   │       ├── huggingface.rs
    │   │       ├── jina.rs
    │   │       ├── mod.rs
    │   │       ├── openai.rs
    │   │       └── voyage.rs
    │   ├── indexer/
    │   │   ├── batch_processor.rs
    │   │   ├── code_region_extractor.rs
    │   │   ├── differential_processor.rs
    │   │   ├── file_processor.rs
    │   │   ├── file_utils.rs
    │   │   ├── git_utils.rs
    │   │   ├── graph_optimization.rs
    │   │   ├── markdown_processor.rs
    │   │   ├── mod.rs
    │   │   ├── path_utils.rs
    │   │   ├── render_utils.rs
    │   │   ├── search.rs
    │   │   ├── signature_extractor.rs
    │   │   ├── text_processing.rs
    │   │   ├── graphrag/
    │   │   │   ├── ai.rs
    │   │   │   ├── builder.rs
    │   │   │   ├── database.rs
    │   │   │   ├── mod.rs
    │   │   │   ├── relationships.rs
    │   │   │   ├── tests.rs
    │   │   │   ├── types.rs
    │   │   │   └── utils.rs
    │   │   └── languages/
    │   │       ├── bash.rs
    │   │       ├── cpp.rs
    │   │       ├── css.rs
    │   │       ├── go.rs
    │   │       ├── javascript.rs
    │   │       ├── json.rs
    │   │       ├── markdown.rs
    │   │       ├── mod.rs
    │   │       ├── php.rs
    │   │       ├── python.rs
    │   │       ├── resolution_utils.rs
    │   │       ├── ruby.rs
    │   │       ├── rust.rs
    │   │       ├── svelte.rs
    │   │       └── typescript.rs
    │   ├── mcp/
    │   │   ├── graphrag.rs
    │   │   ├── logging.rs
    │   │   ├── memory.rs
    │   │   ├── mod.rs
    │   │   ├── proxy.rs
    │   │   ├── semantic_code.rs
    │   │   ├── server.rs
    │   │   ├── types.rs
    │   │   └── lsp/
    │   │       ├── client.rs
    │   │       ├── mod.rs
    │   │       ├── protocol.rs
    │   │       ├── provider.rs
    │   │       └── tools.rs
    │   ├── memory/
    │   │   ├── formatting.rs
    │   │   ├── git_utils.rs
    │   │   ├── manager.rs
    │   │   ├── mod.rs
    │   │   ├── store.rs
    │   │   └── types.rs
    │   ├── store/
    │   │   ├── batch_converter.rs
    │   │   ├── debug.rs
    │   │   ├── graphrag.rs
    │   │   ├── metadata.rs
    │   │   ├── mod.rs
    │   │   ├── table_ops.rs
    │   │   └── vector_optimizer.rs
    │   └── utils/
    │       ├── mod.rs
    │       └── path.rs
    ├── website/
    │   ├── index.html
    │   ├── script.js
    │   └── styles.css
    ├── .cargo/
    │   └── config.toml
    └── .github/
        └── workflows/
            ├── ci.yml
            ├── dependencies.yml
            ├── deploy-website.yml
            └── release.yml


Files Content:

(Files content cropped to 300k characters, download full ingest to see more)
================================================
FILE: README.md
================================================
# Octocode - Intelligent Code Indexer and Graph Builder

**© 2025 Muvon Un Limited (Hong Kong)** | [Website](https://muvon.io) | [Product Page](https://octocode.muvon.io)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Rust](https://img.shields.io/badge/rust-1.70%2B-orange.svg)](https://www.rust-lang.org)

## 🚀 Overview

Octocode is a powerful code indexer and semantic search engine that builds intelligent knowledge graphs of your codebase. It combines advanced AI capabilities with local-first design to provide deep code understanding, relationship mapping, and intelligent assistance for developers.

## ✨ Key Features

- **🔍 Semantic Code Search** - Natural language queries with multi-query support
- **🕸️ Knowledge Graph (GraphRAG)** - Automatic relationship discovery between files
- **🌐 Multi-Language Support** - Rust, Python, JavaScript, TypeScript, Go, PHP, C++, Ruby, and more
- **🧠 AI-Powered Features** - Smart commits, code review, memory system with semantic search
- **🔌 MCP Server Integration** - Built-in Model Context Protocol server with LSP support
- **⚡ Performance & Flexibility** - Intelligent LanceDB optimization, local/cloud embedding models

## 📦 Quick Installation

```bash
# Universal install script (Linux, macOS, Windows)
curl -fsSL https://raw.githubusercontent.com/Muvon/octocode/master/install.sh | sh
```

**Alternative methods:**
- Download from [GitHub Releases](https://github.com/Muvon/octocode/releases)
- Install with Cargo: `cargo install --git https://github.com/Muvon/octocode`

For detailed installation instructions, see [Installation Guide](INSTALL.md).

## 🔑 API Keys Setup

**⚠️ Required for functionality:**

```bash
# Required: Voyage AI (embeddings) - 200M free tokens/month
export VOYAGE_API_KEY="your-voyage-api-key"

# Optional: OpenRouter (LLM features)
export OPENROUTER_API_KEY="your-openrouter-api-key"
```

**Get your free API keys:**
- **Voyage AI**: [Get free API key](https://www.voyageai.com/) (200M tokens/month free)
- **OpenRouter**: [Get API key](https://openrouter.ai/) (optional, for AI features)

## 🚀 Quick Start

```bash
# 1. Index your codebase
octocode index

# 2. Search with natural language
octocode search "HTTP request handling"

# 3. Multi-query search for comprehensive results
octocode search "authentication" "middleware"

# 4. AI-powered git workflow
octocode commit --all

# 5. Start MCP server for AI assistants
octocode mcp --path /path/to/your/project
```

## 📚 Complete Documentation

📖 **Quick Navigation**

- **[Installation Guide](INSTALL.md)** - Detailed installation methods and building from source
- **[Getting Started](doc/GETTING_STARTED.md)** - First steps and basic workflow
- **[API Keys Setup](doc/API_KEYS.md)** - Complete API configuration guide
- **[Configuration Guide](doc/CONFIGURATION.md)** - Configuration system, templates, and customization
- **[Commands Reference](doc/COMMANDS.md)** - Complete command reference with examples
- **[Advanced Usage](doc/ADVANCED_USAGE.md)** - Advanced features and workflows
- **[MCP Integration](doc/MCP_INTEGRATION.md)** - Model Context Protocol server setup
- **[LSP Integration](doc/LSP_INTEGRATION.md)** - Language Server Protocol integration
- **[Memory System](doc/MEMORY_SYSTEM.md)** - Memory management and semantic search
- **[Release Management](doc/RELEASE_MANAGEMENT.md)** - AI-powered release automation
- **[Architecture](doc/ARCHITECTURE.md)** - Core components and system design
- **[Performance](doc/PERFORMANCE.md)** - Performance metrics and optimization
- **[Contributing](doc/CONTRIBUTING.md)** - Development setup and contribution guidelines

## 🌐 Supported Languages

| Language | Extensions | Features |
|----------|------------|----------|
| **Rust** | `.rs` | Full AST parsing, pub/use detection, module structure |
| **Python** | `.py` | Import/class/function extraction, docstring parsing |
| **JavaScript** | `.js`, `.jsx` | ES6 imports/exports, function declarations |
| **TypeScript** | `.ts`, `.tsx` | Type definitions, interface extraction |
| **Go** | `.go` | Package/import analysis, struct/interface parsing |
| **PHP** | `.php` | Class/function extraction, namespace support |
| **C++** | `.cpp`, `.hpp`, `.h` | Include analysis, class/function extraction |
| **Ruby** | `.rb` | Class/module extraction, method definitions |
| **JSON** | `.json` | Structure analysis, key extraction |
| **Bash** | `.sh`, `.bash` | Function and variable extraction |
| **Markdown** | `.md` | Document section indexing, header extraction |

## 🔒 Privacy & Security

- **🏠 Local-first option**: FastEmbed and SentenceTransformer run entirely offline (macOS only)
- **🔑 Secure storage**: API keys stored locally, environment variables supported
- **📁 Respects .gitignore**: Never indexes sensitive files or directories
- **🛡️ MCP security**: Server runs locally with no external network access for search
- **🌐 Cloud embeddings**: Voyage AI and other providers process only file metadata, not source code

## 🤝 Support & Community

- **🐛 Issues**: [GitHub Issues](https://github.com/Muvon/octocode/issues)
- **📧 Email**: [opensource@muvon.io](mailto:opensource@muvon.io)
- **🏢 Company**: Muvon Un Limited (Hong Kong)

## ⚖️ License

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

---

**Built with ❤️ by the Muvon team in Hong Kong**



================================================
FILE: Cargo.toml
================================================
[package]
name = "octocode"
version = "0.8.0"
edition = "2021"
rust-version = "1.82"
authors = ["Muvon Un Limited <opensource@muvon.io>"]
description = "AI-powered code indexer with semantic search, GraphRAG knowledge graphs, and MCP server for multi-language codebases"
homepage = "https://octocode.muvon.io"
repository = "https://github.com/muvon/octocode"
documentation = "https://docs.rs/octocode"
license = "Apache-2.0"
keywords = ["ai", "semantic-search", "code-analysis", "graphrag", "mcp-server"]
categories = ["development-tools", "command-line-utilities", "parsing", "text-processing", "algorithms"]
readme = "README.md"
exclude = [
    "tests/fixtures/*",
    "examples/*",
    ".github/*",
    "docs/*",
    "scripts/*",
    "*.md",
    "install.sh"
]

[features]
default = ["fastembed", "huggingface"]
fastembed = ["dep:fastembed"]
huggingface = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:tokenizers", "dep:hf-hub"]

# Optimized release profile for static linking
[profile.release]
lto = true              # Enable Link Time Optimization
codegen-units = 1       # Reduce parallel code generation units for better optimization
panic = "abort"         # Abort on panic for smaller binary size
strip = true            # Strip symbols from binary
opt-level = "z"         # Optimize for size
overflow-checks = false # Disable overflow checks in release mode

# Cross-compilation configuration is in .cargo/config.toml

[dependencies]
tokio = { version = "1.45.1", features = ["rt-multi-thread", "macros", "time", "sync", "io-util", "io-std", "process"] }
ignore = "0.4.23"
parking_lot = "0.12.4"
lance-table = { version = "0.30.0", default-features = false, features = ["lazy_static"] }
lance = { version = "0.30.0", default-features = false, features = [] }
lancedb = { version = "0.21.0", default-features = false }
arrow = { version = "55.2.0", default-features = false, features = ["prettyprint"] }
serde = { version = "1.0.219", features = ["derive"] }
chrono = { version = "0.4", default-features = false, features = ["serde", "clock"] }
async-trait = "0.1"
tree-sitter = "0.25.6"
tree-sitter-php = "0.23.11"
tree-sitter-rust = "0.23.2"
tree-sitter-python = "0.23.6"
tree-sitter-javascript = "0.23.1"
tree-sitter-json = "0.24.8"
tree-sitter-go = "0.23.4"
tree-sitter-cpp = "0.23.4"
tree-sitter-bash = "0.25.0"
tree-sitter-css = "0.23.2"

uuid = { version = "1.17.0", default-features = false, features = ["v4"] }
tree-sitter-typescript = "0.23.2"
tree-sitter-ruby = "0.23.1"
tree-sitter-svelte-ng = "1.0.2"
reqwest = { version = "0.12.20", features = ["json", "rustls-tls"], default-features = false }
anyhow = "1.0.98"
serde_json = "1.0.140"
sha2 = "0.10.9"
clap = { version = "4.5.40", features = ["derive"] }
clap_complete = "4.5.54"
notify = { version = "8.0.0", default-features = false, features = ["crossbeam-channel", "macos_fsevent"] }
notify-debouncer-mini = "0.6.0"
fastembed = { version = "4.9.1", optional = true }
toml = "0.8.23"
lazy_static = "1.5.0"
futures = { version = "0.3.31", default-features = false, features = ["std"] }
globset = { version = "0.4.16", default-features = false }
regex = { version = "1.11.1", default-features = false, features = ["std"] }
dirs = "6.0.0"
# Candle dependencies for HuggingFace support (optional)
candle-core = { version = "0.9.1", optional = true }
candle-nn = { version = "0.9.1", optional = true }
candle-transformers = { version = "0.9.1", optional = true }
tokenizers = { version = "0.21.2", optional = true }
hf-hub = { version = "0.3.2", features = ["tokio"], optional = true }
# EditorConfig parsing and formatting
ec4rs = "1.2.0"
tracing = "0.1.41"
tracing-subscriber = { version = "0.3.19", features = ["env-filter", "json"] }
tracing-appender = "0.2.3"
tiktoken-rs = "0.7.0"
# LSP integration dependencies
lsp-types = "0.97.0"
url = "2.5.4"
dotenvy = "0.15"

[profile.dev]
opt-level = 1          # Basic optimizations without slowing compilation too much
debug = true           # Keep debug symbols for backtraces
debug-assertions = true # Keep runtime checks
overflow-checks = true  # Keep overflow checks
lto = false            # Disable Link Time Optimization to speed up builds
codegen-units = 256    # Use more codegen units for faster parallel compilation
incremental = true     # Enable incremental compilation
rpath = false          # Disable runtime search path



================================================
FILE: CHANGELOG.md
================================================
# Changelog

## [0.8.0] - 2025-07-11

### 📋 Release Summary

This release introduces enhanced multi-language import resolution and expanded semantic graph operations for improved code indexing and search (f53d5bc9, 10841fbe, c1e4e5f7). New AI architectural analysis settings and additional embedding providers, including OpenAI and Google models, offer greater flexibility and accuracy (386526a7, 3aec68da, e920ae56). Several bug fixes and refinements improve cross-platform stability, indexing reliability, and relationship extraction, enhancing overall system robustness (e6340d09, 2efe3df3, aed122e3, 8b648f71).


### ✨ New Features & Enhancements

- **graphrag**: add import resolver for multi-language imports `f53d5bc9`
- **graphrag**: store relationships incrementally during processing `10841fbe`
- **graphrag**: expand GraphRAG with node and path operations `c1e4e5f7`
- **config**: add AI architectural analysis settings and prompts `386526a7`
- **embedding**: add OpenAI as new embeddings provider `3aec68da`
- **jina**: add new jina embedding models with dimensions `bd0d3146`
- **embedding**: rename get_model_dimension and add Google models map `e726cdda`
- **embedding**: add modular Google embedding provider and docs `e920ae56`
- **models**: add dynamic model discovery and CLI commands `5beba7c0`
- **markdown**: add signature extraction for markdown files `b27a50dc`

### 🔧 Improvements & Optimizations

- **rust**: unify file existence checks with helper method `0385c5e8`
- **import**: remove legacy GraphRAG import resolver module `e8e8740f`
- **models**: unify models list output format with dimensions `123be1db`
- **fastembed**: use fully qualified model names in mapping and l... `e6f1647a`
- **embedding**: unify huggingface prefix and improve model docs `4709ce2d`
- **huggingface**: rename sentence transformers and fix URL resol... `ac193c4b`
- **watch**: remove deprecated ignore patterns and config field `66df3c6c`

### 🐛 Bug Fixes & Stability

- **config**: remove deprecated top_k in favor of max_results `e6340d09`
- **indexer**: prevent infinite recursion by disabling symlink follow `2efe3df3`
- **test**: normalize paths to fix Windows test failures `83b415cd`
- **indexer**: normalize paths and fix Windows tests for parent-child ... `510047b2`
- **graphrag**: prevent duplicate nodes and clean up stale data `aed122e3`
- **config**: correct confidence_threshold value in tests `b014ab7a`
- **graphrag**: apply AI descriptions before node persistence `93f40ddf`
- **graphrag**: improve AI relationship fetching criteria and logging `8b648f71`
- **graphrag**: correctly update nodes with AI-generated descriptions `3ee81fd6`
- **graphrag**: improve import/export extraction and relationship dete... `12d199a0`
- **clear**: skip memory tables when clearing all data `b8075122`
- **test**: simplify embedding provider tests to assume Voyage only `a6383b36`
- **graphrag**: extract and display node relationships with correct in... `34bf6c3e`
- **graphrag**: correct node indexing and display relationships `e93ea0cb`
- **graphrag**: fix incremental indexing and cleanup for GraphRAG data `abc32e2b`
- **graphrag**: fix graph_nodes persistence by unifying storage method `a9ab31af`
- **store**: update file metadata using LanceDB UpdateBuilder API `e1d39b9a`
- **search**: use config default for similarity threshold if unset `103ef056`
- **website**: correct code block indentation in index.html `41676c9f`

### 📚 Documentation & Examples

- add import resolution and safe discovery details in docs `ded521dd`
- **cli**: update model commands and GraphRAG usage details `57abf90d`
- **readme**: remove accidental OpenAI promo from README `0aa869c6`
- **commands**: expand Voyage models list and add OpenAI provider `1bff845a`
- **mcp**: clarify view_signatures tool description and add Markdown ... `ea64de3e`

### 🔄 Other Changes

- **embedding**: add FastEmbed provider creation and validation tests `fb4b38ca`
- **graphrag**: add edge case tests for import resolution `d84f90a5`
- **graphrag**: upgrade config with LLM batching and fallback options `ea884005`
- **release**: clarify purpose of crates upload release `86a490ad`

### 📊 Release Summary

**Total commits**: 45 across 5 categories

✨ **10** new features - *Enhanced functionality*
🔧 **7** improvements - *Better performance & code quality*
🐛 **19** bug fixes - *Improved stability*
📚 **5** documentation updates - *Better developer experience*
🔄 **4** other changes - *Maintenance & tooling*

## [0.7.1] - 2025-06-28

### 📋 Release Summary

This release is made exclusively for uploading the proper version to crates that had broken Rust indexing with an incorrect tree-sitter parser version.


### 📚 Documentation & Examples

- **ci**: fix markdown code block formatting in release workflow `39c0361e`

### 📊 Release Summary

**Total commits**: 1 across 1 categories

📚 **1** documentation update - *Better developer experience*

## [0.7.0] - 2025-06-27

### 📋 Release Summary

This release introduces enhanced search capabilities with distance-based result sorting and improved input handling, alongside streamlined environment configuration and automated changelog generation (97af3e9c, ea78232f, f3c50bbc, 9fab3f2c). Several bug fixes improve search accuracy, error handling, and repository detection (fa171584, 057c7832, e950d9c4). Additional updates include dependency upgrades, codebase optimizations, documentation fixes, and new website deployment.


### ✨ New Features & Enhancements

- **store**: include and sort search results by distance score `97af3e9c`
- **embedding**: add input_type support with manual prefix injection `ea78232f`
- **config**: load environment variables from .env file on startup `f3c50bbc`
- **release**: enhance changelog generation and categorization `9fab3f2c`

### 🔧 Improvements & Optimizations

- **store**: unify vector search optimization with VectorOptimizer `3d9d4281`
- **store**: replace deprecated nearest_to with vector_search API `867a65a9`
- **commit**: add file-type checks to enforce docs type rules `ee523291`

### 🐛 Bug Fixes & Stability

- **mcp**: unify error handling in GraphRAG search execution `fa171584`
- **search**: correct threshold conversion from similarity to distance `057c7832`
- **cli**: use git root for repository detection in commands `e950d9c4`

### 📚 Documentation & Examples

- fix install script URLs to use master branch `8c304970`

### 🔄 Other Changes

- **deps**: upgrade notify, notify-debouncer-mini, dirs, and tree-si... `1d71d547`
- **deps**: upgrade arrow crates to 55.2.0 for dependency updates `1ad07e85`
- **ci**: update Rust version to 1.82 in Cargo.toml and clean workfl... `4bcc8d53`
- **deps**: update Cargo `08a6c6cb`
- Add website `7ace9672`
- **release**: add GitHub Action job to publish crate to crates.io `84af3b80`

### 📊 Release Summary

**Total commits**: 17 across 5 categories

✨ **4** new features - *Enhanced functionality*
🔧 **3** improvements - *Better performance & code quality*
🐛 **3** bug fixes - *Improved stability*
📚 **1** documentation update - *Better developer experience*
🔄 **6** other changes - *Maintenance & tooling*

## [0.6.0] - 2025-06-24

### 📋 Release Summary

This release introduces new filtering options for code searches and limits output size to improve usability. Documentation has been updated for clearer build instructions, and indexing processes have been enhanced for better performance. Several bug fixes address search stability, language detection, and memory management.


### ✨ Features

- **mcp**: add max_tokens parameter to limit tool output size (ba81bd24)
- **mcp**: add max_tokens limit to truncate large outputs in memory a... (0bce4ada)
- **search**: add --language filter for code block searches (daeeb62a)

### 🐛 Bug Fixes

- **graphrag**: use quiet mode in GraphBuilder during search (44d4fd74)
- **mcp**: remove redundant cwd changes and fix startup directory (5cb62cc5)
- **indexer**: correct language detection for file extensions (bd6f0aeb)
- **memory**: remove lock timeouts to prevent premature failures (d27a0987)

### 🔧 Other Changes

- **instructions**: clarify mandatory cargo build flags and usage (a14befc6)
- **readme**: streamline and condense README key features section (bbe70ef2)
- **indexer**: add batch processing and code region extraction mo... (43f50282)
- **indexer**: extract markdown processing into dedicated module (17cc2ab5)
- **indexer**: move signature extraction to dedicated module (a29fb64b)

### 📊 Commit Summary

**Total commits**: 12
- ✨ 3 new features
- 🐛 4 bug fixes
- 🔧 5 other changes

## [0.5.2] - 2025-06-22

### 📋 Release Summary

This release improves search accuracy with enhanced query validation and adjusts memory limits for better resource management. Performance optimizations streamline data processing, complemented by updated documentation to help users fine-tune vector indexing. Several bug fixes enhance overall system reliability and user experience.


### 🐛 Bug Fixes

- **search**: enforce stricter query validation and correct detail levels (9442589a)
- **memory**: reduce max and default memories returned to 5 (50a84fe7)

### 🔧 Other Changes

- **store**: optimize sub-vector factor selection and milestone checks (98bdfdd1)
- **store**: add LanceDB vector index tuning and performance guide (b2c2fa8d)
- **constants**: extract MAX_QUERIES to shared constant (c2722c7c)

### 📊 Commit Summary

**Total commits**: 5
- 🐛 2 bug fixes
- 🔧 3 other changes

## [0.5.1] - 2025-06-21

### 📋 Release Summary

This release includes several bug fixes that enhance command pattern recognition and improve code efficiency. These updates contribute to a smoother and more reliable user experience.


### 🐛 Bug Fixes

- **view**: resolve files with ./ prefix in view command patterns (4ecc5900)
- **clippy**: reduntant conversion (c53c046b)

### 📊 Commit Summary

**Total commits**: 2
- 🐛 2 bug fixes

## [0.5.0] - 2025-06-21

### 📋 Release Summary

This release introduces enhanced search and memory features, including detailed output options and multi-query support, along with new CLI commands and expanded protocol integration. Additional language support and improved documentation provide a better user experience. Several bug fixes and refinements improve rendering accuracy and overall system stability.


### ✨ Features

- **search**: add detail level option for search output (8ade06ba)
- **memory**: add multi-query support for memory retrieval (437e7d4f)
- **docs**: add new CLI commands and usage examples to README (0fdfa552)
- **mcp_proxy**: add HTTP proxy command for multiple MCP servers (26301f7b)
- **mcp**: add HTTP server mode for MCP protocol integration (8ff10302)
- **indexer**: add CSS/SCSS language support with tree-sitter parsers (fe88742a)

### 🐛 Bug Fixes

- **render_utils**: show first 2 and last 2 lines in signature renderings (6a46610f)
- **render_utils**: correct new line rendering in markdown output (a6453c6d)
- **indexer**: truncate signature text output to 5 lines with ellipsis (0f2fe910)

### 🔧 Other Changes

- **proxy**: restrict console logging to debug mode only (4199a6c0)
- **search**: render docs with detail level matching code output (33db16a0)
- **indexer**: extract file and git utilities into modules (03b8f495)
- **svelte**: simplify symbol extraction to script/style only (367f99dd)

### 📊 Commit Summary

**Total commits**: 13
- ✨ 6 new features
- 🐛 3 bug fixes
- 🔧 4 other changes

## [0.4.1] - 2025-06-17

### 📋 Release Summary

This release includes several bug fixes that improve content accuracy and output formatting. Enhancements to search functionality and indexing provide more precise results, while performance optimizations reduce build times.


### 🐛 Bug Fixes

- **embedding**: include line ranges in content hash calculation (cf7c2d1b)
- **indexer**: correct chunk merging to use sorted line numbers (2ec4d221)
- **view**: correct output format handling for view command (6fe41063)

### 🔧 Other Changes

- **view, indexer**: add line numbers to text signature and searc... (981aeb8d)
- **docker**: build release without default Cargo features (8d442bc0)

### 📊 Commit Summary

**Total commits**: 5
- 🐛 3 bug fixes
- 🔧 2 other changes

## [0.4.0] - 2025-06-16

### 📋 Release Summary

This release introduces LSP integration with external server support and enhanced pre-commit hook automation for streamlined workflows. Documentation has been expanded with detailed usage examples and development instructions, while several refinements improve versioning prompts and semantic search clarity. Minor bug fixes address changelog formatting for better readability.


### ✨ Features

- **docs**: add LSP integration docs and CLI usage examples (7dfd5c20)
- **mcp**: add LSP support with external server integration (29bbf98a)
- **commit**: add automatic pre-commit hook integration with AI commi... (07a48fde)
- **commit**: run pre-commit hooks before generating commit message (92aaf04a)
- **release**: update versioning prompt and add lock file update (786e1fe3)

### 🐛 Bug Fixes

- **docs**: remove brackets from commit hashes in changelog (92bad9dd)

### 🔧 Other Changes

- **docker**: remove Cargo.lock from .dockerignore (d72ae449)
- **cargo**: narrow Tokio and dependencies features for leaner build (3e6b6789)
- add comprehensive Octocode development instructions (75c3add1)
- **cli**: set version from Cargo.toml environment variable (6ad09c16)
- **mcp/lsp**: simplify LSP tool inputs by replacing character wi... (616032e8)
- **lsp**: simplify LSP responses to plain text format (5f8487a8)
- **mcp**: clarify semantic search guidance in tool description (83551bba)
- **mcp**: rename search_graphrag to graphrag_search for consistency (cf1d8428)
- **mcp**: rename search_code tool to semantic_search to avoid AI... (93ca7008)
- **commit**: clarify commit message rules and types (380cadcc)

### 📊 Commit Summary

**Total commits**: 16
- ✨ 5 new features
- 🐛 1 bug fix
- 🔧 10 other changes

## [0.3.0] - 2025-06-14

### 📋 Release Summary

This release enhances search functionality by increasing the maximum allowed queries and adding a text output format for results. Improvements to memory handling and command output formatting boost reliability and consistency. Additional fixes address changelog formatting, test stability, and performance optimizations across components.


### ✨ Features

- **indexer**: increase max allowed queries from 3 to 5 (9098d58e)
- **commit,release**: improve handling of breaking changes in commands (67f06276)
- **search**: add text output format for search results (b2cbbbfe)

### 🐛 Bug Fixes

- **release**: preserve trailing newline in changelog on update (cebc98e0)
- **memory**: add UTF-8 sanitization and lock timeout handling (85cb6356)
- **tests**: fix test failures and apply code formatting (7e645ae2)
- **memory,commit,review**: use char count for truncation limits (4ed5e732)
- **mcp**: use actually used original_dir variable for cwd restore (60ec9b77)

### 🔧 Other Changes

- **mcp**: reduce token usage in tool definitions and schemas (04db399f)
- **semantic_code**: clarify multi-term search usage in tool descript... (0f931263)
- **graphrag**: unify and improve text output formatting (27476075)
- **memory**: unify memory formatting and remove sanitization (00e72942)
- **commands**: unify output format handling with OutputFormat enum (9f95e7bc)
- add Cargo.lock and track it in repo (b34051b2)
- **changelog**: add initial release notes for v0.1.0 (91ae04ff)

### 📝 All Commits

- cebc98e0 fix(release): preserve trailing newline in changelog on update *by Don Hardman*
- 9098d58e feat(indexer): increase max allowed queries from 3 to 5 *by Don Hardman*
- 04db399f perf(mcp): reduce token usage in tool definitions and schemas *by Don Hardman*
- 0f931263 docs(semantic_code): clarify multi-term search usage in tool descript... *by Don Hardman*
- 27476075 refactor(graphrag): unify and improve text output formatting *by Don Hardman*
- 85cb6356 fix(memory): add UTF-8 sanitization and lock timeout handling *by Don Hardman*
- 67f06276 feat(commit,release): improve handling of breaking changes in commands *by Don Hardman*
- 7e645ae2 fix(tests): fix test failures and apply code formatting *by Don Hardman*
- 00e72942 refactor(memory): unify memory formatting and remove sanitization *by Don Hardman*
- 4ed5e732 fix(memory,commit,review): use char count for truncation limits *by Don Hardman*
- 9f95e7bc refactor(commands): unify output format handling with OutputFormat enum *by Don Hardman*
- b2cbbbfe feat(search): add text output format for search results *by Don Hardman*
- b34051b2 chore: add Cargo.lock and track it in repo *by Don Hardman*
- 60ec9b77 fix(mcp): use actually used original_dir variable for cwd restore *by Don Hardman*
- 91ae04ff docs(changelog): add initial release notes for v0.1.0 *by Don Hardman*

All notable changes to this project will be documented in this file.

## [0.2.0] - 2025-06-12

### ✨ Features

- add mode option to selectively clear tables
- add multi-query search usage and support details
- add hierarchical bottom-up chunking for docs
- add show-file option to display file chunks
- add --no-verify flag to skip git hooks
- add GraphRAG data cleanup on file removal
- improve UTF-8 slicing and path handling; build from D...
- build GraphRAG from existing DB if enabled
- add detailed multi-mode search with markdown output

### 🐛 Bug Fixes

- preserve formatting when updating version fields
- merge tiny chunks to reduce excessive chunk creation
- add optional context field to data schema
- update default model names and versions
- suppress MCP server logs during graph loading
- properly handle .noindex ignore files
- remove unnecessary timeouts on memory ops
- update Rust version and copy config templates
- require curl and update repo URLs to Muvon/octocode
- fix variable interpolation in release workflow URLs

### 🔧 Other Changes

- docs: replace "reindex" with "index" for accuracy in docs
- refactor: centralize search embeddings generation logic
- docs: add AI-powered release management docs and CLI usage
- refactor: unify GraphRAG config under graphrag section
- refactor: use shared HTTP client with pooling
- chore: update Apache License text to latest version
- chore: add Rust formatting and linting hooks
- refactor: move git file detection to utils module and clean code

## [0.1.0] - 2025-06-06

**Intelligent Code Indexer and Semantic Search Engine**

### ✨ Core Features
- **🔍 Semantic Code Search** - Natural language queries across your entire codebase
- **🕸️ Knowledge Graph (GraphRAG)** - Automatic relationship discovery between files and modules
- **🧠 AI Memory System** - Store and search project insights, decisions, and context
- **🔌 MCP Server** - Built-in Model Context Protocol for AI assistant integration

### 🌐 Language Support
**11 Languages**: Rust, Python, JavaScript, TypeScript, Go, PHP, C++, Ruby, JSON, Bash, Markdown

### 🛠️ AI-Powered Tools
- Smart commit message generation
- Code review with best practices analysis
- Auto-reindexing with file watching
- Multi-LLM support via OpenRouter

### ⚡ Performance & Privacy
- **Local-first option** (FastEmbed/SentenceTransformer on macOS)
- **Cloud embeddings** (Voyage AI - 200M free tokens/month)
- Respects `.gitignore` - never indexes sensitive files
- Optimized batch processing with Lance columnar database



================================================
FILE: Dockerfile
================================================
# Multi-stage Dockerfile for octocode
# Stage 1: Build
FROM rust:1.87-slim as builder

# Install system dependencies
RUN apt-get update && apt-get install -y \
		pkg-config \
		protobuf-compiler \
		libssl-dev \
		&& rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Copy manifests
COPY Cargo.toml Cargo.lock ./

# Copy source code and config templates
COPY src ./src
COPY config-templates ./config-templates

# Build the application
RUN cargo build --release --no-default-features

# Stage 2: Runtime
FROM debian:bookworm-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
		ca-certificates \
		&& rm -rf /var/lib/apt/lists/* \
		&& update-ca-certificates

# Create a non-root user
RUN groupadd -r octocode && useradd -r -g octocode octocode

# Create app directory
WORKDIR /app

# Copy the binary from builder stage
COPY --from=builder /app/target/release/octocode /usr/local/bin/octocode

# Change ownership to non-root user
RUN chown -R octocode:octocode /app

# Switch to non-root user
USER octocode

# Expose port (if applicable)
# EXPOSE 8080

# Health check (customize based on your application)
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
		CMD octocode --help || exit 1

# Set the entrypoint
ENTRYPOINT ["octocode"]
CMD ["--help"]



================================================
FILE: INSTALL.md
================================================
# Installation Guide

## Quick Install

### Universal Script (Unix/Linux/macOS/Windows)
```bash
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh
```

**Works on:**
- Linux (any distribution, x86_64 and ARM64)
- macOS (Intel and Apple Silicon)
- Windows (x86_64 and ARM64 via Git Bash, WSL, MSYS2, Cygwin)
- Any Unix-like system with `/bin/sh`

### Installation Options
```bash
# Install specific version
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --version 0.1.0

# Install to custom directory
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --install-dir /usr/local/bin

# Install for specific target
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --target x86_64-unknown-linux-musl

# Environment variables
export OCTOCODE_INSTALL_DIR=/opt/bin
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh
```

## Manual Installation

### Download Pre-built Binaries

Download the appropriate binary for your platform from the [latest release](https://github.com/muvon/octocode/releases/latest):

| Platform | Architecture | Download |
|----------|--------------|----------|
| Linux | x86_64 (static) | `octocode-VERSION-x86_64-unknown-linux-musl.tar.gz` |
| Linux | ARM64 (static) | `octocode-VERSION-aarch64-unknown-linux-musl.tar.gz` |
| Windows | x86_64 | `octocode-VERSION-x86_64-pc-windows-msvc.zip` |
| Windows | ARM64 | `octocode-VERSION-aarch64-pc-windows-msvc.zip` |
| macOS | x86_64 | `octocode-VERSION-x86_64-apple-darwin.tar.gz` |
| macOS | ARM64 | `octocode-VERSION-aarch64-apple-darwin.tar.gz` |

### Extract and Install

#### Unix/Linux/macOS
```bash
# Extract the archive
tar xzf octocode-VERSION-TARGET.tar.gz

# Move to a directory in your PATH
mv octocode ~/.local/bin/
# or
sudo mv octocode /usr/local/bin/
```

#### Windows
```bash
# In Git Bash, WSL, or MSYS2
tar xzf octocode-VERSION-x86_64-pc-windows-gnu.zip

# Move to a directory in your PATH
mv octocode.exe ~/.local/bin/
# or copy to Windows PATH directory
cp octocode.exe /c/Windows/System32/  # (requires admin)
```

## Install from Source

### Prerequisites
- Rust 1.87.0 or later
- Protocol Buffers compiler (`protoc`)

### Install protoc

#### Ubuntu/Debian
```bash
sudo apt-get install protobuf-compiler
```

#### macOS
```bash
brew install protobuf
```

#### Windows
```bash
# In Git Bash or WSL
choco install protoc
# or download from: https://github.com/protocolbuffers/protobuf/releases
```

### Build and Install
```bash
# Clone the repository
git clone https://github.com/muvon/octocode.git
cd octocode

# Build and install
cargo install --path .
```

## Using Cargo

If you have Rust installed, you can install directly from crates.io:

```bash
cargo install octocode
```

## Verify Installation

```bash
octocode --version
```

## Package Managers

### Homebrew (macOS)
```bash
# Coming soon
brew install octocode
```

### Chocolatey (Windows)
```bash
# In Git Bash, PowerShell, or Command Prompt
# Coming soon
choco install octocode
```

### Scoop (Windows)
```bash
# In PowerShell or Command Prompt
# Coming soon
scoop install octocode
```

## Custom Installation Options

### Install Script Options

The installation script supports several options and works on all platforms:

```bash
# Install specific version
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --version 0.1.0

# Install to custom directory
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --install-dir /usr/local/bin

# Install for specific target
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --target x86_64-unknown-linux-musl

# Show help
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --help
```

### Environment Variables

- `OCTOCODE_INSTALL_DIR`: Override default installation directory
- `OCTOCODE_VERSION`: Override version to install

Example:
```bash
export OCTOCODE_INSTALL_DIR=/opt/octocode/bin
curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh
```

## Troubleshooting

### Permission Denied
If you get permission denied errors, make sure the installation directory is writable or use `sudo`:

```bash
sudo curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh -s -- --install-dir /usr/local/bin
```

### Binary Not in PATH
If the binary is not found after installation, add the installation directory to your PATH:

```bash
# For bash/zsh
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

# For fish
echo 'set -gx PATH $HOME/.local/bin $PATH' >> ~/.config/fish/config.fish
```

### musl vs glibc (Linux)
- Use `x86_64-unknown-linux-musl` for maximum compatibility (static linking)
- Use `x86_64-unknown-linux-gnu` if you need glibc-specific features

### macOS Gatekeeper
If macOS prevents running the binary, you may need to:

```bash
# Remove quarantine attribute
xattr -d com.apple.quarantine /path/to/octocode

# Or allow in System Preferences > Security & Privacy
```



================================================
FILE: install.sh
================================================
#!/bin/sh

# Octocode Installation Script
# Universal installation script that works on Unix, Linux, macOS, and Windows
# Works with: bash, zsh, sh, Git Bash, WSL, MSYS2
# Requires: curl (for downloading releases)

set -e

# Configuration
REPO="Muvon/octocode"
BINARY_NAME="octocode"
INSTALL_DIR="${OCTOCODE_INSTALL_DIR:-$HOME/.local/bin}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Detect platform and architecture
detect_platform() {
    local os arch

    # Detect OS
    case "$(uname -s)" in
        Linux*)     os="unknown-linux" ;;
        Darwin*)    os="apple-darwin" ;;
        CYGWIN*|MINGW*|MSYS*)    os="pc-windows" ;;
        *)          log_error "Unsupported operating system: $(uname -s)"; exit 1 ;;
    esac

    # Detect architecture
    case "$(uname -m)" in
        x86_64|amd64)   arch="x86_64" ;;
        arm64|aarch64)  arch="aarch64" ;;
        *)              log_error "Unsupported architecture: $(uname -m)"; exit 1 ;;
    esac

    # Determine target triple and preferred variant
    case "$os-$arch" in
        unknown-linux-x86_64)    echo "x86_64-unknown-linux-musl" ;;   # Static musl binary
        unknown-linux-aarch64)   echo "aarch64-unknown-linux-musl" ;;  # ARM64 Linux support
        apple-darwin-x86_64)     echo "x86_64-apple-darwin" ;;
        apple-darwin-aarch64)    echo "aarch64-apple-darwin" ;;
        pc-windows-x86_64)       echo "x86_64-pc-windows-msvc" ;;       # MSVC for better compatibility
        pc-windows-aarch64)      echo "aarch64-pc-windows-msvc" ;;      # ARM64 Windows support
        *)                       log_error "Unsupported platform: $os-$arch"; exit 1 ;;
    esac
}

# Get the latest release version (including prereleases)
get_latest_version() {
    if command_exists curl; then
        curl -s "https://api.github.com/repos/$REPO/releases" | \
            grep '"tag_name":' | \
            head -1 | \
            sed -E 's/.*"([^"]+)".*/\1/'
    else
        log_error "curl is required but not found. Please install curl."
        log_info "On Ubuntu/Debian: sudo apt-get install curl"
        log_info "On CentOS/RHEL: sudo yum install curl"
        log_info "On macOS: curl is pre-installed"
        log_info "On Windows: curl is available in Windows 10+ or install via chocolatey"
        exit 1
    fi
}

# Download and extract binary
download_and_install() {
    local version="$1"
    local target="$2"
    local tmp_dir

    # Create temporary directory (compatible with all systems)
    if command -v mktemp >/dev/null 2>&1; then
        tmp_dir=$(mktemp -d)
    else
        # Fallback for systems without mktemp
        tmp_dir="/tmp/octocode-install-$$"
        mkdir -p "$tmp_dir"
    fi

    # Ensure cleanup on exit
    trap "rm -rf '$tmp_dir'" EXIT INT TERM

    log_info "Downloading $BINARY_NAME $version for $target..."

    # Determine file extension and extract command
    local ext="tar.gz"
    local extract_cmd="tar xzf"
    local binary_name="$BINARY_NAME"

    if [ "$target" != "${target#*windows}" ]; then
        ext="zip"
        binary_name="${BINARY_NAME}.exe"
        # Check for unzip command
        if command -v unzip >/dev/null 2>&1; then
            extract_cmd="unzip -q"
        else
            log_error "unzip command not found. Please install unzip to extract Windows binaries."
            exit 1
        fi
    fi

    local filename="${BINARY_NAME}-${version}-${target}.${ext}"
    local url="https://github.com/$REPO/releases/download/$version/$filename"

    log_info "Downloading from: $url"

    # Download using curl (required)
    if command_exists curl; then
        if ! curl -fsSL "$url" -o "$tmp_dir/$filename"; then
            log_error "Download failed. Please check:"
            log_error "1. Internet connection"
            log_error "2. Release exists: $url"
            log_error "3. GitHub is accessible"
            exit 1
        fi
    else
        log_error "curl is required but not found. Please install curl."
        log_info "On Ubuntu/Debian: sudo apt-get install curl"
        log_info "On CentOS/RHEL: sudo yum install curl"
        log_info "On macOS: curl is pre-installed"
        log_info "On Windows: curl is available in Windows 10+ or install via chocolatey"
        exit 1
    fi

    # Extract
    log_info "Extracting binary..."
    cd "$tmp_dir" || exit 1

    if ! $extract_cmd "$filename"; then
        log_error "Failed to extract archive"
        exit 1
    fi

    # Find the binary
    local binary_path="$tmp_dir/$binary_name"

    if [ ! -f "$binary_path" ]; then
        log_error "Binary '$binary_name' not found in archive"
        log_error "Archive contents:"
        ls -la "$tmp_dir/"
        exit 1
    fi

    # Create install directory
    if [ ! -d "$INSTALL_DIR" ]; then
        if ! mkdir -p "$INSTALL_DIR"; then
            log_error "Failed to create install directory: $INSTALL_DIR"
            exit 1
        fi
    fi

    # Install binary
    log_info "Installing to $INSTALL_DIR..."
    local target_path="$INSTALL_DIR/$binary_name"

    if ! cp "$binary_path" "$target_path"; then
        log_error "Failed to copy binary to $target_path"
        exit 1
    fi

    if ! chmod +x "$target_path"; then
        log_error "Failed to make binary executable"
        exit 1
    fi

    log_success "$BINARY_NAME installed successfully!"
}

# Check if install directory is in PATH
check_path() {
    case ":$PATH:" in
        *":$INSTALL_DIR:"*)
            return 0
            ;;
        *)
            log_warning "$INSTALL_DIR is not in your PATH"
            log_info "Add the following line to your shell profile (.bashrc, .zshrc, .profile, etc.):"
            printf "export PATH=\"%s:\$PATH\"\n" "$INSTALL_DIR"
            echo ""
            log_info "Or run the following command to add it to your current session:"
            printf "export PATH=\"%s:\$PATH\"\n" "$INSTALL_DIR"
            echo ""
            ;;
    esac
}

# Verify installation
verify_installation() {
    local binary_name="$BINARY_NAME"

    # Add .exe extension for Windows
    case "$(uname -s)" in
        CYGWIN*|MINGW*|MSYS*) binary_name="${BINARY_NAME}.exe" ;;
    esac

    local binary_path="$INSTALL_DIR/$binary_name"

    if [ -x "$binary_path" ]; then
        log_success "Installation verified!"
        log_info "Run '$BINARY_NAME --version' to check the installed version"

        # Try to run the binary if it's in PATH
        if command -v "$BINARY_NAME" >/dev/null 2>&1; then
            local version_output
            if version_output=$("$BINARY_NAME" --version 2>/dev/null); then
                log_info "Installed version: $version_output"
            fi
        fi
    else
        log_error "Installation verification failed: $binary_path not found or not executable"
        exit 1
    fi
}

# Check prerequisites
check_prerequisites() {
    if ! command_exists curl; then
        log_error "curl is required but not found."
        echo ""
        log_info "Please install curl first:"
        log_info "• Ubuntu/Debian: sudo apt-get install curl"
        log_info "• CentOS/RHEL: sudo yum install curl"
        log_info "• Fedora: sudo dnf install curl"
        log_info "• Alpine: apk add curl"
        log_info "• macOS: curl is pre-installed"
        log_info "• Windows: curl is available in Windows 10+ or install via chocolatey"
        echo ""
        log_info "After installing curl, run this script again."
        exit 1
    fi

    # Test curl functionality
    if ! curl --version >/dev/null 2>&1; then
        log_error "curl is installed but not working properly."
        log_info "Please check your curl installation."
        exit 1
    fi
}

# Main installation function
main() {
    local version target

    log_info "Installing $BINARY_NAME..."

    # Check prerequisites first
    check_prerequisites

    # Parse command line arguments
    while [ $# -gt 0 ]; do
        case $1 in
            --version)
                version="$2"
                shift 2
                ;;
            --target)
                target="$2"
                shift 2
                ;;
            --install-dir)
                INSTALL_DIR="$2"
                shift 2
                ;;
            --help|-h)
                cat << 'EOF'
Octocode Installation Script

USAGE:
    install.sh [OPTIONS]

REQUIREMENTS:
    curl                    Required for downloading releases

OPTIONS:
    --version <VERSION>     Install specific version (default: latest)
    --target <TARGET>       Install for specific target platform
    --install-dir <DIR>     Installation directory (default: $HOME/.local/bin)
    --help, -h              Show this help message

EXAMPLES:
    ./install.sh                                          # Install latest version
    ./install.sh --version 0.1.0                         # Install specific version
    ./install.sh --install-dir /usr/local/bin             # Install to custom directory
    ./install.sh --target x86_64-unknown-linux-musl      # Install for specific target

SUPPORTED TARGETS:
    x86_64-unknown-linux-musl    Linux x86_64 (static, recommended)
    aarch64-unknown-linux-musl   Linux ARM64 (static)
    x86_64-apple-darwin          macOS x86_64
    aarch64-apple-darwin         macOS ARM64
    x86_64-pc-windows-msvc       Windows x86_64
    aarch64-pc-windows-msvc      Windows ARM64

ENVIRONMENT VARIABLES:
    OCTOCODE_INSTALL_DIR         Override default installation directory
    OCTOCODE_VERSION            Override version to install

EXAMPLES WITH ENVIRONMENT VARIABLES:
    export OCTOCODE_INSTALL_DIR=/opt/bin
    ./install.sh

    curl -fsSL https://raw.githubusercontent.com/Muvon/octocode/master/install.sh | sh
    curl -fsSL https://raw.githubusercontent.com/Muvon/octocode/master/install.sh | sh -s -- --version 0.1.0

EOF
                exit 0
                ;;
            *)
                log_error "Unknown option: $1"
                log_info "Use --help for usage information"
                exit 1
                ;;
        esac
    done

    # Override with environment variables if set
    version="${version:-$OCTOCODE_VERSION}"
    INSTALL_DIR="${INSTALL_DIR:-$OCTOCODE_INSTALL_DIR}"

    # Detect platform if not specified
    if [ -z "$target" ]; then
        target=$(detect_platform)
        log_info "Detected platform: $target"
    fi

    # Get latest version if not specified
    if [ -z "$version" ]; then
        log_info "Fetching latest release information..."
        version=$(get_latest_version)
        if [ -z "$version" ]; then
            log_error "Failed to get latest version"
            exit 1
        fi
        log_info "Latest version: $version"
    fi

    # Download and install
    download_and_install "$version" "$target"

    # Check PATH
    check_path

    # Verify installation
    verify_installation

    log_success "Installation complete!"
    echo ""
    log_info "To get started, run: $BINARY_NAME --help"
}

# Run main function
main "$@"



================================================
FILE: INSTRUCTIONS.md
================================================
# Octocode Development Instructions

## Core Principles

### Strict Configuration Management
- **NO DEFAULTS**: All configuration must be explicitly defined in `config-templates/default.toml`
- **Template-First**: Update template file when adding new config options
- **Environment Override**: Use env vars for sensitive data (API keys)
- **Version Control**: Config has version field for future migrations

### Code Reuse & Architecture

#### Embedding Provider Architecture
Embedding providers follow a strict modular architecture with each provider in its own file:

```rust
// Standard provider structure (src/embedding/provider/{provider}.rs)
pub struct ProviderImpl {
    model_name: String,
    dimension: usize,
}

impl ProviderImpl {
    pub fn new(model: &str) -> Result<Self> {
        // Validate supported models first - fail fast with proper error messages
        let dimension = Self::get_model_dimension_static(model)?;
        Ok(Self {
            model_name: model.to_string(),
            dimension,
        })
    }

    fn get_model_dimension_static(model: &str) -> Result<usize> {
        match model {
            "model-1" => Ok(768),
            "model-2" => Ok(1024),
            _ => Err(anyhow::anyhow!(
                "Unsupported model: '{}'. Supported models: model-1 (768d), model-2 (1024d)",
                model
            )),
        }
    }
}

// Required imports for all providers
use super::super::types::InputType;
use super::{EmbeddingProvider, HTTP_CLIENT};
```

**Provider Files Structure**:
- `mod.rs`: Shared code (HTTP_CLIENT, trait, factory) - 110 lines
- `{provider}.rs`: Individual provider implementations
- Feature-gated: fastembed.rs, huggingface.rs
- Always available: jina.rs (138 lines), voyage.rs (154 lines), google.rs (116 lines)

**Current Provider Models**:
- **Jina**: jina-embeddings-v4 (2048d), jina-clip-v2 (1024d), jina-embeddings-v3 (1024d), jina-clip-v1 (768d), jina-embeddings-v2-base-es (768d), jina-embeddings-v2-base-code (768d), jina-embeddings-v2-base-de (768d), jina-embeddings-v2-base-zh (768d), jina-embeddings-v2-base-en (768d)
- **Google**: gemini-embedding-001 (3072d), text-embedding-005 (768d), text-multilingual-embedding-002 (768d)
- **Voyage**: voyage-3.5, voyage-code-2, voyage-finance-2, etc. (use 'info' command for dimensions)

**Result-based Constructor Pattern**:
All providers use `pub fn new(model: &str) -> Result<Self>` with graceful error handling and proper model validation. Factory function calls providers with `?` operator for consistent error propagation.

#### Indexer Core Pattern
```rust
// Always use this pattern for file processing
let lang_impl = languages::get_language(language)?;
parser.set_language(&lang_impl.get_ts_language())?;
extract_meaningful_regions(tree.root_node(), contents, lang_impl.as_ref(), &mut regions);
```

#### Watcher Integration
- Use `NoindexWalker` for file discovery (respects .gitignore + .noindex)
- Git optimization: only reindex changed files between commits
- File metadata caching for skip-unchanged logic

#### Storage Pattern
```rust
// Batch processing for efficiency
if should_process_batch(&blocks_batch, |b| &b.content, config) {
    process_blocks_batch(store, &blocks_batch, config).await?;
    blocks_batch.clear();
    flush_if_needed(store, &mut batches_processed, config, false).await?;
}
```

### LanceDB Performance & Vector Store Guidelines

#### Intelligent Vector Index Optimization
Octocode uses an intelligent vector index optimizer that automatically tunes LanceDB parameters based on dataset characteristics. **No configuration required** - all optimizations are automatic.

#### Key Performance Features
- **Smart Index Creation**: Skips indexing for small datasets (< 1K rows) where brute force is faster
- **Optimal Parameters**: Automatically calculates partitions, sub-vectors, and search parameters
- **Growth-Aware**: Recreates indexes with better parameters as datasets grow
- **Consistent Distance**: Always uses Cosine distance for semantic similarity

#### Best Practices for Store Usage

```rust
// ✅ GOOD: Use the optimized store methods
store.store_code_blocks(&blocks, &embeddings).await?;
store.store_text_blocks(&blocks, &embeddings).await?;
store.store_document_blocks(&blocks, &embeddings).await?;

// ✅ GOOD: Search with optimized parameters (automatic)
let results = store.get_code_blocks_with_config(embedding, Some(limit), None).await?;

// ❌ AVOID: Manual index creation (optimizer handles this)
// table.create_index(&["embedding"], Index::Auto) // Don't do this

// ❌ AVOID: Fixed parameters (optimizer calculates optimal values)
// .num_partitions(256) // Don't hardcode
```

#### Performance Characteristics
- **Small datasets (< 1K rows)**: Brute force search (fastest)
- **Medium datasets (1K-100K rows)**: Optimized IVF_PQ index with intelligent parameters
- **Large datasets (> 100K rows)**: Growth-aware optimization with enhanced recall
- **Search queries**: Automatic nprobes (5-15% of partitions) + refine_factor for better accuracy

#### Memory Module Integration
The memory system (`src/memory/store.rs`) uses the same intelligent optimization:
```rust
// Memory searches automatically use optimized parameters
let results = memory_store.search_memories(&query).await?;
```

#### Monitoring and Debugging
- Index creation timing is logged at INFO level
- Growth optimization triggers are logged with dataset statistics
- Search parameter optimization is logged at DEBUG level
- All failures are gracefully handled with WARNING logs

## Project Structure

### Core Modules
- `src/indexer/` - Tree-sitter parsing, semantic extraction
- `src/indexer/languages/` - Language-specific implementations
- `src/indexer/graphrag/` - Knowledge graph generation
- `src/embedding/` - Multi-provider embedding system with dynamic model discovery
- `src/commands/` - CLI command implementations (index, search, models, etc.)
- `src/mcp/` - Model Context Protocol server

### Model Management System
- **Dynamic Model Discovery**: No hardcoded model-dimension mappings
- **Provider Validation**: Fail-fast during provider creation for invalid models
- **CLI Commands**: `octocode models list [provider]` and `octocode models info provider:model`
- **Feature Detection**: Proper feature-gating shows actual provider availability

### MCP Server Modes
- **Stdin Mode** (default): Standard MCP protocol over stdin/stdout for AI assistant integration
- **HTTP Mode** (`--bind=host:port`): HTTP server for web-based integrations and testing
  ```bash
  # Stdin mode (default)
  octocode mcp --path=/path/to/project

  # HTTP mode
  octocode mcp --bind=0.0.0.0:12345 --path=/path/to/project
  ```

### Key Files
- `config-templates/default.toml` - Single source of configuration truth
- `src/config.rs` - Config loading with template fallback
- `src/indexer/mod.rs` - File processing pipeline
- `src/store.rs` - Lance database operations

## Development Patterns

### Adding New Language Support
1. Create `src/indexer/languages/{lang}.rs`
2. Implement `Language` trait with meaningful_kinds
3. Add to `languages/mod.rs` registry
4. Update `detect_language()` function

### Adding Config Options
1. Update struct in `src/config.rs`
2. Add defaults in `Default` impl
3. **MANDATORY**: Update `config-templates/default.toml`
4. Add validation if needed

### Adding MCP Server Features
1. **Stdin Mode**: Default mode for AI assistant integration
2. **HTTP Mode**: Add `--bind=host:port` for web-based access
3. **Tool Providers**: Implement in `src/mcp/{provider}.rs` with Clone trait
4. **Request Handling**: Use existing pattern for both stdin and HTTP modes
5. **State Management**: Use `Arc<Mutex<>>` for shared state across async handlers

### File Processing Pipeline
1. `create_walker()` - Respects .gitignore/.noindex
2. Git optimization check for changed files
3. Language detection → Tree-sitter parsing
4. Semantic region extraction with smart merging
5. Batch embedding generation
6. Lance database storage

### GraphRAG Integration
- Enabled via `config.graphrag.enabled`
- Builds relationships from AST imports/exports
- Uses LLM for file descriptions (optional)
- Incremental updates on file changes

## Performance Guidelines

### Indexing Optimization
- Batch size: 16 files per embedding batch
- Flush frequency: Every 2 batches (32 files)
- Token limit: 100k tokens per batch
- Git optimization: Skip unchanged files

### Memory Management
- Progressive file counting during indexing
- Preload file metadata in HashMap for O(1) lookup
- Smart merging of single-line declarations
- Context-aware markdown chunking

### Database Efficiency
- Use `content_exists()` before processing
- Batch operations for inserts/updates
- Regular flush cycles for persistence
- Differential processing for file changes
- **Intelligent vector index optimization** (automatic, no configuration needed)
- **Growth-aware index recreation** at dataset milestones
- **Optimized search parameters** (nprobes, refine_factor) calculated per query

## Watch Mode & File Handling

### File Discovery
```rust
let walker = NoindexWalker::create_walker(&current_dir).build();
// Respects both .gitignore and .noindex patterns
```

### Change Detection
- Git commit hash tracking for optimization
- File modification time caching
- Differential block processing
- Cleanup of deleted/ignored files

### Ignore Patterns
- `.gitignore` - Standard git ignore
- `.noindex` - Octocode-specific ignore
- Config ignore patterns for global exclusions

## Quick Start Checklist

1. **Config First**: Always update `config-templates/default.toml`
2. **No Defaults**: Explicit configuration for all options
3. **Reuse Patterns**: Follow existing indexer/storage patterns
4. **Batch Processing**: Use established batch sizes and flush cycles
5. **Git Integration**: Leverage commit-based optimization
6. **Test Incrementally**: Use watch mode for development iteration

## Advanced Topics

### LanceDB Performance Troubleshooting

#### Index Creation Issues
- Check logs for "Creating optimized vector index" messages
- Verify dataset size is appropriate for indexing (>= 1000 rows)
- Monitor index creation timing - should complete in seconds for most datasets

#### Search Performance Issues
- Enable DEBUG logging to see search parameter optimization
- Check if indexes exist: `list_indices()` should show "embedding" indexes
- Verify distance_type is consistently Cosine across all operations

#### Growth Optimization Monitoring
- Look for "Dataset growth detected" log messages at milestones
- Monitor index recreation timing for large datasets
- Check that row counts align with expected growth patterns

#### Memory Module Performance
- Memory system uses same optimization as main store
- Check memory table row counts and index status
- Verify embedding dimensions match between memory and main store

### Development Performance Tips

#### MANDATORY BUILD COMMANDS:
- ALWAYS use `--no-default-features` for ALL cargo commands during development
- cargo build --no-default-features
- cargo check --no-default-features --message-format=short
- cargo test --no-default-features
- NEVER use --release unless explicitly requested
- NEVER use default cargo build - ALWAYS add --no-default-features flag
- **Always run clippy** before finalizing code to ensure clean, warning-free code:
  ```bash
  cargo clippy --all-features --all-targets -- -D warnings
  ```
- **Prefer tokio primitives** over external dependencies when possible (e.g., use tokio for HTTP instead of axum)

#### Code Quality Standards
- **Zero clippy warnings** - All code must pass `cargo clippy` without warnings
- **Minimal dependencies** - Reuse existing dependencies before adding new ones
- **Clone trait** - Add `#[derive(Clone)]` to structs that need to be shared across async contexts
- **Error handling** - Use proper `Result<T>` types and meaningful error messages

#### MCP Server Development
- **Stdin mode** (default): Use for standard MCP protocol compliance
- **HTTP mode** (`--bind=host:port`): Use for web-based integrations
- **Pure tokio** implementation for HTTP to avoid unnecessary dependencies
- **CORS headers** included for browser compatibility

#### Testing Approach
- **Unit tests** for individual components
- **Integration tests** for full workflows
- **Manual testing** with real projects during development
- **HTTP endpoint testing** using curl or similar tools

## Development Patterns

### Feature-Gating Best Practices
```rust
// Module declarations in mod.rs
#[cfg(feature = "provider")]
pub mod provider;

// Conditional compilation in factory functions
#[cfg(feature = "provider")]
{ Ok(Box::new(ProviderImpl::new(model)?)) }
#[cfg(not(feature = "provider"))]
{ Err(anyhow::anyhow!("Provider not compiled")) }
```

### Shared Resource Patterns
```rust
// HTTP client sharing across providers
static HTTP_CLIENT: LazyLock<Client> = LazyLock::new(|| {
    Client::builder()
        .pool_max_idle_per_host(10)
        .pool_idle_timeout(Duration::from_secs(30))
        .timeout(Duration::from_secs(120))
        .build()
        .expect("Failed to create HTTP client")
});
```

## Quality Standards

- **Single Responsibility** - Each provider in its own file
- **Fail-fast validation** - Validate models during provider creation
- **Dynamic discovery** - No hardcoded dimensions where possible
- **Async-first** - Use tokio throughout for non-blocking operations
- **Error resilience** - Graceful degradation when optional features fail



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2025 Muvon Un Limited

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: Makefile
================================================
# Makefile for octocode
# Best practices for Rust development and CI/CD

# Default target
.DEFAULT_GOAL := help

# Variables
BINARY_NAME := octocode
VERSION := $(shell grep '^version =' Cargo.toml | cut -d '"' -f2)
RUST_VERSION := $(shell rustc --version)
TARGET_DIR := target
RELEASE_DIR := $(TARGET_DIR)/release
DEBUG_DIR := $(TARGET_DIR)/debug

# Compilation targets
TARGETS := x86_64-unknown-linux-musl \
					aarch64-unknown-linux-musl \
					x86_64-pc-windows-msvc \
					aarch64-pc-windows-msvc \
					x86_64-apple-darwin \
					aarch64-apple-darwin

# Colors for output
GREEN := \033[0;32m
YELLOW := \033[0;33m
RED := \033[0;31m
BLUE := \033[0;34m
NC := \033[0m # No Color

# Check if we're in a git repository
GIT_AVAILABLE := $(shell git status >/dev/null 2>&1 && echo "yes" || echo "no")

.PHONY: help
help: ## Show this help message
	@echo "$(BLUE)octocode v$(VERSION)$(NC)"
	@echo "$(BLUE)Rust version: $(RUST_VERSION)$(NC)"
	@echo ""
	@echo "$(YELLOW)Available targets:$(NC)"
	@awk 'BEGIN {FS = ":.*##"; printf ""} /^[a-zA-Z_-]+:.*?##/ { printf "  $(GREEN)%-20s$(NC) %s\n", $$1, $$2 }' $(MAKEFILE_LIST)

.PHONY: install-deps
install-deps: ## Install development dependencies
	@echo "$(YELLOW)Installing development dependencies...$(NC)"
	rustup component add clippy rustfmt
	cargo install cargo-audit cargo-outdated cargo-machete cargo-nextest
	@echo "$(GREEN)Dependencies installed successfully$(NC)"

.PHONY: install-targets
install-targets: ## Install compilation targets
	@echo "$(YELLOW)Installing compilation targets...$(NC)"
	@for target in $(TARGETS); do \
		echo "Installing $$target..."; \
		rustup target add $$target; \
	done
	@echo "$(GREEN)Compilation targets installed$(NC)"

.PHONY: check
check: ## Run cargo check
	@echo "$(YELLOW)Running cargo check...$(NC)"
	cargo check --all-targets --all-features

.PHONY: build
build: ## Build the project in debug mode
	@echo "$(YELLOW)Building $(BINARY_NAME) in debug mode...$(NC)"
	cargo build

.PHONY: build-release
build-release: ## Build the project in release mode
	@echo "$(YELLOW)Building $(BINARY_NAME) in release mode...$(NC)"
	cargo build --release
	@echo "$(GREEN)Release binary built: $(RELEASE_DIR)/$(BINARY_NAME)$(NC)"

.PHONY: build-static
build-static: ## Build static binary for current platform
	@echo "$(YELLOW)Building static binary...$(NC)"
ifeq ($(shell uname),Darwin)
	# macOS doesn't support fully static linking, but we can optimize
	cargo build --release --target x86_64-apple-darwin
	cargo build --release --target aarch64-apple-darwin
else ifeq ($(shell uname),Linux)
	# Linux static build with musl - native compilation
	# Both x86_64 and ARM64 can be built natively with musl-tools
ifeq ($(shell uname -m),x86_64)
	cargo build --release --target x86_64-unknown-linux-musl
else
	cargo build --release --target aarch64-unknown-linux-musl
endif
else
	# Fallback to regular release build
	cargo build --release
endif
	@echo "$(GREEN)Static binary built successfully$(NC)"

.PHONY: build-all
build-all: install-targets ## Build for all supported platforms
	@echo "$(YELLOW)Building for all supported platforms...$(NC)"
	@for target in $(TARGETS); do \
		echo "Building for $$target..."; \
		cargo build --release --target $$target; \
		if [ $$? -eq 0 ]; then \
			echo "$(GREEN)✓ $$target built successfully$(NC)"; \
		else \
			echo "$(RED)✗ $$target build failed$(NC)"; \
		fi; \
	done

.PHONY: test
test: ## Run tests
	@echo "$(YELLOW)Running tests...$(NC)"
	cargo test

.PHONY: test-verbose
test-verbose: ## Run tests with verbose output
	@echo "$(YELLOW)Running tests with verbose output...$(NC)"
	cargo test -- --nocapture

.PHONY: test-nextest
test-nextest: ## Run tests with nextest (faster test runner)
	@echo "$(YELLOW)Running tests with nextest...$(NC)"
	cargo nextest run

.PHONY: lint
lint: ## Run clippy lints
	@echo "$(YELLOW)Running clippy lints...$(NC)"
	cargo clippy --all-targets --all-features -- -D warnings

.PHONY: lint-fix
lint-fix: ## Run clippy with automatic fixes
	@echo "$(YELLOW)Running clippy with automatic fixes...$(NC)"
	cargo clippy --all-targets --all-features --fix --allow-dirty -- -D warnings

.PHONY: format
format: ## Format code with rustfmt
	@echo "$(YELLOW)Formatting code...$(NC)"
	cargo fmt

.PHONY: format-check
format-check: ## Check code formatting
	@echo "$(YELLOW)Checking code formatting...$(NC)"
	cargo fmt -- --check

.PHONY: audit
audit: ## Run security audit
	@echo "$(YELLOW)Running security audit...$(NC)"
	cargo audit

.PHONY: outdated
outdated: ## Check for outdated dependencies
	@echo "$(YELLOW)Checking for outdated dependencies...$(NC)"
	cargo outdated

.PHONY: unused-deps
unused-deps: ## Check for unused dependencies
	@echo "$(YELLOW)Checking for unused dependencies...$(NC)"
	cargo machete

.PHONY: clean
clean: ## Clean build artifacts
	@echo "$(YELLOW)Cleaning build artifacts...$(NC)"
	cargo clean
	@echo "$(GREEN)Build artifacts cleaned$(NC)"

.PHONY: clean-target
clean-target: ## Clean only target directory
	@echo "$(YELLOW)Cleaning target directory...$(NC)"
	rm -rf $(TARGET_DIR)
	@echo "$(GREEN)Target directory cleaned$(NC)"

.PHONY: install
install: build-release ## Install the binary to ~/.cargo/bin
	@echo "$(YELLOW)Installing $(BINARY_NAME)...$(NC)"
	cargo install --path .
	@echo "$(GREEN)$(BINARY_NAME) installed successfully$(NC)"

.PHONY: uninstall
uninstall: ## Uninstall the binary
	@echo "$(YELLOW)Uninstalling $(BINARY_NAME)...$(NC)"
	cargo uninstall $(BINARY_NAME)
	@echo "$(GREEN)$(BINARY_NAME) uninstalled successfully$(NC)"

.PHONY: install-completions
install-completions: build-release ## Install shell completions
	@echo "$(YELLOW)Installing shell completions...$(NC)"
	@./scripts/install-completions.sh
	@echo "$(GREEN)Shell completions installed!$(NC)"

.PHONY: test-completions
test-completions: build-release ## Test shell completion generation
	@echo "$(YELLOW)Testing shell completion generation...$(NC)"
	@./scripts/test-completions.sh

.PHONY: run
run: ## Run the application in debug mode
	@echo "$(YELLOW)Running $(BINARY_NAME) in debug mode...$(NC)"
	cargo run

.PHONY: run-release
run-release: ## Run the application in release mode
	@echo "$(YELLOW)Running $(BINARY_NAME) in release mode...$(NC)"
	cargo run --release

.PHONY: size
size: build-release ## Show binary size
	@echo "$(YELLOW)Binary size information:$(NC)"
	@if [ -f "$(RELEASE_DIR)/$(BINARY_NAME)" ]; then \
		ls -lh $(RELEASE_DIR)/$(BINARY_NAME); \
		file $(RELEASE_DIR)/$(BINARY_NAME); \
	else \
		echo "$(RED)Release binary not found. Run 'make build-release' first.$(NC)"; \
	fi

.PHONY: bench
bench: ## Run benchmarks
	@echo "$(YELLOW)Running benchmarks...$(NC)"
	cargo bench

.PHONY: doc
doc: ## Generate documentation
	@echo "$(YELLOW)Generating documentation...$(NC)"
	cargo doc --no-deps --open

.PHONY: doc-private
doc-private: ## Generate documentation including private items
	@echo "$(YELLOW)Generating documentation (including private)...$(NC)"
	cargo doc --no-deps --document-private-items --open

.PHONY: release-dry
release-dry: ## Dry run of cargo release
	@echo "$(YELLOW)Dry run of cargo release...$(NC)"
	cargo publish --dry-run

.PHONY: release
release: test lint audit ## Publish to crates.io
	@echo "$(YELLOW)Publishing to crates.io...$(NC)"
	cargo publish

.PHONY: git-tag
git-tag: ## Create and push git tag for current version
ifeq ($(GIT_AVAILABLE),yes)
	@echo "$(YELLOW)Creating git tag v$(VERSION)...$(NC)"
	git tag -a v$(VERSION) -m "Release v$(VERSION)"
	git push origin v$(VERSION)
	@echo "$(GREEN)Git tag v$(VERSION) created and pushed$(NC)"
else
	@echo "$(RED)Not in a git repository$(NC)"
endif

.PHONY: ci
ci: format-check lint test audit ## Run all CI checks locally
	@echo "$(GREEN)All CI checks passed!$(NC)"

.PHONY: ci-quick
ci-quick: format-check lint test ## Run quick CI checks (no audit)
	@echo "$(GREEN)Quick CI checks passed!$(NC)"

.PHONY: setup
setup: install-deps install-targets ## Setup development environment
	@echo "$(GREEN)Development environment setup complete!$(NC)"

.PHONY: info
info: ## Show project information
	@echo "$(BLUE)Project Information:$(NC)"
	@echo "  Name: $(BINARY_NAME)"
	@echo "  Version: $(VERSION)"
	@echo "  Rust version: $(RUST_VERSION)"
	@echo "  Target directory: $(TARGET_DIR)"
	@echo "  Release directory: $(RELEASE_DIR)"
	@echo "  Supported targets: $(TARGETS)"
	@if [ -f "$(RELEASE_DIR)/$(BINARY_NAME)" ]; then \
		echo "  Release binary: $(GREEN)✓ Available$(NC)"; \
	else \
		echo "  Release binary: $(RED)✗ Not built$(NC)"; \
	fi

# Create target directories if they don't exist
$(TARGET_DIR):
	mkdir -p $(TARGET_DIR)

$(RELEASE_DIR): $(TARGET_DIR)
	mkdir -p $(RELEASE_DIR)

$(DEBUG_DIR): $(TARGET_DIR)
	mkdir -p $(DEBUG_DIR)



================================================
FILE: rust-toolchain.toml
================================================
[toolchain]
channel = "1.87.0"
components = ["rustfmt", "clippy"]



================================================
FILE: rustfmt.toml
================================================
hard_tabs = true



================================================
FILE: .dockerignore
================================================
# Rust
target/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Git
.git/
.gitignore

# Documentation
*.md
docs/

# CI/CD
.github/

# Development
.env
.env.local

# Logs
*.log

# Test artifacts
coverage/
test-results/

# Build artifacts
dist/
releases/

# Other
.goose/
.octocode/
.octodev/



================================================
FILE: .editorconfig
================================================
root = true

[*]
end_of_line = lf
charset = utf-8
trim_trailing_whitespace = true
insert_final_newline = true
visual_wrap = true
max_line_length = 120

[*]
indent_style = tab
indent_size = 2

[**.yaml]
indent_style = space
indent_size = 2

[**.yml]
indent_style = space
indent_size = 2

[**.md]
indent_style = space
indent_size = 2



================================================
FILE: .noindex
================================================
Cargo.lock
test_files/**



================================================
FILE: .pre-commit-config.yaml
================================================
# Pre-commit configuration for Rust project
repos:
  # Rust-specific hooks
  - repo: https://github.com/doublify/pre-commit-rust
    rev: v1.0
    hooks:
      # Format code with cargo fmt
      - id: fmt
        name: cargo fmt
        description: Format Rust code with cargo fmt
        entry: cargo fmt
        language: system
        files: \.rs$
        args: ['--all', '--']

      # Run cargo clippy for linting
      - id: clippy
        name: cargo clippy
        description: Lint Rust code with cargo clippy
        entry: cargo clippy
        language: system
        files: \.rs$
        args: ['--all-targets', '--all-features', '--', '-D', 'warnings']

      # Check that code compiles
      - id: cargo-check
        name: cargo check
        description: Check that Rust code compiles
        entry: cargo check
        language: system
        files: \.rs$
        args: ['--all-targets', '--all-features']

  # General hooks
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      # Remove trailing whitespace
      - id: trailing-whitespace

      # Ensure files end with newline
      - id: end-of-file-fixer

      # Check for merge conflicts
      - id: check-merge-conflict

      # Check YAML syntax
      - id: check-yaml

      # Check TOML syntax
      - id: check-toml

      # Prevent large files from being committed
      - id: check-added-large-files
        args: ['--maxkb=1000']



================================================
FILE: config-templates/default.toml
================================================
# Default configuration for Octocode
# This file contains all default values and serves as the template for new installations
# Version: 1 (for future migration support)
version = 1

[openrouter]
model = "openai/gpt-4.1-mini"
base_url = "https://openrouter.ai/api/v1"
timeout = 120
# api_key = "" # Set via environment variable OPENROUTER_API_KEY

[index]
chunk_size = 2000
chunk_overlap = 100
embeddings_batch_size = 16  # 16 files per batch - table.add() every 16 files for better persistence
embeddings_max_tokens_per_batch = 100000  # Keep existing token limit
flush_frequency = 2  # Flush every 2 batches = every 32 files for coordinated persistence
require_git = true  # Require git repository for indexing

[search]
max_results = 20
similarity_threshold = 0.65
output_format = "markdown"
max_files = 10
context_lines = 3
search_block_max_characters = 400  # Maximum characters to display per code/text/doc block

[embedding]
code_model = "voyage:voyage-code-3"
text_model = "voyage:voyage-3.5-lite"

# API keys are sourced from environment variables:
# JINA_API_KEY, VOYAGE_API_KEY, GOOGLE_API_KEY

[graphrag]
enabled = false
use_llm = false

[graphrag.llm]
description_model = "openai/gpt-4.1-mini"
relationship_model = "openai/gpt-4.1-mini"

# AI processing batch size - how many files to analyze per AI call
# Smaller values reduce API costs and token usage, larger values improve efficiency
ai_batch_size = 8

# Maximum tokens per batch request to avoid exceeding model limits
# Controls the total token count across all files in a single batch
max_batch_tokens = 16384

# Timeout for batch AI requests in seconds
# Longer timeouts accommodate larger batches but may delay processing
batch_timeout_seconds = 60

# Whether to fallback to individual AI calls if batch processing fails
# Recommended: true for reliability, false for strict batch-only processing
fallback_to_individual = true

# Maximum content sample size sent to AI for analysis (in tokens)
# Controls token usage and context window management using existing token estimation
max_sample_tokens = 1500

# Confidence threshold for filtering AI-discovered relationships (0.0-1.0)
# Higher values = more selective, only high-confidence relationships
# Recommended: 0.6 for balanced results, 0.8 for high precision
confidence_threshold = 0.6

# Weight assigned to architectural relationships discovered by AI
# Higher values make AI-discovered relationships more prominent in graph
architectural_weight = 0.9

# System prompt for AI architectural relationship discovery
# Pure system instructions without data embedding
relationship_system_prompt = """You are an expert software architect specializing in code analysis. Analyze the provided code files and identify meaningful ARCHITECTURAL relationships that go beyond simple imports.

Focus on these relationship types:
- 'imports': Module/package imports and dependencies
- 'implements': Interface implementation, trait implementation
- 'extends': Class inheritance, module extension
- 'calls': Function/method calls between modules
- 'uses': Utility usage, service consumption
- 'configures': Configuration setup, dependency injection
- 'factory_creates': Factory pattern instantiation
- 'observer_pattern': Event listening, callback registration
- 'strategy_pattern': Algorithm selection, behavior delegation
- 'adapter_pattern': Interface adaptation, wrapper usage
- 'architectural_dependency': High-level system dependencies

Respond with a JSON array of relationships. Each relationship must include:
- source_path: relative path of source file
- target_path: relative path of target file
- relation_type: one of the types listed above
- description: specific explanation of HOW the relationship works
- confidence: 0.0-1.0 confidence score (use 0.8+ for clear relationships)

Only include relationships with clear architectural significance. Avoid trivial imports."""

# System prompt for AI file description generation
# Pure system instructions - file data sent separately
description_system_prompt = """You are a senior software engineer analyzing code architecture. Provide a concise 2-3 sentence description of the file's ROLE and PURPOSE in the system.

Focus on:
- What architectural layer this file belongs to (API, business logic, data access, utilities, etc.)
- Its primary responsibility and how it contributes to the system
- Key patterns or architectural decisions it implements

Avoid listing specific functions/classes. Instead, describe the file's architectural significance and how it fits into the larger system design."""



================================================
FILE: doc/ADVANCED_USAGE.md
================================================
# Advanced Usage

## AI-Powered Git Workflow

### Smart Commit Messages

```bash
# Basic usage - analyze staged changes and generate commit message
git add .
octocode commit

# Add all changes and commit in one step
octocode commit --all

# Provide extra context to help AI generate better commit message
octocode commit --message "Refactoring the authentication system to support OAuth2"

# Auto-commit without confirmation
octocode commit --all --yes
```

The AI analyzes your staged changes and creates contextual commit messages following conventional commit format with proper scope and description. For large changes affecting multiple files, it automatically adds detailed bullet points.

**Example output for multi-file changes:**
```
feat(auth): implement OAuth2 authentication

- Add OAuth2 provider configuration
- Implement token validation middleware
- Update user model with OAuth2 fields
- Add comprehensive test coverage
```

### AI-Powered Code Review

```bash
# Review staged changes for best practices and issues
git add .
octocode review

# Review all changes at once
octocode review --all

# Focus on specific areas
octocode review --focus security
octocode review --focus performance
octocode review --focus maintainability

# Filter by severity level
octocode review --severity critical    # Only critical issues
octocode review --severity high        # Critical and high issues
octocode review --severity low         # All issues

# Output in JSON for integration with other tools
octocode review --json
```

**Example review output:**
```
📊 Code Review Summary
═══════════════════════════════════════════════
📁 Files reviewed: 3
🔍 Total issues found: 5
🚨 Critical: 1 | ⚠️  High: 2 | 📝 Medium: 2 | 💡 Low: 0
📈 Overall Score: 75/100

🚨 Issues Found
═══════════════════════════════════════════════
🔥 Hardcoded API Key [CRITICAL]
   Category: Security
   Location: src/api.rs:42-44
   Description: API key hardcoded in source code
   💡 Suggestion: Move to environment variables or config file
```

### AI-Powered Release Management

Octocode provides intelligent release automation with AI-powered version calculation and changelog generation.

```bash
# Preview what would be done (always recommended first)
octocode release --dry-run

# Create a release with AI version calculation
octocode release

# Force a specific version (bypasses AI calculation)
octocode release --force-version "2.0.0"

# Skip confirmation prompt for automation
octocode release --yes

# Use custom changelog file
octocode release --changelog "HISTORY.md"
```

**How it works:**

1. **Project Detection**: Automatically detects project type (Rust, Node.js, PHP, Go)
2. **Version Analysis**: Extracts current version from project files or git tags
3. **Commit Analysis**: Analyzes commits since last release using conventional commit format
4. **AI Calculation**: Uses LLM to determine appropriate semantic version bump
5. **Changelog Generation**: Creates structured changelog with categorized changes
6. **File Updates**: Updates project files (Cargo.toml, package.json, composer.json, VERSION)
7. **Git Operations**: Creates release commit and annotated tag

**Conventional Commits Support:**
- `feat:` → Minor version bump (0.1.0 → 0.2.0)
- `fix:` → Patch version bump (0.1.0 → 0.1.1)
- `BREAKING CHANGE` or `!` → Major version bump (0.1.0 → 1.0.0)
- `chore:`, `docs:`, `style:`, etc. → Patch version bump

**Example workflow:**
```bash
# 1. Make your changes and commit them
git add .
octocode commit

# 2. Preview the release
octocode release --dry-run

# 3. Create the release
octocode release

# 4. Push to remote
git push origin main --tags
```

## MCP Server Integration

### Setting Up MCP Server

1. **Start the server:**
   ```bash
   octocode mcp --path /path/to/your/project
   ```

2. **Configure in Claude Desktop** (add to config):
   ```json
   {
     "mcpServers": {
       "octocode": {
         "command": "octocode",
         "args": ["mcp", "--path", "/path/to/your/project"]
       }
     }
   }
   ```

3. **Use with other MCP-compatible AI assistants** by configuring the server endpoint

### LSP Integration (NEW!)

Octocode now supports Language Server Protocol (LSP) integration for enhanced code navigation and analysis capabilities.

#### Starting MCP Server with LSP

```bash
# Start MCP server with LSP integration
octocode mcp --path /path/to/your/project --with-lsp "rust-analyzer"

# For other language servers
octocode mcp --path /path/to/your/project --with-lsp "pylsp"
octocode mcp --path /path/to/your/project --with-lsp "typescript-language-server --stdio"
```

#### Available LSP Tools

| Tool | Description | Parameters |
|------|-------------|------------|
| **lsp_goto_definition** | Navigate to symbol definition | `file_path`, `line`, `symbol` |
| **lsp_hover** | Get symbol information and documentation | `file_path`, `line`, `symbol` |
| **lsp_find_references** | Find all references to a symbol | `file_path`, `line`, `symbol`, `include_declaration` |
| **lsp_document_symbols** | List all symbols in a document | `file_path` |
| **lsp_workspace_symbols** | Search symbols across workspace | `query` |
| **lsp_completion** | Get code completion suggestions | `file_path`, `line`, `symbol` |

#### LSP Tool Usage Examples

**Simple Symbol Navigation:**
```json
{
  "file_path": "src/main.rs",
  "line": 15,
  "symbol": "println"
}
```

**Find References:**
```json
{
  "file_path": "src/auth.rs",
  "line": 42,
  "symbol": "authenticate_user",
  "include_declaration": true
}
```

**Code Completion:**
```json
{
  "file_path": "src/api.rs",
  "line": 25,
  "symbol": "std::vec"
}
```

#### LSP Features

- **Simplified Interface**: Use line numbers + symbol names instead of exact character positions
- **Smart Symbol Resolution**: Automatically finds symbols on specified lines with multiple fallback strategies
- **AI-Friendly Output**: Clean, readable text responses optimized for AI consumption
- **Multi-Language Support**: Works with any LSP server (rust-analyzer, pylsp, typescript-language-server, etc.)
- **Automatic Position Calculation**: Handles character positioning internally
- **Robust Symbol Matching**: Word boundaries, case-insensitive, partial matching, and namespace handling

#### Supported Language Servers

- **Rust**: `rust-analyzer`
- **Python**: `pylsp`, `pyright`
- **TypeScript/JavaScript**: `typescript-language-server --stdio`
- **Go**: `gopls`
- **C/C++**: `clangd`
- **Java**: `jdtls`
- **And any other LSP-compliant language server**

### Available MCP Tools

| Tool | Description | Parameters |
|------|-------------|------------|
| **semantic_search** | Semantic code search across the codebase (supports multi-query) | `query` (string or array), `mode` (string: all/code/docs/text), `detail_level` (string), `max_results` (integer) |
| **graphrag** | Advanced GraphRAG operations: search, get-node, get-relationships, find-path, overview | `operation` (string), `query` (string), `node_id` (string), `source_id` (string), `target_id` (string), `max_depth` (integer), `format` (string) |
| **memorize** | Store important information for future reference | `title` (string), `content` (string), `tags` (array) |
| **remember** | Retrieve stored information by query (supports multi-query) | `query` (string or array), `memory_types` (array), `tags` (array), `related_files` (array), `limit` (integer) |
| **forget** | Remove stored information | `query` (string), `confirm` (boolean) |

#### semantic_search Tool Details

**Single Query (Traditional):**
```json
{
  "query": "authentication functions",
  "mode": "code",
  "detail_level": "partial",
  "max_results": 5
}
```

**Multi-Query Search (NEW!):**
```json
{
  "query": ["authentication", "middleware"],
  "mode": "all",
  "detail_level": "full",
  "max_results": 10
}
```

**Parameters:**
- `query`: String or array of strings (max 3 queries for optimal performance)
- `mode`: Search scope - "all" (default), "code", "docs", or "text"
- `detail_level`: Content detail - "signatures", "partial" (default), or "full"
- `max_results`: Maximum results to return (1-20, default: 3)

**Multi-Query Benefits:**
- **Comprehensive Results**: Find code related to multiple concepts
- **Smart Deduplication**: Same code blocks shown once even if matching multiple queries
- **Relevance Boosting**: Results matching multiple queries get higher scores
- **Parallel Processing**: Fast execution with concurrent search processing

### Key Features

- **Intelligent File Watching**: Reindexes code when files change with smart debouncing and ignore pattern support
- **Memory Persistence**: Stores insights across sessions
- **Multi-tool Integration**: Combines search and memory capabilities
- **Debug Mode**: Enhanced logging for troubleshooting and performance monitoring
- **Git Context**: Memory entries automatically tagged with commit info
- **Process Management**: Prevents multiple concurrent indexing operations for optimal performance

## Advanced Search Techniques

### Search Modes

```bash
# Search specific content types
octocode search "database schema" --mode code      # Only code
octocode search "API documentation" --mode docs    # Only docs
octocode search "configuration" --mode text        # Only text files
octocode search "error handling" --mode all        # All content types
```

### Multi-Query Search (NEW!)

Combine multiple search terms for comprehensive results. Maximum 3 queries supported for optimal performance.

```bash
# Basic multi-query search
octocode search "authentication" "middleware"
octocode search "jwt" "token" "validation"

# Multi-query with specific modes
octocode search "error" "handling" --mode code
octocode search "api" "documentation" --mode docs

# Multi-query with other options
octocode search "database" "connection" --threshold 0.7 --expand
octocode search "auth" "security" --json
```

**How Multi-Query Works:**
- **Parallel Processing**: Each query runs simultaneously for speed
- **Smart Deduplication**: Same code blocks from different queries shown once
- **Relevance Boosting**: Results matching multiple queries get higher scores
- **Same Output Format**: Results look identical to single-query search

**Best Practices:**
- Use related terms: `"jwt" "token"` instead of unrelated terms
- Combine concepts: `"authentication" "middleware"` for auth middleware code
- Use specific terms: `"database" "connection"` instead of vague terms
- Limit to 3 queries: More queries don't necessarily improve results

### Similarity Thresholds

```bash
# High precision search
octocode search "error handling" --threshold 0.8

# Broad results
octocode search "API calls" --threshold 0.3

# Default threshold (0.1)
octocode search "authentication"
```

### Symbol Context Expansion

```bash
# Include related code context
octocode search "user authentication" --expand

# Standard search (no expansion)
octocode search "user authentication"
```

### Output Formats

```bash
# JSON output for programmatic use
octocode search "API endpoints" --json
octocode view "src/**/*.rs" --json

# Markdown for documentation
octocode search "middleware" --md
octocode view "src/**/*.rs" --md
```

## Knowledge Graph Operations

### Basic GraphRAG Commands

```bash
# Search the relationship graph
octocode graphrag search --query "database models"

# Get detailed information about a file
octocode graphrag get-node --node-id "src/auth/mod.rs"

# Find relationships for a specific file
octocode graphrag get-relationships --node-id "src/auth/mod.rs"

# Find connections between two modules
octocode graphrag find-path --source-id "src/auth/mod.rs" --target-id "src/database/mod.rs"

# Get graph overview
octocode graphrag overview
```

### Advanced GraphRAG Usage

```bash
# Export graph structure to markdown
octocode graphrag overview --md > project-structure.md

# Search with JSON output for processing
octocode graphrag search --query "authentication" --json

# Get node information in JSON format
octocode graphrag get-node --node-id "src/main.rs" --json
```

### Import Resolution Features

The GraphRAG system includes an intelligent import resolver that maps import statements to actual file paths across multiple languages:

**Supported Languages:**
- **Rust**: `use`, `mod` statements with crate resolution
- **JavaScript/TypeScript**: `import`, `require` with node_modules and relative paths
- **Python**: `import`, `from` statements with package resolution
- **Go**: `import` statements with module path resolution
- **PHP**: `require`, `include`, `use` statements
- **C/C++**: `#include` directives
- **Ruby**: `require`, `load` statements
- **Bash**: `source`, `.` commands

**Features:**
- **Cached resolution**: Import paths cached for performance
- **Cross-language support**: Handles mixed-language projects
- **Intelligent path mapping**: Resolves relative and absolute imports
- **File grouping**: Processes files by language for efficient resolution

## Memory Management

### Through MCP Server

```bash
# Start MCP server to access memory tools
octocode mcp

# Then use through AI assistants:
# - Store architectural decisions
# - Remember bug fixes and their solutions
# - Track feature requirements and implementation notes
# - Maintain development insights across sessions
```

### Memory Types and Organization

The memory system supports different types of information:

- **code**: Code snippets and implementations
- **architecture**: System design decisions
- **bug_fix**: Bug reports and solutions
- **feature**: Feature requirements and specifications
- **documentation**: Important documentation notes
- **user_preference**: User-specific preferences
- **decision**: Project decisions and rationale
- **learning**: Insights and lessons learned
- **configuration**: Setup and configuration notes
- **testing**: Test strategies and results
- **performance**: Performance optimizations
- **security**: Security considerations
- **insight**: General insights and observations

## Custom Model Configuration

### Using Different Models for Different Tasks

```bash
# Use Claude for better code understanding
octocode config --model "anthropic/claude-3.5-sonnet"

# Use local models via OpenRouter
octocode config --model "local/llama-3.2-70b"
```

### Per-Task Model Configuration

```toml
[graphrag]
description_model = "openai/gpt-4o"
relationship_model = "anthropic/claude-3.5-sonnet"

[openrouter]
model = "openai/gpt-4o-mini"  # Default for other tasks
```

## File Signature Analysis

### Viewing Code Structure

```bash
# View code signatures in current directory
octocode view

# View specific files with glob patterns
octocode view "src/**/*.rs"
octocode view "**/*.py"
octocode view "src/auth/*.ts"

# Output formats
octocode view --json                    # JSON format
octocode view --md                      # Markdown format
octocode view "src/**/*.rs" --md        # Specific files in markdown
```

### Use Cases for Signature Analysis

- **Code Review**: Understand structure before detailed review
- **Documentation**: Generate API documentation
- **Refactoring**: Identify patterns and dependencies
- **Onboarding**: Help new developers understand codebase structure

## Real-time Monitoring

### Watch Mode

```bash
# Watch for changes and auto-index
octocode watch

# Watch with custom debounce time (1-30 seconds, default: 2)
octocode watch --debounce 5

# Watch with custom additional delay (0-5000ms, default: 1000ms)
octocode watch --additional-delay 2000

# Combine both timing options
octocode watch --debounce 3 --additional-delay 1500

# Watch in quiet mode (less output)
octocode watch --quiet

# Watch without git requirements
octocode watch --no-git
```

### Enhanced File Filtering

The watch mode now properly respects ignore patterns from:
- `.gitignore` - Standard Git ignore patterns
- `.noindex` - Custom ignore patterns for indexing

**Supported ignore patterns:**
- Exact matches: `file.txt`
- Directory patterns: `directory/`
- Wildcard patterns: `*.log`, `temp*`
- File extensions: `*.tmp`

**Default ignored paths:**
- `.octocode/`, `target/`, `.git/`
- `node_modules/`, `.vscode/`, `.idea/`
- `.DS_Store`, `Thumbs.db`, `.tmp`, `.temp`

### Performance Optimizations

The watch mode includes several performance improvements:
- **Debouncing**: Prevents rapid re-indexing on multiple file changes
- **Smart filtering**: Early filtering of irrelevant file events
- **Process management**: Prevents multiple concurrent indexing operations

### Integration with Development Workflow

```bash
# Start watching in background with optimal settings
octocode watch --quiet --debounce 2 --additional-delay 1000 &

# For development with frequent changes (faster response)
octocode watch --debounce 1 --additional-delay 500

# For large projects (conservative settings)
octocode watch --debounce 5 --additional-delay 2000

# Continue development...
# Files are automatically indexed as you work

# Stop watching
pkill -f "octocode watch"
```

## Batch Operations and Automation

### Scripting Examples

```bash
#!/bin/bash
# Complete reindex script
octocode clear
octocode index
octocode mcp &
echo "Octocode ready for development"
```

```bash
#!/bin/bash
# Daily maintenance script
octocode clear
octocode index
octocode graphrag overview --md > docs/project-structure.md
octocode view "src/**/*.rs" --md > docs/api-reference.md
```

### CI/CD Integration

```yaml
# GitHub Actions example
- name: Generate Code Documentation
  run: |
    cargo build --release
    ./target/release/octocode index
    ./target/release/octocode view "src/**/*.rs" --md > docs/api.md
    ./target/release/octocode graphrag overview --md > docs/structure.md
```

## Debugging and Troubleshooting

### Debug Commands

```bash
# List all indexed files
octocode debug --list-files

# Check configuration
octocode config --show

# Clear all data and start fresh
octocode clear

# Reindex with verbose output
octocode index
```

### MCP Server Debugging

```bash
# Start MCP server with debug logging
octocode mcp --debug

# Check server status and file watcher behavior
octocode mcp --debug --path /path/to/project
```

**Debug output includes:**
- File watcher startup and ignore pattern loading
- Debouncing events and timing information
- Process spawning and completion status
- Indexing performance metrics

### Common Issues and Solutions

1. **Slow indexing**: Reduce chunk size or use faster embedding models
2. **Poor search results**: Adjust similarity threshold or try different embedding models
3. **Memory issues**: Reduce max_memories or clear old data
4. **Git integration not working**: Ensure you're in a git repository and have staged changes

## Performance Optimization

### For Large Codebases

```toml
[index]
chunk_size = 1000        # Smaller chunks for faster processing
embeddings_batch_size = 64  # Larger batches for better throughput

[search]
max_results = 20         # Limit results for faster response
similarity_threshold = 0.2  # Higher threshold for more relevant results

[memory]
max_memories = 50000     # Increase for large projects
```

### Memory Usage Optimization

```bash
# Clear old data periodically
octocode clear

# Use local embedding models to reduce API calls
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"

# Limit search results
octocode config --max-results 20
```



================================================
FILE: doc/API_KEYS.md
================================================
# API Keys Setup Guide

Octocode requires API keys for embedding generation and optional AI features. This guide covers all supported providers and setup methods.

## Required: Embedding Providers

Octocode needs embeddings to function. You must configure at least one embedding provider.

### Voyage AI (Recommended)

**Free tier**: 200M tokens per month
**Best for**: High-quality embeddings with generous free tier

```bash
# Set environment variable
export VOYAGE_API_KEY="your-voyage-api-key"

# Or configure in config file
octocode config \
  --code-embedding-model "voyage:voyage-code-3" \
  --text-embedding-model "voyage:voyage-3.5-lite"
```

**Get API key**: [voyageai.com](https://www.voyageai.com/)

### Jina AI

**Best for**: Code-specialized embeddings

```bash
# Set environment variable
export JINA_API_KEY="your-jina-api-key"

# Configure models
octocode config \
  --code-embedding-model "jina:jina-embeddings-v2-base-code" \
  --text-embedding-model "jina:jina-embeddings-v3"
```

**Get API key**: [jina.ai](https://jina.ai/)

### Google AI

**Best for**: Integration with Google ecosystem

```bash
# Set environment variable
export GOOGLE_API_KEY="your-google-api-key"

# Configure models
octocode config \
  --code-embedding-model "google:text-embedding-004" \
  --text-embedding-model "google:text-embedding-004"
```

**Get API key**: [Google AI Studio](https://makersuite.google.com/app/apikey)

### OpenAI

**Best for**: High-quality embeddings with latest models

```bash
# Set environment variable
export OPENAI_API_KEY="your-openai-api-key"

# Configure models
octocode config \
  --code-embedding-model "openai:text-embedding-3-small" \
  --text-embedding-model "openai:text-embedding-3-small"

# Or use large model for higher quality
octocode config \
  --code-embedding-model "openai:text-embedding-3-large" \
  --text-embedding-model "openai:text-embedding-3-large"
```

**Get API key**: [OpenAI Platform](https://platform.openai.com/api-keys)

**Available models:**
- `text-embedding-3-small` - 1536 dimensions, cost-effective
- `text-embedding-3-large` - 3072 dimensions, highest quality
- `text-embedding-ada-002` - 1536 dimensions, legacy model

### Local Models (macOS Only)

**Best for**: Privacy, no API costs, offline usage

```bash
# FastEmbed (fastest)
octocode config \
  --code-embedding-model "fastembed:all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:multilingual-e5-small"

# SentenceTransformer (highest quality)
octocode config \
  --code-embedding-model "huggingface:microsoft/codebert-base" \
  --text-embedding-model "huggingface:sentence-transformers/all-mpnet-base-v2"
```

**Supported Architectures:**
- **BERT**: Standard BERT models (e.g., `sentence-transformers/all-MiniLM-L6-v2`)
- **JinaBERT**: Jina's BERT variants (e.g., `jinaai/jina-embeddings-v2-base-code`)

**Popular Models:**
- `sentence-transformers/all-MiniLM-L6-v2` - 384 dimensions, fast and efficient
- `sentence-transformers/all-mpnet-base-v2` - 768 dimensions, high quality
- `microsoft/codebert-base` - 768 dimensions, code-specialized
- `jinaai/jina-embeddings-v2-base-code` - 768 dimensions, code-optimized

**Note**: Local models require building from source on macOS. Prebuilt binaries use cloud embeddings only.

## Optional: LLM Provider

For AI-powered features like commit messages, code review, and GraphRAG descriptions.

### OpenRouter (Recommended)

**Best for**: Access to multiple LLM providers through one API

```bash
# Set environment variable
export OPENROUTER_API_KEY="your-openrouter-api-key"

# Configure default model
octocode config --model "openai/gpt-4o-mini"

# Or use Claude for better code understanding
octocode config --model "anthropic/claude-3.5-sonnet"
```

**Get API key**: [openrouter.ai](https://openrouter.ai/)

**Popular models:**
- `openai/gpt-4o-mini` - Fast and cost-effective
- `openai/gpt-4o` - High quality
- `anthropic/claude-3.5-sonnet` - Excellent for code
- `google/gemini-pro` - Good balance

## Platform Limitations

### Windows/Linux
- **Must use cloud embeddings** (Voyage AI, Jina AI, Google)
- **Cannot use local models** (FastEmbed, SentenceTransformer)
- **Reason**: ONNX Runtime compatibility issues

### macOS
- **Can use all providers** (cloud + local)
- **Local models available** when building from source
- **Prebuilt binaries** use cloud embeddings only

## Configuration Methods

### Environment Variables (Recommended)

```bash
# Add to your shell profile (.bashrc, .zshrc, etc.)
export VOYAGE_API_KEY="your-voyage-api-key"
export OPENROUTER_API_KEY="your-openrouter-api-key"

# Reload your shell
source ~/.bashrc  # or ~/.zshrc
```

### Configuration File

API keys are stored in `~/.local/share/octocode/config.toml`:

```toml
[embedding.voyage]
api_key = "your-voyage-api-key"

[embedding.jina]
api_key = "your-jina-api-key"

[embedding.google]
api_key = "your-google-api-key"

[openrouter]
api_key = "your-openrouter-api-key"
model = "openai/gpt-4o-mini"
```

### Command Line Configuration

```bash
# View current configuration
octocode config --show

# Set embedding models
octocode config --code-embedding-model "voyage:voyage-code-3"
octocode config --text-embedding-model "voyage:voyage-3.5-lite"

# Set LLM model
octocode config --model "anthropic/claude-3.5-sonnet"
```

## Model Recommendations

### For Code Understanding (code_model)

**Best Quality:**
- `huggingface:microsoft/codebert-base` (768 dim, local)
- `jina:jina-embeddings-v2-base-code` (768 dim, cloud)
- `voyage:voyage-code-3` (1024 dim, cloud)

**Fast Local:**
- `fastembed:all-MiniLM-L6-v2` (384 dim)
- `fastembed:BAAI/bge-small-en-v1.5` (384 dim)

### For Text/Documentation (text_model)

**Best Quality:**
- `huggingface:sentence-transformers/all-mpnet-base-v2` (768 dim, local)
- `jina:jina-embeddings-v3` (1024 dim, cloud)
- `voyage:voyage-3.5-lite` (1024 dim, cloud)
- `openai:text-embedding-3-large` (3072 dim, cloud)

**Fast Local:**
- `fastembed:multilingual-e5-small` (384 dim)
- `huggingface:sentence-transformers/all-MiniLM-L6-v2` (384 dim)

## Quick Setup Examples

### Free Tier Setup (Recommended)

```bash
# Use Voyage AI free tier (200M tokens/month)
export VOYAGE_API_KEY="your-voyage-api-key"

octocode config \
  --code-embedding-model "voyage:voyage-code-3" \
  --text-embedding-model "voyage:voyage-3.5-lite"

# Optional: Add OpenRouter for AI features
export OPENROUTER_API_KEY="your-openrouter-api-key"
octocode config --model "openai/gpt-4o-mini"
```

### Local-Only Setup (macOS)

```bash
# No API keys required
octocode config \
  --code-embedding-model "fastembed:all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:multilingual-e5-small"

# AI features disabled without OpenRouter key
```

### High-Quality Setup

```bash
# Best embedding quality
export JINA_API_KEY="your-jina-api-key"
export OPENROUTER_API_KEY="your-openrouter-api-key"

octocode config \
  --code-embedding-model "jina:jina-embeddings-v2-base-code" \
  --text-embedding-model "jina:jina-embeddings-v3" \
  --model "anthropic/claude-3.5-sonnet"
```

## Verification

### Test Embedding Configuration

```bash
# Index a small project to test embeddings
octocode index

# If successful, embeddings are working
# If errors, check API keys and model names
```

### Test LLM Configuration

```bash
# Test AI features (requires staged changes)
git add .
octocode commit --dry-run

# If successful, LLM is working
# If errors, check OpenRouter API key
```

### Debug Configuration Issues

```bash
# Show current configuration
octocode config --show

# Check for configuration errors
RUST_LOG=debug octocode index
```

## Cost Management

### Free Tiers

- **Voyage AI**: 200M tokens/month (very generous)
- **OpenRouter**: Varies by model, some have free tiers
- **Google AI**: 15 requests/minute free tier

### Cost Optimization

```bash
# Use smaller, faster models
octocode config \
  --code-embedding-model "voyage:voyage-3.5-lite" \
  --text-embedding-model "voyage:voyage-3.5-lite"

# Use local models when possible (macOS)
octocode config \
  --code-embedding-model "fastembed:all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:multilingual-e5-small"

# Reduce chunk sizes to use fewer tokens
octocode config --chunk-size 1000
```

## Security Best Practices

### Environment Variables

```bash
# Add to shell profile, not to git
echo 'export VOYAGE_API_KEY="your-key"' >> ~/.bashrc

# Use different keys for different environments
export VOYAGE_API_KEY_DEV="dev-key"
export VOYAGE_API_KEY_PROD="prod-key"
```

### Configuration File Security

```bash
# Ensure config file is not world-readable
chmod 600 ~/.local/share/octocode/config.toml

# Don't commit config files with API keys
echo "config.toml" >> .gitignore
```

## Troubleshooting

### API Key Not Working

1. **Check key format**: Ensure no extra spaces or characters
2. **Verify provider**: Make sure you're using the correct provider prefix
3. **Test directly**: Try the API key with curl or provider's test tools
4. **Check quotas**: Ensure you haven't exceeded rate limits

### Model Not Found

1. **Check model name**: Verify exact model name from provider docs
2. **Check provider prefix**: Ensure correct prefix (voyage:, jina:, etc.)
3. **Update configuration**: Use `octocode config --show` to verify

### Local Models Not Available

1. **Check platform**: Local models only work on macOS
2. **Build from source**: Prebuilt binaries don't include local models
3. **Install dependencies**: Ensure ONNX Runtime is available

For more help, see [Configuration Guide](CONFIGURATION.md) or [Getting Started](GETTING_STARTED.md).



================================================
FILE: doc/ARCHITECTURE.md
================================================
# Architecture

## Core Components

Octocode is built with a modular architecture that separates concerns and enables efficient code analysis and search.

### 1. Indexer Engine
- **Multi-language code parser** using Tree-sitter
- **AST extraction** for semantic understanding
- **Symbol detection** (functions, classes, imports, exports)
- **Chunk-based processing** for large files
- **Safe symlink handling** - Prevents infinite recursion by disabling symlink following
- **Intelligent file discovery** with .gitignore and .noindex pattern support

### 2. Embedding System
- **Multiple providers**: FastEmbed (local), SentenceTransformer (local), Jina AI, Voyage AI, Google, OpenAI (cloud)
- **Dual embedding models**: Separate models for code and text/documentation
- **Batch processing** for efficient embedding generation
- **Provider auto-detection** from model string format
- **Input type support** for query vs document optimization

### 3. Vector Database
- **Lance columnar database** for fast similarity search
- **Efficient storage** (~10KB per file)
- **Fast retrieval** with similarity thresholds
- **Metadata indexing** for filtering
- **File metadata tracking** with modification time updates using LanceDB UpdateBuilder API

### 4. GraphRAG Builder
- **AI-powered relationship extraction** between files
- **Multi-language import resolver** - Maps import statements to actual file paths
- **Import/export dependency tracking** with intelligent path resolution
- **Module hierarchy analysis** with cross-language support
- **Intelligent file descriptions** using LLMs
- **Cached import resolution** for optimized repeated lookups
- **Language-specific import handling** (Rust, JavaScript/TypeScript, Python, Go, PHP, C/C++, Ruby, Bash)

### 5. Search Engine
- **Semantic similarity search** using vector embeddings
- **Keyword boosting** for exact matches
- **Multi-mode search** (code, docs, text, all)
- **Configurable similarity thresholds**

### 6. MCP Server
- **Model Context Protocol** server implementation
- **Intelligent file watching** with debouncing and ignore pattern support
- **Process management** to prevent concurrent indexing operations
- **Tool integration** for AI assistants
- **Debug mode** with enhanced logging and performance monitoring

### 7. Memory System
- **Persistent storage** for insights and context
- **Semantic memory search** using embeddings
- **Git integration** with automatic commit tagging
- **Memory types** (code, architecture, bug fixes, etc.)

### 8. Git Integration
- **Smart commit message generation** using AI
- **Staged changes analysis**
- **Code review assistant** with best practices checking
- **Multiple LLM support** via OpenRouter

## Knowledge Graph Structure

### Nodes
Each file/module in the codebase becomes a node with:
- **File path and metadata** (size, modification time, etc.)
- **AI-generated descriptions** explaining the file's purpose
- **Extracted symbols** (functions, classes, variables, etc.)
- **Import/export lists** for dependency tracking
- **Vector embeddings** for semantic search

### Relationships
Connections between nodes represent different types of relationships:
- **`imports`**: Direct import dependencies between files
- **`sibling_module`**: Files in the same directory
- **`parent_module`** / **`child_module`**: Hierarchical relationships

### Graph Operations
- **Search**: Find nodes by semantic query
- **Get Node**: Retrieve detailed information about a specific file
- **Get Relationships**: Find all connections for a node
- **Find Path**: Discover connection paths between two nodes
- **Overview**: Get high-level graph statistics

## Data Flow

1. **Indexing Phase**:
   ```
   Source Files → Tree-sitter Parser → Symbol Extraction → Embedding Generation → Vector Storage
                                                        ↓
   GraphRAG Analysis ← AI Description Generation ← Chunk Processing
   ```

2. **Search Phase**:
   ```
   Query → Embedding Generation → Vector Similarity Search → Result Ranking → Response
   ```

3. **Memory Phase**:
   ```
   Input → Semantic Processing → Vector Storage → Git Context Tagging → Persistence
   ```

## Supported Languages

| Language | Extensions | Parser Features |
|----------|------------|----------------|
| **Rust** | `.rs` | Full AST parsing, pub/use detection, module structure |
| **Python** | `.py` | Import/class/function extraction, docstring parsing |
| **JavaScript** | `.js`, `.jsx` | ES6 imports/exports, function declarations |
| **TypeScript** | `.ts`, `.tsx` | Type definitions, interface extraction, modules |
| **Go** | `.go` | Package/import analysis, function extraction |
| **PHP** | `.php` | Class/function extraction, namespace support |
| **C++** | `.cpp`, `.hpp`, `.h` | Include analysis, function/class extraction |
| **Ruby** | `.rb` | Class/module extraction, method definitions |
| **JSON** | `.json` | Structure analysis, key extraction |
| **Bash** | `.sh`, `.bash` | Function and variable extraction |
| **Markdown** | `.md` | Document section indexing, header extraction |

## Performance Characteristics

### Indexing Performance
- **Speed**: 100-500 files/second (varies by file size and complexity)
- **Memory**: ~50MB base + ~1KB per indexed file
- **Storage**: ~10KB per file in Lance database
- **Scalability**: Tested with codebases up to 100k+ files

### Search Performance
- **Latency**: <100ms for most queries
- **Throughput**: 1000+ queries/second
- **Memory**: Constant memory usage regardless of result size
- **Accuracy**: High semantic relevance with configurable thresholds

### Optimization Strategies
- **Chunking**: Configurable chunk sizes for different file types
- **Batch Processing**: Efficient embedding generation
- **Caching**: Vector embeddings cached for reuse
- **Incremental Updates**: Only index changed files



================================================
FILE: doc/COMMANDS.md
================================================
# Commands Reference

Complete reference for all Octocode commands with examples and options.

## Core Commands

### `octocode index`

Index your codebase for semantic search.

```bash
# Basic indexing
octocode index

# Verbose output
octocode index --verbose

# Force reindex (ignore cache)
octocode index --force

# Index specific directory
octocode index /path/to/project
```

**What it does:**
- Scans all supported files in your project
- Extracts code symbols and structure using Tree-sitter
- Generates embeddings for semantic search
- Builds knowledge graph relationships (if enabled)
- Stores everything in local LanceDB database
- **Safe file discovery** - Prevents infinite recursion from symlinks
- **Respects ignore patterns** - Honors .gitignore and .noindex files

### `octocode search`

Semantic search across your codebase.

```bash
# Basic search
octocode search "user authentication"

# Multi-query search (NEW!)
octocode search "authentication" "middleware"
octocode search "jwt" "token" "validation"

# Search specific content types
octocode search "database connection" --mode code
octocode search "API documentation" --mode docs
octocode search "configuration" --mode text
octocode search "error handling" --mode all

# Control result details
octocode search "auth" --detail-level signatures  # Function signatures only
octocode search "auth" --detail-level partial     # Smart truncation (default)
octocode search "auth" --detail-level full        # Complete implementations

# Adjust similarity and results
octocode search "auth" --threshold 0.7 --max-results 10

# Output formats
octocode search "auth" --json     # JSON output
octocode search "auth" --md       # Markdown output

# Symbol expansion
octocode search "user authentication" --expand
```

**Search modes:**
- `all` - Search across all content types (default)
- `code` - Search only in code blocks
- `docs` - Search only in documentation files
- `text` - Search only in plain text files

### `octocode view`

View code signatures and structure.

```bash
# View current directory
octocode view

# View specific files with patterns
octocode view "src/**/*.rs"
octocode view "**/*.py"
octocode view "src/auth/*.ts"

# Output formats
octocode view --json              # JSON format
octocode view --md                # Markdown format
octocode view "src/**/*.rs" --md  # Specific files in markdown
```

### `octocode config`

Manage configuration settings.

```bash
# View current configuration
octocode config --show

# Set embedding models
octocode config --code-embedding-model "voyage:voyage-code-3"
octocode config --text-embedding-model "voyage:voyage-3.5-lite"

# Set LLM model
octocode config --model "anthropic/claude-3.5-sonnet"

# Search settings
octocode config --max-results 20
octocode config --similarity-threshold 0.3

# Enable/disable features
octocode config --graphrag-enabled true
octocode config --graphrag-enabled false

# Performance tuning
octocode config --chunk-size 2000
octocode config --embeddings-batch-size 16
```

### `octocode models`

Discover and validate embedding models dynamically.

```bash
# List all supported models for all providers
octocode models list

# List models for specific provider
octocode models list voyage
octocode models list openai
octocode models list jina

# Get detailed information about a specific model
octocode models info voyage:voyage-code-3
octocode models info openai:text-embedding-3-small
octocode models info jina:jina-embeddings-v4

# Validate model support and get dimensions
octocode models info google:text-embedding-004
```

**Output format:**
- **Enumerated lists**: Models displayed with numbers and dimensions (e.g., "1. voyage-3.5 (1024d)")
- **Model count headers**: Shows total number of models found per provider
- **Dimension information**: Each model shows its embedding dimension
- **Unified format**: Consistent output across all providers

**Supported providers:**
- `voyage` - Voyage AI models (8 models: voyage-code-3, voyage-3.5-lite, etc.)
- `openai` - OpenAI embedding models (text-embedding-3-small, text-embedding-3-large, etc.)
- `jina` - Jina AI models (9 models: jina-embeddings-v4, jina-clip-v2, etc.)
- `google` - Google AI models (text-embedding-004, gemini-embedding-001, etc.)
- `fastembed` - Local FastEmbed models (macOS only)
- `huggingface` - HuggingFace models (macOS only)

**Features:**
- **Dynamic discovery**: No hardcoded model lists, real-time API validation
- **Fail-fast validation**: Instantly verify if a model is supported
- **Dimension detection**: Get exact embedding dimensions for each model
- **Feature-gated**: Shows only available providers based on build features

## AI-Powered Git Commands

### `octocode commit`

Generate intelligent commit messages with AI.

```bash
# Basic usage - analyze staged changes
git add .
octocode commit

# Add all files and commit in one step
octocode commit --all

# Provide context for better commit messages
octocode commit --message "Refactoring authentication system"

# Auto-commit without confirmation
octocode commit --all --yes

# Skip pre-commit hooks
octocode commit --no-verify

# Dry run (show what would be committed)
octocode commit --dry-run
```

**Pre-commit Integration:**
- Automatically runs pre-commit hooks if available
- Uses `--all-files` when `--all` flag is specified
- Re-stages modified files after pre-commit runs
- Generates AI commit message after pre-commit completes

### `octocode review`

AI-powered code review for best practices.

```bash
# Review staged changes
git add .
octocode review

# Review all changes
octocode review --all

# Focus on specific areas
octocode review --focus security
octocode review --focus performance
octocode review --focus maintainability

# Filter by severity
octocode review --severity critical    # Only critical issues
octocode review --severity high        # Critical and high issues
octocode review --severity low         # All issues

# Output format
octocode review --json                 # JSON output for tooling
```

### `octocode release`

AI-powered release management with version calculation.

```bash
# Preview release (recommended first step)
octocode release --dry-run

# Create release with AI version calculation
octocode release

# Force specific version
octocode release --force-version "2.0.0"

# Skip confirmation prompt
octocode release --yes

# Custom changelog file
octocode release --changelog "HISTORY.md"
```

**Supported project types:**
- Rust (Cargo.toml)
- Node.js (package.json)
- PHP (composer.json)
- Go (go.mod)

## MCP Server Commands

### `octocode mcp`

Start Model Context Protocol server for AI assistants.

```bash
# Basic MCP server
octocode mcp --path /path/to/your/project

# With LSP integration
octocode mcp --path /path/to/project --with-lsp "rust-analyzer"
octocode mcp --path /path/to/project --with-lsp "pylsp"
octocode mcp --path /path/to/project --with-lsp "typescript-language-server --stdio"

# HTTP mode (instead of stdin/stdout)
octocode mcp --bind "127.0.0.1:8080" --path /path/to/project

# Custom port
octocode mcp --path /path/to/project --port 3001

# Debug mode with enhanced logging
octocode mcp --path /path/to/project --debug
```

**Available MCP tools:**
- `semantic_search` - Semantic code search (supports multi-query)
- `graphrag` - Advanced GraphRAG operations (search, get-node, get-relationships, find-path, overview)
- `memorize` - Store information for future reference
- `remember` - Retrieve stored information (supports multi-query)
- `forget` - Remove stored information
- `lsp_*` - LSP integration tools (when --with-lsp is used)

### `octocode mcp-proxy`

Start MCP proxy server for multiple repositories.

```bash
# Start proxy server
octocode mcp-proxy --bind "127.0.0.1:8080" --path /path/to/parent/directory

# Custom configuration
octocode mcp-proxy --bind "0.0.0.0:9000" --path /workspace --debug
```

**Features:**
- Automatically discovers git repositories
- Creates MCP instances for each repository
- Provides unified access to multiple projects

## Knowledge Graph Commands

### `octocode graphrag`

Knowledge graph operations using GraphRAG.

```bash
# Search the relationship graph
octocode graphrag search --query "authentication modules"

# Get detailed information about a file
octocode graphrag get-node --node-id "src/auth/mod.rs"

# Find relationships for a specific file
octocode graphrag get-relationships --node-id "src/auth/mod.rs"

# Find connections between two modules
octocode graphrag find-path \
  --source-id "src/auth/mod.rs" \
  --target-id "src/database/mod.rs"

# Get graph overview
octocode graphrag overview

# Export formats
octocode graphrag overview --md > project-structure.md
octocode graphrag search --query "auth" --json
```

## Memory Management Commands

### `octocode memory`

Manage the memory system for storing insights and context.

```bash
# Store new information
octocode memory memorize \
  --title "Authentication Bug Fix" \
  --content "Fixed JWT token validation race condition" \
  --memory-type bug_fix \
  --importance 0.8 \
  --tags security,jwt,auth \
  --files src/auth.rs,src/middleware/auth.rs

# Search memories semantically
octocode memory remember "JWT authentication issues"
octocode memory remember "authentication" "security" "bugs"

# Retrieve specific memory
octocode memory get abc123

# Update existing memory
octocode memory update abc123 --add-tags performance

# Filter memories
octocode memory by-type bug_fix
octocode memory by-tags security,auth
octocode memory for-files src/auth.rs

# List recent memories
octocode memory recent --limit 10

# Memory statistics
octocode memory stats

# Create relationships between memories
octocode memory relate source-id target-id

# Cleanup old memories
octocode memory cleanup

# Delete specific memory
octocode memory forget --memory-id abc123

# Delete all memories (careful!)
octocode memory clear-all --yes
```

**Memory types:**
- `code` - Code-related insights
- `architecture` - Architectural decisions
- `bug_fix` - Bug reports and solutions
- `feature` - Feature implementations
- `documentation` - Documentation notes
- `user_preference` - User preferences
- `decision` - Project decisions
- `learning` - Insights and lessons
- `configuration` - Setup notes
- `testing` - Test strategies
- `performance` - Performance optimizations
- `security` - Security considerations
- `insight` - General observations

## Utility Commands

### `octocode format`

Format code according to .editorconfig rules.

```bash
# Format all supported files
octocode format

# Preview changes without applying
octocode format --dry-run

# Format specific files
octocode format src/main.rs src/lib.rs

# Format and commit changes
octocode format --commit

# Verbose output
octocode format --verbose
```

### `octocode logs`

View MCP server logs.

```bash
# View logs for current project
octocode logs

# Follow logs in real-time
octocode logs --follow

# Show only error logs
octocode logs --errors-only

# Show more/fewer lines
octocode logs --lines 50

# View logs for all projects
octocode logs --all
```

### `octocode watch`

Auto-index files when they change.

```bash
# Basic watch mode
octocode watch

# Custom debounce timing (1-30 seconds)
octocode watch --debounce 5

# Custom additional delay (0-5000ms)
octocode watch --additional-delay 2000

# Combine timing options
octocode watch --debounce 3 --additional-delay 1500

# Quiet mode (less output)
octocode watch --quiet

# Watch without git requirements
octocode watch --no-git
```

### `octocode clear`

Clear database tables (preserves memory tables).

```bash
# Clear all data (preserves memory tables)
octocode clear --mode all

# Clear specific data types
octocode clear --mode code
octocode clear --mode docs
octocode clear --mode text

# Default mode (all)
octocode clear
```

**Note**: The clear command now preserves memory-related tables to maintain your memory system data. Use `octocode memory clear-all` to clear memories specifically.

### `octocode completion`

Generate shell completion scripts.

```bash
# Generate completions for your shell
octocode completion bash > ~/.bash_completion.d/octocode
octocode completion zsh > ~/.zsh/completions/_octocode
octocode completion fish > ~/.config/fish/completions/octocode.fish

# Or install using make (if available)
make install-completions
```

## Global Options

Most commands support these global options:

```bash
# Verbose output
octocode <command> --verbose

# JSON output (where applicable)
octocode <command> --json

# Markdown output (where applicable)
octocode <command> --md

# Help for any command
octocode <command> --help
octocode help <command>
```

## Command Combinations

### Complete Reindex Workflow

```bash
# Clear old data and reindex
octocode clear --all --yes
octocode index --verbose

# Start MCP server
octocode mcp --path . &
```

### Daily Development Workflow

```bash
# Start watching for changes
octocode watch --quiet &

# Work on code...
# Files are automatically indexed

# Search for relevant code
octocode search "error handling patterns"

# Commit with AI assistance
git add .
octocode commit

# Review changes
octocode review --focus security
```

### Documentation Generation

```bash
# Generate comprehensive documentation
octocode view "src/**/*.rs" --md > docs/api-reference.md
octocode graphrag overview --md > docs/architecture.md

# Create project structure overview
octocode search "main components" --md > docs/components.md
```

### Batch Memory Operations

```bash
# Store multiple related memories
octocode memory memorize --title "Auth System" --content "..." --tags auth,security
octocode memory memorize --title "DB Layer" --content "..." --tags database,performance

# Search across all memories
octocode memory remember "system architecture"

# Get statistics
octocode memory stats
```

For more detailed information about specific features, see:
- [Advanced Usage](ADVANCED_USAGE.md) - Advanced workflows and techniques
- [MCP Integration](MCP_INTEGRATION.md) - Detailed MCP server setup
- [Configuration](CONFIGURATION.md) - Complete configuration reference



================================================
FILE: doc/CONFIGURATION.md
================================================
# Configuration

Octocode stores configuration in `~/.local/share/octocode/config.toml`. View current settings with:

```bash
octocode config --show
```

## Quick Setup Examples

### Local Embedding Models (No API Keys Required)

```bash
# Use SentenceTransformer (recommended for quality)
octocode config \
  --code-embedding-model "huggingface:microsoft/codebert-base" \
  --text-embedding-model "huggingface:sentence-transformers/all-mpnet-base-v2"

# Use FastEmbed (recommended for speed)
octocode config \
  --code-embedding-model "fastembed:Xenova/all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:intfloat/multilingual-e5-small"

# Mix providers as needed
octocode config \
  --code-embedding-model "huggingface:microsoft/codebert-base" \
  --text-embedding-model "fastembed:intfloat/multilingual-e5-small"
```

### Cloud Embedding Models (API Keys Required)

```bash
# Use cloud providers for highest quality
octocode config \
  --code-embedding-model "jina:jina-embeddings-v2-base-code" \
  --text-embedding-model "voyage:voyage-3.5-lite"

# OpenAI models (high quality)
octocode config \
  --code-embedding-model "openai:text-embedding-3-small" \
  --text-embedding-model "openai:text-embedding-3-small"

# Google models
octocode config \
  --code-embedding-model "google:text-embedding-004" \
  --text-embedding-model "google:text-embedding-004"
```

## Configuration File Structure

```toml
[openrouter]
model = "openai/gpt-4o-mini"
api_key = "your-openrouter-key"  # Or set OPENROUTER_API_KEY env var

[embedding]
# Direct model configuration - provider auto-detected from prefix
code_model = "huggingface:microsoft/codebert-base"
text_model = "huggingface:sentence-transformers/all-mpnet-base-v2"

# Provider-specific sections only for API keys
[embedding.jina]
api_key = "your-jina-key"  # Or set JINA_API_KEY env var

[embedding.voyage]
api_key = "your-voyage-key"  # Or set VOYAGE_API_KEY env var

[embedding.google]
api_key = "your-google-key"  # Or set GOOGLE_API_KEY env var

[graphrag]
enabled = true
description_model = "openai/gpt-4o-mini"
relationship_model = "openai/gpt-4o-mini"

[search]
max_results = 50
similarity_threshold = 0.1

[index]
chunk_size = 2000
graphrag_enabled = true

[memory]
enabled = true
max_memories = 10000
```

## Embedding Providers

### Supported Providers

| Provider | Format | API Key Required | Local/Cloud | Quality | Speed |
|----------|--------|------------------|-------------|---------|-------|
| **SentenceTransformer** | `huggingface:model-name` | ❌ No | 🖥️ Local | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **FastEmbed** | `fastembed:model-name` | ❌ No | 🖥️ Local | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Jina AI** | `jina:model-name` | ✅ Yes | ☁️ Cloud | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Voyage AI** | `voyage:model-name` | ✅ Yes | ☁️ Cloud | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Google** | `google:model-name` | ✅ Yes | ☁️ Cloud | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |

### Model Recommendations

#### For Code Understanding (code_model)

**Best Quality:**
```bash
huggingface:microsoft/codebert-base                    # 768 dim, BERT, excellent for code
huggingface:jinaai/jina-embeddings-v2-base-code       # 768 dim, JinaBERT, code-optimized
jina:jina-embeddings-v2-base-code                     # 768 dim, specialized for code
voyage:voyage-code-3                                  # 1024 dim, latest code model
openai:text-embedding-3-small                         # 1536 dim, versatile for code
```

**Fast Local:**
```bash
fastembed:Xenova/all-MiniLM-L6-v2                     # 384 dim, fast and efficient
fastembed:Xenova/bge-small-en-v1.5                    # 384 dim, good balance
```

#### For Text Understanding (text_model)

**Best Quality:**
```bash
huggingface:sentence-transformers/all-mpnet-base-v2   # 768 dim, BERT, excellent quality
huggingface:BAAI/bge-base-en-v1.5                     # 768 dim, BERT, high performance
jina:jina-embeddings-v3                               # 1024 dim, latest Jina model
voyage:voyage-3.5-lite                                # 1024 dim, excellent for text
openai:text-embedding-3-large                         # 3072 dim, highest quality
openai:text-embedding-3-small                         # 1536 dim, cost-effective
```

**Fast Local:**
```bash
fastembed:intfloat/multilingual-e5-small                 # 384 dim, supports multiple languages
huggingface:sentence-transformers/all-MiniLM-L6-v2    # 384 dim, BERT, fast
```

**Note**: HuggingFace provider supports BERT and JinaBERT architectures with automatic dimension detection.
```

## Environment Variables

```bash
# OpenRouter for AI features
export OPENROUTER_API_KEY="your-openrouter-api-key"

# Cloud embedding providers (if using)
export JINA_API_KEY="your-jina-key"
export VOYAGE_API_KEY="your-voyage-key"
export GOOGLE_API_KEY="your-google-key"
export OPENAI_API_KEY="your-openai-key"
```

**Note**: Environment variables always take priority over config file settings.

## Configuration Sections

### [openrouter]
Controls AI model usage for GraphRAG and Git features.

- `model`: OpenRouter model identifier (default: "openai/gpt-4o-mini")
- `api_key`: API key (prefer environment variable)

### [embedding]
Core embedding configuration.

- `code_model`: Model for code embedding
- `text_model`: Model for text/documentation embedding

### [graphrag]

Knowledge graph generation settings.

- `enabled`: Enable/disable GraphRAG features
- `use_llm`: Enable AI-powered relationship discovery and file descriptions

### [graphrag.llm]

LLM-specific configuration for GraphRAG AI features.

- `description_model`: Model for generating file descriptions
- `relationship_model`: Model for extracting relationships
- `ai_batch_size`: Number of files to analyze per AI call for cost optimization (default: 8)
- `max_batch_tokens`: Maximum tokens per batch request to avoid model limits (default: 16384)
- `batch_timeout_seconds`: Timeout for batch AI requests in seconds (default: 60)
- `fallback_to_individual`: Whether to fallback to individual AI calls if batch fails (default: true)
- `max_sample_tokens`: Maximum content sample size sent to AI (default: 1500)
- `confidence_threshold`: Confidence threshold for AI relationships (default: 0.8)
- `architectural_weight`: Weight for AI-discovered relationships (default: 0.9)
- `relationship_system_prompt`: System prompt for relationship discovery
- `description_system_prompt`: System prompt for file descriptions

**Performance Note**: Increasing `ai_batch_size` reduces API costs by processing multiple files per request, but may increase latency. Adjust `max_batch_tokens` to stay within model context limits.

### [search]
Search behavior configuration.

- `max_results`: Maximum search results to return
- `similarity_threshold`: Minimum similarity score for results

### [index]
Indexing behavior settings.

- `chunk_size`: Size of text chunks for embedding
- `graphrag_enabled`: Enable GraphRAG during indexing

### [memory]
Memory system configuration.

- `enabled`: Enable/disable memory features
- `max_memories`: Maximum number of memories to store

## Command Line Configuration

```bash
# View current configuration
octocode config --show

# Set embedding models
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"
octocode config --text-embedding-model "fastembed:multilingual-e5-small"

# Set OpenRouter model
octocode config --model "anthropic/claude-3.5-sonnet"

# Enable/disable GraphRAG
octocode config --graphrag-enabled true
octocode config --graphrag-enabled false

# Set search parameters
octocode config --max-results 100
octocode config --similarity-threshold 0.3
```

## MCP Server Configuration

### Basic MCP Setup

```bash
# Start MCP server with default settings
octocode mcp --path /path/to/project

# Start with custom port
octocode mcp --path /path/to/project --port 3001

# Start with debug logging
octocode mcp --path /path/to/project --debug
```

### LSP Integration

```bash
# Enable LSP integration with Rust
octocode mcp --path /path/to/rust/project --with-lsp "rust-analyzer"

# Enable LSP integration with Python
octocode mcp --path /path/to/python/project --with-lsp "pylsp"

# Enable LSP integration with TypeScript
octocode mcp --path /path/to/ts/project --with-lsp "typescript-language-server --stdio"

# Custom LSP server with arguments
octocode mcp --path /path/to/project --with-lsp "custom-lsp --config config.json"
```

### MCP Configuration File

The MCP server uses command-line arguments rather than configuration file settings. The main configuration is handled through the existing `config.toml` structure:

```toml
# Octocode configuration (config-templates/default.toml)
version = 1

[openrouter]
model = "openai/gpt-4.1-mini"
base_url = "https://openrouter.ai/api/v1"
timeout = 120

[index]
chunk_size = 2000
chunk_overlap = 100
embeddings_batch_size = 16
require_git = true

[search]
max_results = 20
similarity_threshold = 0.65
output_format = "markdown"

[embedding]
code_model = "voyage:voyage-code-3"
text_model = "voyage:voyage-3.5-lite"

[graphrag]
enabled = false
use_llm = false
```

**Note**: MCP server settings like port, debug mode, and LSP integration are controlled via command-line flags, not configuration file options.

### Claude Desktop Integration

Add to your Claude Desktop configuration file:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "octocode": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/your/project"]
    },
    "octocode-with-lsp": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/your/project", "--with-lsp", "rust-analyzer"]
    }
  }
}
```

### Multiple Projects Setup

```json
{
  "mcpServers": {
    "octocode-rust": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/rust/project", "--with-lsp", "rust-analyzer", "--port", "3001"]
    },
    "octocode-python": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/python/project", "--with-lsp", "pylsp", "--port", "3002"]
    },
    "octocode-typescript": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/ts/project", "--with-lsp", "typescript-language-server --stdio", "--port", "3003"]
    }
  }
}
```

## Performance Tuning

### For Speed
```toml
[embedding]
code_model = "fastembed:all-MiniLM-L6-v2"
text_model = "fastembed:multilingual-e5-small"

[index]
chunk_size = 1000
embeddings_batch_size = 64

[search]
max_results = 20
```

### For Quality
```toml
[embedding]
code_model = "huggingface:microsoft/codebert-base"
text_model = "huggingface:sentence-transformers/all-mpnet-base-v2"

[index]
chunk_size = 2000

[search]
max_results = 50
similarity_threshold = 0.1
```

### For Large Codebases
```toml
[index]
chunk_size = 1500
embeddings_batch_size = 32

[search]
max_results = 30
similarity_threshold = 0.2

[memory]
max_memories = 50000
```



================================================
FILE: doc/CONTRIBUTING.md
================================================
# Contributing to Octocode

We welcome contributions! This project is part of the larger Muvon ecosystem and follows our open-source contribution guidelines.

## Development Setup

### Prerequisites

- **Rust 1.70+** (install from [rustup.rs](https://rustup.rs/))
- **Git** for version control
- **Basic understanding** of Rust, embeddings, and vector databases

### Getting Started

```bash
# Clone the repository
git clone https://github.com/muvon/octocode.git
cd octocode

# Build the project (MANDATORY: always use --no-default-features)
cargo build --no-default-features

# Run tests
cargo test --no-default-features

# Check code quality
cargo check --no-default-features --message-format=short
cargo clippy --no-default-features

# Run with debug logging
RUST_LOG=debug cargo run -- index
```

### Development Dependencies

The project uses several key dependencies:

- **Tree-sitter**: For parsing multiple programming languages
- **Lance**: Vector database for embeddings storage
- **Tokio**: Async runtime
- **Clap**: Command-line interface
- **Serde**: Serialization/deserialization
- **Reqwest**: HTTP client for API calls

## Project Structure

```
octocode/
├── src/
│   ├── main.rs              # CLI entry point
│   ├── config/              # Configuration management
│   ├── indexer/             # Code indexing and parsing
│   │   ├── languages/       # Language-specific parsers
│   │   └── embeddings/      # Embedding providers
│   ├── search/              # Search engine implementation
│   ├── graphrag/            # Knowledge graph functionality
│   ├── memory/              # Memory management system
│   ├── git/                 # Git integration features
│   ├── mcp/                 # MCP server implementation
│   └── utils/               # Utility functions
├── tests/                   # Integration tests
├── docs/                    # Documentation
└── examples/                # Usage examples
```

## Adding Language Support

Language parsers are located in `src/indexer/languages/`. Each language needs:

### 1. Tree-sitter Grammar Dependency

Add the tree-sitter grammar to `Cargo.toml`:

```toml
[dependencies]
tree-sitter-your-language = "0.x.x"
```

### 2. Language Implementation

Create `src/indexer/languages/your_lang.rs`:

```rust
use tree_sitter::{Language, Query};
use crate::indexer::languages::{LanguageParser, ParsedSymbol, SymbolType};

pub struct YourLanguageParser;

impl LanguageParser for YourLanguageParser {
    fn language() -> Language {
        tree_sitter_your_language::language()
    }

    fn file_extensions() -> &'static [&'static str] {
        &[".your_ext"]
    }

    fn extract_symbols(&self, source: &str) -> Vec<ParsedSymbol> {
        // Implementation for extracting functions, classes, etc.
        vec![]
    }

    fn extract_imports(&self, source: &str) -> Vec<String> {
        // Implementation for extracting import statements
        vec![]
    }

    fn extract_exports(&self, source: &str) -> Vec<String> {
        // Implementation for extracting export statements
        vec![]
    }
}
```

### 3. Registration

Add to `src/indexer/languages/mod.rs`:

```rust
pub mod your_lang;

// In the get_parser function:
match extension {
    // ... existing cases
    ".your_ext" => Some(Box::new(your_lang::YourLanguageParser)),
    _ => None,
}
```

### 4. Testing

Create tests in `tests/languages/test_your_lang.rs`:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_your_language_parsing() {
        let source = r#"
            // Your language sample code
        "#;

        let parser = YourLanguageParser;
        let symbols = parser.extract_symbols(source);

        assert!(!symbols.is_empty());
        // Add specific assertions
    }
}
```

## Adding Embedding Providers

Embedding providers are in `src/embedding/provider/`. To add a new provider:

1. Create provider file (e.g., `your_provider.rs`)
2. Implement the `EmbeddingProvider` trait
3. Add to module exports in `mod.rs`

Supported providers: FastEmbed, Jina, Voyage, Google, HuggingFace (BERT/JinaBERT), OpenAI

### 1. Provider Implementation

Create `src/indexer/embeddings/your_provider.rs`:

```rust
use async_trait::async_trait;
use crate::indexer::embeddings::{EmbeddingProvider, EmbeddingResult};

pub struct YourProvider {
    api_key: String,
    model: String,
}

#[async_trait]
impl EmbeddingProvider for YourProvider {
    async fn embed_texts(&self, texts: &[String]) -> EmbeddingResult<Vec<Vec<f32>>> {
        // Implementation for generating embeddings
        Ok(vec![])
    }

    fn model_name(&self) -> &str {
        &self.model
    }

    fn dimensions(&self) -> usize {
        // Return embedding dimensions
        768
    }
}
```

### 2. Provider Registration

Add to `src/indexer/embeddings/mod.rs`:

```rust
pub mod your_provider;

// In the create_provider function:
if model.starts_with("yourprovider:") {
    let model_name = model.strip_prefix("yourprovider:").unwrap();
    return Ok(Box::new(your_provider::YourProvider::new(api_key, model_name)?));
}
```

## Code Style and Guidelines

### Rust Style

- Follow standard Rust formatting (`cargo fmt`)
- Use `cargo clippy` for linting
- Write comprehensive tests for new features
- Document public APIs with rustdoc comments

### Error Handling

Use the project's error types:

```rust
use crate::error::{OctocodeError, Result};

fn your_function() -> Result<String> {
    // Use ? operator for error propagation
    let result = some_operation()?;
    Ok(result)
}
```

### Async Code

Use `tokio` for async operations:

```rust
use tokio::fs;

async fn read_file(path: &str) -> Result<String> {
    let content = fs::read_to_string(path).await?;
    Ok(content)
}
```

## Testing

### Running Tests

```bash
# Run all tests
cargo test

# Run specific test module
cargo test test_rust_parser

# Run with output
cargo test -- --nocapture

# Run integration tests
cargo test --test integration
```

### Test Categories

1. **Unit Tests**: Test individual functions and modules
2. **Integration Tests**: Test complete workflows
3. **Language Tests**: Test language parser implementations
4. **Embedding Tests**: Test embedding provider integrations

### Writing Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_indexing_workflow() {
        let temp_dir = TempDir::new().unwrap();
        // Test implementation
    }

    #[test]
    fn test_symbol_extraction() {
        let source = "fn main() {}";
        let symbols = extract_symbols(source);
        assert_eq!(symbols.len(), 1);
    }
}
```

## Documentation

### Code Documentation

Use rustdoc comments for public APIs:

```rust
/// Extracts symbols from source code using tree-sitter parsing.
///
/// # Arguments
///
/// * `source` - The source code to parse
/// * `language` - The programming language
///
/// # Returns
///
/// A vector of parsed symbols including functions, classes, and variables.
///
/// # Examples
///
/// ```
/// let symbols = extract_symbols("fn main() {}", Language::Rust);
/// assert!(!symbols.is_empty());
/// ```
pub fn extract_symbols(source: &str, language: Language) -> Vec<ParsedSymbol> {
    // Implementation
}
```

### Updating Documentation

When adding features, update:

1. **README.md**: If it affects the main workflow
2. **doc/CONFIGURATION.md**: For new configuration options
3. **doc/ADVANCED_USAGE.md**: For new advanced features
4. **doc/ARCHITECTURE.md**: For architectural changes

## Submitting Changes

### Pull Request Process

1. **Fork the repository** and create a feature branch
2. **Make your changes** following the style guidelines
3. **Add tests** for new functionality
4. **Update documentation** as needed
5. **Run the test suite** to ensure everything passes
6. **Submit a pull request** with a clear description

### Commit Messages

Follow conventional commit format:

```
feat(indexer): add support for Go language parsing

- Implement Go-specific symbol extraction
- Add import/export detection for Go modules
- Include comprehensive test coverage

Closes #123
```

### PR Description Template

```markdown
## Description
Brief description of the changes.

## Type of Change
- [ ] Bug fix
- [ ] New feature
- [ ] Breaking change
- [ ] Documentation update

## Testing
- [ ] Unit tests added/updated
- [ ] Integration tests added/updated
- [ ] Manual testing performed

## Checklist
- [ ] Code follows style guidelines
- [ ] Self-review completed
- [ ] Documentation updated
- [ ] Tests pass locally
```

## Release Process

### Version Numbering

We follow [Semantic Versioning](https://semver.org/):

- **MAJOR**: Breaking changes
- **MINOR**: New features (backward compatible)
- **PATCH**: Bug fixes (backward compatible)

### Release Checklist

1. Update version in `Cargo.toml`
2. Update `CHANGELOG.md`
3. Run full test suite
4. Create release tag
5. Build and test release binary
6. Update documentation

## Getting Help

### Communication Channels

- **GitHub Issues**: Bug reports and feature requests
- **Email**: [opensource@muvon.io](mailto:opensource@muvon.io)
- **Discussions**: GitHub Discussions for questions

### Reporting Issues

When reporting bugs, include:

1. **Environment**: OS, Rust version, Octocode version
2. **Steps to reproduce**: Clear reproduction steps
3. **Expected behavior**: What should happen
4. **Actual behavior**: What actually happens
5. **Logs**: Relevant error messages or debug output

### Feature Requests

For feature requests, provide:

1. **Use case**: Why is this feature needed?
2. **Proposed solution**: How should it work?
3. **Alternatives**: Other approaches considered
4. **Additional context**: Any other relevant information

## Code of Conduct

We are committed to providing a welcoming and inclusive environment. Please:

- Be respectful and constructive in discussions
- Focus on what is best for the community
- Show empathy towards other community members
- Accept constructive criticism gracefully

## License

By contributing to Octocode, you agree that your contributions will be licensed under the Apache License 2.0.



================================================
FILE: doc/GETTING_STARTED.md
================================================
# Getting Started with Octocode

This guide will help you get up and running with Octocode quickly.

## Prerequisites

Before you start, make sure you have:

- **Octocode installed** - See [Installation Guide](../INSTALL.md)
- **API keys configured** - See [API Keys Setup](API_KEYS.md)
- **Git repository** - Octocode works best with git repositories

## First Steps

### 1. Navigate to Your Project

```bash
cd /path/to/your/project
```

### 2. Index Your Codebase

```bash
# Index current directory
octocode index

# Watch for progress
octocode index --verbose
```

**What happens during indexing:**
- Scans all supported files in your project
- Extracts code symbols and structure
- Generates embeddings for semantic search
- Builds knowledge graph relationships
- Stores everything in local database

### 3. Try Your First Search

```bash
# Basic semantic search
octocode search "user authentication"

# Search specific content types
octocode search "database connection" --mode code
octocode search "API documentation" --mode docs
```

### 4. Explore Multi-Query Search

```bash
# Combine related terms for comprehensive results
octocode search "authentication" "middleware"
octocode search "jwt" "token" "validation"
octocode search "database" "connection" "pool"
```

## Basic Workflow

### Daily Development Cycle

```bash
# 1. Start watching for changes (optional)
octocode watch &

# 2. Work on your code...
# Files are automatically indexed as you work

# 3. Search for relevant code
octocode search "error handling patterns"

# 4. Use AI-powered git workflow
git add .
octocode commit  # Generates intelligent commit message

# 5. Review your changes
octocode review --focus security
```

### Working with Memory System

```bash
# Store important insights
octocode memory memorize \
  --title "Authentication Bug Fix" \
  --content "Fixed JWT token validation race condition" \
  --memory-type bug_fix \
  --tags security,jwt

# Search your memory
octocode memory remember "JWT authentication issues"

# Get memories by type
octocode memory by-type bug_fix
```

## Configuration Basics

### View Current Configuration

```bash
octocode config --show
```

### Essential Configuration

```bash
# Use faster local models (macOS only)
octocode config \
  --code-embedding-model "fastembed:Xenova/all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:intfloat/multilingual-e5-small"

# Enable GraphRAG for relationship analysis
octocode config --graphrag-enabled true

# Set search preferences
octocode config --max-results 20 --similarity-threshold 0.3
```

## MCP Server for AI Assistants

### Basic MCP Setup

1. **Start the server:**
   ```bash
   octocode mcp --path /path/to/your/project
   ```

2. **Configure in Claude Desktop:**
   ```json
   {
     "mcpServers": {
       "octocode": {
         "command": "octocode",
         "args": ["mcp", "--path", "/path/to/your/project"]
       }
     }
   }
   ```

3. **Use with AI assistant:**
   - Ask: "Search for authentication functions in my codebase"
   - Ask: "What are the main components in this project?"
   - Ask: "Remember this bug fix for future reference"

### With LSP Integration

```bash
# Start with language server support
octocode mcp --path /path/to/rust/project --with-lsp "rust-analyzer"
octocode mcp --path /path/to/python/project --with-lsp "pylsp"
```

## Common Use Cases

### Code Exploration

```bash
# Understand new codebase
octocode view "**/*.rs" --md > project-overview.md
octocode graphrag overview --md > architecture.md

# Find similar patterns
octocode search "error handling" --expand
```

### Debugging and Maintenance

```bash
# Find related code
octocode search "authentication" "session" "login"

# Search for specific patterns
octocode search "TODO" "FIXME" --mode all

# Review recent changes
git add .
octocode review --severity high
```

### Documentation Generation

```bash
# Generate API documentation
octocode view "src/**/*.rs" --json > api-docs.json

# Create project structure overview
octocode graphrag overview --md > STRUCTURE.md
```

## Troubleshooting

### Slow Indexing

```bash
# Use faster embedding models
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"

# Disable GraphRAG temporarily
octocode config --graphrag-enabled false
```

### Poor Search Results

```bash
# Adjust similarity threshold
octocode config --similarity-threshold 0.1  # More results
octocode config --similarity-threshold 0.5  # Fewer, more relevant results

# Try different search modes
octocode search "your query" --mode code     # Only code
octocode search "your query" --mode docs     # Only documentation
```

### API Rate Limits

```bash
# Switch to local models (macOS only)
octocode config \
  --code-embedding-model "fastembed:Xenova/all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:intfloat/multilingual-e5-small"
```

## Next Steps

Once you're comfortable with the basics:

1. **Explore Advanced Features** - See [Advanced Usage](ADVANCED_USAGE.md)
2. **Optimize Performance** - See [Performance Guide](PERFORMANCE.md)
3. **Set Up MCP Integration** - See [MCP Integration](MCP_INTEGRATION.md)
4. **Configure for Your Workflow** - See [Configuration Guide](CONFIGURATION.md)

## Getting Help

- **Documentation**: Browse all guides in the `doc/` directory
- **Issues**: [GitHub Issues](https://github.com/Muvon/octocode/issues)
- **Email**: [opensource@muvon.io](mailto:opensource@muvon.io)



================================================
FILE: doc/LSP_INTEGRATION.md
================================================
# LSP Integration Guide

## Overview

Octocode integrates with Language Server Protocol (LSP) to provide enhanced code navigation and analysis capabilities through the MCP server. This integration allows AI assistants to perform intelligent code operations like go-to-definition, hover information, find references, and code completion.

## Quick Start

### Starting MCP Server with LSP

```bash
# Basic MCP server
octocode mcp --path /path/to/your/project

# With LSP integration
octocode mcp --path /path/to/your/project --with-lsp "rust-analyzer"
```

### Claude Desktop Configuration

Add to your Claude Desktop configuration:

```json
{
  "mcpServers": {
    "octocode": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/your/project", "--with-lsp", "rust-analyzer"]
    }
  }
}
```

## Supported Language Servers

### Rust
```bash
octocode mcp --path /path/to/rust/project --with-lsp "rust-analyzer"
```

### Python
```bash
# Using pylsp
octocode mcp --path /path/to/python/project --with-lsp "pylsp"

# Using pyright
octocode mcp --path /path/to/python/project --with-lsp "pyright-langserver --stdio"
```

### TypeScript/JavaScript
```bash
octocode mcp --path /path/to/ts/project --with-lsp "typescript-language-server --stdio"
```

### Go
```bash
octocode mcp --path /path/to/go/project --with-lsp "gopls"
```

### C/C++
```bash
octocode mcp --path /path/to/cpp/project --with-lsp "clangd"
```

### Java
```bash
octocode mcp --path /path/to/java/project --with-lsp "jdtls"
```

## Available LSP Tools

### lsp_goto_definition

Navigate to the definition of a symbol.

**Parameters:**
- `file_path` (string): Relative path to the file
- `line` (integer): Line number (1-indexed)
- `symbol` (string): Symbol name to find definition for

**Example:**
```json
{
  "file_path": "src/main.rs",
  "line": 15,
  "symbol": "println"
}
```

**Response:**
```
Definition found at std/io.rs:1234:5
```

### lsp_hover

Get detailed information about a symbol including type information, documentation, and signatures.

**Parameters:**
- `file_path` (string): Relative path to the file
- `line` (integer): Line number (1-indexed)
- `symbol` (string): Symbol name to get information for

**Example:**
```json
{
  "file_path": "src/auth.rs",
  "line": 42,
  "symbol": "authenticate_user"
}
```

**Response:**
```
Hover info (42:5-42:20):
fn authenticate_user(username: &str, password: &str) -> Result<User, AuthError>

Authenticates a user with the provided credentials.
Returns the authenticated user or an authentication error.
```

### lsp_find_references

Find all references to a symbol across the workspace.

**Parameters:**
- `file_path` (string): Relative path to the file
- `line` (integer): Line number (1-indexed)
- `symbol` (string): Symbol name to find references for
- `include_declaration` (boolean, optional): Include symbol declaration in results (default: true)

**Example:**
```json
{
  "file_path": "src/auth.rs",
  "line": 42,
  "symbol": "authenticate_user",
  "include_declaration": true
}
```

**Response:**
```
Found 5 reference(s):
1. src/auth.rs:42:5
2. src/api/login.rs:15:12
3. src/middleware/auth.rs:28:8
4. tests/auth_test.rs:35:9
5. tests/integration_test.rs:67:15
```

### lsp_document_symbols

List all symbols in a document with their types and locations.

**Parameters:**
- `file_path` (string): Relative path to the file

**Example:**
```json
{
  "file_path": "src/auth.rs"
}
```

**Response:**
```
Found 8 symbol(s):
1. User (struct) at 5:1
2. AuthError (enum) at 12:1
3. authenticate_user (function) at 25:1
4. hash_password (function) at 45:1
5. verify_password (function) at 58:1
6. generate_token (function) at 72:1
7. validate_token (function) at 85:1
8. refresh_token (function) at 98:1
```

### lsp_workspace_symbols

Search for symbols across the entire workspace.

**Parameters:**
- `query` (string): Symbol search query

**Example:**
```json
{
  "query": "auth"
}
```

**Response:**
```
Found 12 symbol(s) in workspace:
1. authenticate_user (function) in src/auth.rs:25
2. AuthError (enum) in src/auth.rs:12
3. AuthMiddleware (struct) in src/middleware/auth.rs:8
4. auth_required (function) in src/middleware/auth.rs:35
5. AuthConfig (struct) in src/config.rs:45
...
```

### lsp_completion

Get code completion suggestions at a specific position.

**Parameters:**
- `file_path` (string): Relative path to the file
- `line` (integer): Line number (1-indexed)
- `symbol` (string): Partial symbol or prefix to complete

**Example:**
```json
{
  "file_path": "src/api.rs",
  "line": 25,
  "symbol": "std::vec"
}
```

**Response:**
```
Found 5 completion(s):
1. Vec (struct) - A contiguous growable array type
2. VecDeque (struct) - A double-ended queue implemented with a growable ring buffer
3. vec! (macro) - Creates a Vec containing the arguments
4. vector (module) - Vector utilities
5. vec_map (module) - A vector-based map implementation
```

## Symbol Resolution

The LSP integration uses intelligent symbol resolution to find symbols on specified lines:

### Resolution Strategies

1. **Exact Match with Word Boundaries**: Finds exact symbol matches respecting word boundaries
2. **Substring Search**: Finds symbols as substrings within the line
3. **Case-Insensitive Match**: Falls back to case-insensitive matching
4. **Partial Identifier Matching**: Finds symbols within larger identifiers
5. **Namespace Handling**: Handles qualified names like `std::vec::Vec`
6. **Intelligent Fallback**: Uses first meaningful identifier if exact symbol not found

### Examples

**Line:** `let result = authenticate_user(username, password);`

- Symbol `authenticate_user` → Found at position 14
- Symbol `user` → Found within `authenticate_user` at position 14
- Symbol `auth` → Found within `authenticate_user` at position 14

**Line:** `use std::collections::HashMap;`

- Symbol `HashMap` → Found at position 21
- Symbol `std::collections::HashMap` → Found at position 5
- Symbol `collections` → Found at position 10

## Error Handling

### Common Issues

1. **LSP Server Not Found**
   ```
   Error: LSP server 'rust-analyzer' not found in PATH
   ```
   **Solution**: Install the language server and ensure it's in your PATH

2. **Symbol Not Found**
   ```
   Symbol 'unknown_symbol' not found on line 15
   ```
   **Solution**: Verify the symbol exists on the specified line or use a more general symbol

3. **File Not Opened**
   ```
   File src/main.rs not found in document contents
   ```
   **Solution**: The LSP server automatically opens files on-demand

### Debugging

Start the MCP server with debug logging:

```bash
octocode mcp --path /path/to/project --with-lsp "rust-analyzer" --debug
```

This provides detailed information about:
- LSP server startup and initialization
- Symbol resolution attempts and fallbacks
- File opening and content synchronization
- Request/response communication with the LSP server

## Advanced Configuration

### Custom LSP Server Commands

You can use any LSP-compliant language server:

```bash
# Custom command with arguments
octocode mcp --path /path/to/project --with-lsp "custom-lsp --flag value"

# Language server with specific configuration
octocode mcp --path /path/to/project --with-lsp "pylsp -v --config-file .pylsp.json"
```

### Multiple Language Support

For projects with multiple languages, start separate MCP servers:

```bash
# Terminal 1: Rust project
octocode mcp --path /path/to/rust/project --with-lsp "rust-analyzer" --port 3001

# Terminal 2: Python project
octocode mcp --path /path/to/python/project --with-lsp "pylsp" --port 3002
```

### Performance Optimization

For large projects:

1. **Use project-specific LSP configuration**
2. **Limit LSP server memory usage**
3. **Configure appropriate timeouts**
4. **Use incremental synchronization**

## Integration Examples

### With Claude Desktop

1. **Configure the MCP server:**
   ```json
   {
     "mcpServers": {
       "octocode-rust": {
         "command": "octocode",
         "args": ["mcp", "--path", "/path/to/rust/project", "--with-lsp", "rust-analyzer"]
       }
     }
   }
   ```

2. **Use in conversations:**
   ```
   Can you show me the definition of the authenticate_user function in src/auth.rs line 42?
   ```

3. **AI assistant will use:**
   ```json
   {
     "tool": "lsp_goto_definition",
     "arguments": {
       "file_path": "src/auth.rs",
       "line": 42,
       "symbol": "authenticate_user"
     }
   }
   ```

### With Other MCP Clients

The LSP tools work with any MCP-compatible client. Configure the server endpoint and use the tools programmatically or through natural language interfaces.

## Best Practices

1. **Use Specific Symbols**: Prefer exact symbol names over partial matches
2. **Combine with Semantic Search**: Use LSP tools alongside Octocode's semantic search for comprehensive code understanding
3. **Cache Results**: LSP operations can be expensive; cache results when possible
4. **Handle Errors Gracefully**: Always handle cases where symbols or definitions aren't found
5. **Use Appropriate Tools**: Choose the right LSP tool for your use case:
   - `goto_definition` for navigation
   - `hover` for documentation
   - `find_references` for understanding usage
   - `completion` for code assistance
   - `document_symbols` for file overview
   - `workspace_symbols` for project-wide search

## Troubleshooting

### LSP Server Issues

1. **Check if language server is installed:**
   ```bash
   which rust-analyzer
   which pylsp
   ```

2. **Verify language server works independently:**
   ```bash
   rust-analyzer --version
   pylsp --help
   ```

3. **Check project setup:**
   - Ensure project files are in the correct format
   - Verify language server configuration files exist
   - Check for compilation errors that might affect LSP

### Performance Issues

1. **Large Projects**: Use project-specific ignore patterns
2. **Slow Responses**: Increase timeout values or use faster hardware
3. **Memory Usage**: Monitor LSP server memory consumption
4. **Network Issues**: Ensure proper localhost connectivity

### Symbol Resolution Issues

1. **Symbol Not Found**: Try broader symbol names or check line content
2. **Wrong Position**: Verify line numbers are 1-indexed
3. **Case Sensitivity**: Use exact case or rely on fallback matching
4. **Namespace Issues**: Try both qualified and unqualified names

For additional help, see the [Advanced Usage](ADVANCED_USAGE.md) guide or open an issue on GitHub.



================================================
FILE: doc/MCP_INTEGRATION.md
================================================
# MCP Server Integration Guide

Complete guide for integrating Octocode with AI assistants using the Model Context Protocol (MCP).

## Overview

Octocode provides a built-in MCP server that enables AI assistants to interact with your codebase through semantic search, memory management, and LSP integration. The server supports both stdin/stdout mode (for direct AI assistant integration) and HTTP mode (for web-based integrations).

## Quick Start

### Basic MCP Server

```bash
# Start MCP server for current project
octocode mcp --path .

# Start for specific project
octocode mcp --path /path/to/your/project

# Start with debug logging
octocode mcp --path . --debug
```

### HTTP Mode

```bash
# Start HTTP server on specific port
octocode mcp --bind "127.0.0.1:8080" --path .

# Bind to all interfaces
octocode mcp --bind "0.0.0.0:8080" --path /path/to/project
```

## Claude Desktop Integration

### Configuration

Add to your Claude Desktop configuration file:

**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "octocode": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/your/project"]
    }
  }
}
```

### Multiple Projects

```json
{
  "mcpServers": {
    "octocode-rust": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/rust/project", "--port", "3001"]
    },
    "octocode-python": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/python/project", "--port", "3002"]
    },
    "octocode-typescript": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/ts/project", "--port", "3003"]
    }
  }
}
```

### With LSP Integration

```json
{
  "mcpServers": {
    "octocode-rust": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/rust/project", "--with-lsp", "rust-analyzer"]
    },
    "octocode-python": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/python/project", "--with-lsp", "pylsp"]
    },
    "octocode-typescript": {
      "command": "octocode",
      "args": ["mcp", "--path", "/path/to/ts/project", "--with-lsp", "typescript-language-server --stdio"]
    }
  }
}
```

## Available MCP Tools

### semantic_search

Semantic search across your codebase with multi-query support.

**Parameters:**
- `query` (string or array) - Search query or multiple queries
- `mode` (string, optional) - Search scope: "all", "code", "docs", "text"
- `detail_level` (string, optional) - Detail level: "signatures", "partial", "full"
- `max_results` (integer, optional) - Maximum results to return (1-20)
- `threshold` (number, optional) - Similarity threshold (0.0-1.0)

**Single Query Example:**
```json
{
  "query": "authentication functions",
  "mode": "code",
  "detail_level": "partial",
  "max_results": 5
}
```

**Multi-Query Example:**
```json
{
  "query": ["authentication", "middleware", "security"],
  "mode": "all",
  "detail_level": "full",
  "max_results": 10
}
```

### graphrag

Advanced relationship-aware GraphRAG operations for code analysis. Supports multiple operations for exploring the knowledge graph.

**Parameters:**
- `operation` (string, required) - Operation to perform: "search", "get-node", "get-relationships", "find-path", "overview"
- `query` (string, optional) - Search query for 'search' operation
- `node_id` (string, optional) - Node identifier for 'get-node' and 'get-relationships' operations
- `source_id` (string, optional) - Source node identifier for 'find-path' operation
- `target_id` (string, optional) - Target node identifier for 'find-path' operation
- `max_depth` (integer, optional) - Maximum path depth for 'find-path' operation (default: 3)
- `format` (string, optional) - Output format: "text", "json", "markdown" (default: "text")
- `max_tokens` (integer, optional) - Maximum tokens in output (default: 2000)

**Operation Examples:**

**Search for nodes by semantic query:**
```json
{
  "operation": "search",
  "query": "How does user authentication flow through the system?"
}
```

**Get detailed node information:**
```json
{
  "operation": "get-node",
  "node_id": "src/auth/mod.rs",
  "format": "markdown"
}
```

**Find all relationships for a node:**
```json
{
  "operation": "get-relationships",
  "node_id": "src/auth/mod.rs",
  "format": "text"
}
```

**Find connection paths between nodes:**
```json
{
  "operation": "find-path",
  "source_id": "src/auth/mod.rs",
  "target_id": "src/database/mod.rs",
  "max_depth": 3,
  "format": "markdown"
}
```

**Get graph overview and statistics:**
```json
{
  "operation": "overview",
  "format": "json"
}
```

### memorize

Store important information for future reference.

**Parameters:**
- `title` (string) - Short descriptive title
- `content` (string) - Detailed content to remember
- `memory_type` (string, optional) - Type of memory (code, bug_fix, feature, etc.)
- `importance` (number, optional) - Importance score 0.0-1.0
- `tags` (array, optional) - Tags for categorization
- `related_files` (array, optional) - Related file paths

**Example:**
```json
{
  "title": "JWT Authentication Bug Fix",
  "content": "Fixed race condition in token refresh logic by adding mutex lock around token validation",
  "memory_type": "bug_fix",
  "importance": 0.8,
  "tags": ["security", "jwt", "race-condition"],
  "related_files": ["src/auth/jwt.rs", "src/middleware/auth.rs"]
}
```

### remember

Retrieve stored information with semantic search.

**Parameters:**
- `query` (string or array) - Search query or multiple related queries
- `memory_types` (array, optional) - Filter by memory types
- `tags` (array, optional) - Filter by tags
- `related_files` (array, optional) - Filter by related files
- `limit` (integer, optional) - Maximum memories to return

**Single Query Example:**
```json
{
  "query": "JWT authentication issues",
  "memory_types": ["bug_fix", "security"],
  "limit": 5
}
```

**Multi-Query Example:**
```json
{
  "query": ["authentication", "security", "bugs"],
  "tags": ["jwt", "security"],
  "limit": 10
}
```

### forget

Remove stored information.

**Parameters:**
- `memory_id` (string, optional) - Specific memory ID to forget
- `query` (string, optional) - Query to find memories to forget
- `memory_types` (array, optional) - Filter by memory types when using query
- `tags` (array, optional) - Filter by tags when using query
- `confirm` (boolean) - Must be true to confirm deletion

**Example:**
```json
{
  "memory_id": "abc123-def456-789",
  "confirm": true
}
```

## LSP Integration Tools

When started with `--with-lsp`, additional tools become available:

### lsp_goto_definition

Navigate to symbol definition.

**Parameters:**
- `file_path` (string) - Relative path to file
- `line` (integer) - Line number (1-indexed)
- `symbol` (string) - Symbol name

**Example:**
```json
{
  "file_path": "src/main.rs",
  "line": 15,
  "symbol": "authenticate_user"
}
```

### lsp_hover

Get symbol information and documentation.

**Parameters:**
- `file_path` (string) - Relative path to file
- `line` (integer) - Line number (1-indexed)
- `symbol` (string) - Symbol name

### lsp_find_references

Find all references to a symbol.

**Parameters:**
- `file_path` (string) - Relative path to file
- `line` (integer) - Line number (1-indexed)
- `symbol` (string) - Symbol name
- `include_declaration` (boolean, optional) - Include declaration in results

### lsp_completion

Get code completion suggestions.

**Parameters:**
- `file_path` (string) - Relative path to file
- `line` (integer) - Line number (1-indexed)
- `symbol` (string) - Partial symbol to complete

### lsp_document_symbols

List all symbols in a document.

**Parameters:**
- `file_path` (string) - Relative path to file

### lsp_workspace_symbols

Search symbols across workspace.

**Parameters:**
- `query` (string) - Symbol search query

## MCP Proxy Server

For managing multiple repositories, use the MCP proxy server:

```bash
# Start proxy server
octocode mcp-proxy --bind "127.0.0.1:8080" --path /path/to/parent/directory
```

**Features:**
- Automatically discovers git repositories in the specified directory
- Creates MCP server instances for each repository
- Provides unified HTTP interface for multiple projects
- Supports dynamic repository addition/removal

**Configuration:**
```json
{
  "mcpServers": {
    "octocode-proxy": {
      "command": "octocode",
      "args": ["mcp-proxy", "--bind", "127.0.0.1:8080", "--path", "/workspace"]
    }
  }
}
```

## Usage Examples

### Code Exploration

**Ask Claude:**
> "Can you search for authentication-related code in my project?"

**Claude uses:**
```json
{
  "tool": "semantic_search",
  "arguments": {
    "query": ["authentication", "auth", "login"],
    "mode": "code",
    "max_results": 10
  }
}
```

### Architecture Understanding

**Ask Claude:**
> "How are the database components connected in this system?"

**Claude uses:**
```json
{
  "tool": "graphrag",
  "arguments": {
    "operation": "search",
    "query": "database component relationships and data flow patterns"
  }
}
```

### Memory Management

**Ask Claude:**
> "Remember this bug fix: We fixed the JWT token validation by adding proper error handling"

**Claude uses:**
```json
{
  "tool": "memorize",
  "arguments": {
    "title": "JWT Token Validation Bug Fix",
    "content": "Fixed JWT token validation by adding proper error handling to prevent authentication bypass",
    "memory_type": "bug_fix",
    "tags": ["jwt", "security", "authentication"]
  }
}
```

### Code Navigation

**Ask Claude:**
> "Show me the definition of the authenticate_user function in src/auth.rs line 42"

**Claude uses:**
```json
{
  "tool": "lsp_goto_definition",
  "arguments": {
    "file_path": "src/auth.rs",
    "line": 42,
    "symbol": "authenticate_user"
  }
}
```

## Advanced Configuration

### Custom MCP Server Settings

```bash
# Start with custom settings
octocode mcp \
  --path /path/to/project \
  --port 3001 \
  --debug \
  --with-lsp "rust-analyzer"

# HTTP mode with custom binding
octocode mcp \
  --bind "0.0.0.0:8080" \
  --path /path/to/project \
  --debug
```

### Multiple Language Servers

For projects with multiple languages, start separate MCP servers:

```bash
# Terminal 1: Rust project
octocode mcp --path /rust/project --with-lsp "rust-analyzer" --port 3001

# Terminal 2: Python project
octocode mcp --path /python/project --with-lsp "pylsp" --port 3002

# Terminal 3: TypeScript project
octocode mcp --path /ts/project --with-lsp "typescript-language-server --stdio" --port 3003
```

### Environment-Specific Configuration

```bash
# Development environment
octocode mcp --path . --debug --with-lsp "rust-analyzer"

# Production environment
octocode mcp --path /app --bind "127.0.0.1:8080" --quiet
```

## Integration with Other AI Assistants

### Generic MCP Client

Any MCP-compatible client can connect to Octocode:

```python
# Python example using MCP client library
import mcp

client = mcp.Client("octocode", ["mcp", "--path", "/path/to/project"])

# Use semantic search
result = await client.call_tool("semantic_search", {
    "query": "authentication functions",
    "mode": "code"
})
```

### HTTP API Integration

When using HTTP mode, you can integrate with web applications:

```javascript
// JavaScript example
const response = await fetch('http://localhost:8080/tools/semantic_search', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    query: ["authentication", "middleware"],
    mode: "code",
    max_results: 5
  })
});

const results = await response.json();
```

## Performance Optimization

### For Large Codebases

```bash
# Optimize for large projects
octocode mcp \
  --path /large/project \
  --with-lsp "rust-analyzer" \
  --debug

# Configure search limits
octocode config --max-results 20 --similarity-threshold 0.3
```

### Memory Management

```bash
# Monitor memory usage
octocode memory stats

# Clean up old memories periodically
octocode memory cleanup

# Limit memory storage
octocode config --max-memories 10000
```

## Troubleshooting

### MCP Server Not Starting

1. **Check path exists**: Ensure the project path is valid
2. **Check permissions**: Ensure read access to the project directory
3. **Check port availability**: Ensure the port isn't already in use
4. **Check LSP server**: Ensure language server is installed and in PATH

### AI Assistant Not Connecting

1. **Check configuration**: Verify Claude Desktop config syntax
2. **Check paths**: Ensure absolute paths in configuration
3. **Restart assistant**: Restart Claude Desktop after config changes
4. **Check logs**: Use `--debug` flag to see detailed logs

### LSP Integration Issues

1. **Check LSP server**: Verify language server works independently
2. **Check project setup**: Ensure project files are valid
3. **Check symbol resolution**: Try broader symbol names
4. **Check file paths**: Ensure files exist and are accessible

### Performance Issues

1. **Limit search results**: Use smaller `max_results` values
2. **Increase thresholds**: Use higher similarity thresholds
3. **Optimize indexing**: Use faster embedding models
4. **Monitor resources**: Check CPU and memory usage

For more detailed information, see:
- [LSP Integration Guide](LSP_INTEGRATION.md)
- [Advanced Usage](ADVANCED_USAGE.md)
- [Configuration Guide](CONFIGURATION.md)



================================================
FILE: doc/MEMORY_SYSTEM.md
================================================
# Memory System Guide

Complete guide to Octocode's memory system for storing and retrieving project insights, decisions, and context.

## Overview

Octocode's memory system allows you to store important information about your project that persists across sessions. It uses semantic search with vector embeddings to help you find relevant memories based on context, not just keywords.

## Key Features

- **Semantic Search**: Find memories using natural language queries
- **Vector Embeddings**: Powered by the same embedding models as code search
- **Memory Types**: Organize memories by category (bugs, features, architecture, etc.)
- **Tag System**: Flexible tagging for better organization
- **File Relationships**: Link memories to specific files
- **Git Integration**: Automatically tag memories with commit information
- **Importance Scoring**: Prioritize memories by importance (0.0-1.0)
- **Memory Relationships**: Create connections between related memories

## Memory Operations

### Storing Information (memorize)

```bash
# Basic memory storage
octocode memory memorize \
  --title "JWT Authentication Bug Fix" \
  --content "Fixed race condition in token refresh logic by adding mutex lock"

# Comprehensive memory with all options
octocode memory memorize \
  --title "Database Connection Pool Optimization" \
  --content "Increased pool size from 10 to 50 connections and added connection health checks. This reduced database timeout errors by 90% under high load." \
  --memory-type performance \
  --importance 0.9 \
  --tags database,performance,optimization,production \
  --files src/database/pool.rs,src/config/database.rs
```

**Required parameters:**
- `--title`: Short descriptive title
- `--content`: Detailed information to store

**Optional parameters:**
- `--memory-type`: Category of memory (see Memory Types below)
- `--importance`: Score from 0.0 to 1.0 (higher = more important)
- `--tags`: Comma-separated tags for organization
- `--files`: Comma-separated file paths related to this memory

### Searching Memories (remember)

```bash
# Basic semantic search
octocode memory remember "JWT authentication issues"

# Multi-query search for comprehensive results
octocode memory remember "authentication" "security" "bugs"

# Filter by memory types
octocode memory remember "performance issues" --memory-types performance,bug_fix

# Filter by tags
octocode memory remember "database problems" --tags database,performance

# Filter by related files
octocode memory remember "auth system" --related-files src/auth.rs

# Limit results and set minimum relevance
octocode memory remember "optimization" --limit 10 --min-relevance 0.7

# JSON output for programmatic use
octocode memory remember "security" --format json
```

### Retrieving Specific Memory (get)

```bash
# Get memory by ID
octocode memory get abc123-def456-789

# JSON output
octocode memory get abc123-def456-789 --format json
```

### Updating Memories (update)

```bash
# Add tags to existing memory
octocode memory update abc123-def456-789 --add-tags critical,hotfix

# Remove tags
octocode memory update abc123-def456-789 --remove-tags outdated

# Update importance
octocode memory update abc123-def456-789 --importance 0.8

# Add related files
octocode memory update abc123-def456-789 --add-files src/new_module.rs

# Update title and content
octocode memory update abc123-def456-789 \
  --title "Updated Authentication System" \
  --content "Completely refactored auth system with OAuth2 support"
```

### Organizing Memories

```bash
# List memories by type
octocode memory by-type bug_fix
octocode memory by-type architecture

# List memories by tags
octocode memory by-tags security,authentication
octocode memory by-tags performance

# List memories for specific files
octocode memory for-files src/auth.rs
octocode memory for-files src/database/

# List recent memories
octocode memory recent --limit 20

# Get memory statistics
octocode memory stats
```

### Memory Relationships (relate)

```bash
# Create relationship between memories
octocode memory relate source-memory-id target-memory-id

# Find related memories
octocode memory get abc123 --include-related
```

### Cleanup and Maintenance

```bash
# Clean up old, low-importance memories
octocode memory cleanup

# Delete specific memory
octocode memory forget --memory-id abc123-def456-789

# Delete memories by query (careful!)
octocode memory forget --query "outdated documentation" --confirm

# Clear all memories (very careful!)
octocode memory clear-all --yes
```

## Memory Types

Organize your memories using these predefined types:

| Type | Description | Use Cases |
|------|-------------|-----------|
| `code` | Code-related insights and patterns | Code snippets, implementation notes, coding patterns |
| `architecture` | Architectural decisions and patterns | System design, component relationships, architectural choices |
| `bug_fix` | Bug reports and solutions | Bug descriptions, root causes, solutions, workarounds |
| `feature` | Feature implementations and decisions | Feature requirements, implementation details, design decisions |
| `documentation` | Documentation notes and updates | Documentation improvements, missing docs, content updates |
| `user_preference` | User-specific preferences | Personal workflow preferences, tool configurations |
| `decision` | Project decisions and rationale | Technical decisions, trade-offs, reasoning behind choices |
| `learning` | Insights and lessons learned | Lessons from failures, best practices discovered |
| `configuration` | Setup and configuration notes | Environment setup, deployment configurations |
| `testing` | Test strategies and results | Testing approaches, test results, quality metrics |
| `performance` | Performance optimizations and metrics | Performance improvements, benchmarks, bottlenecks |
| `security` | Security considerations and fixes | Security vulnerabilities, fixes, best practices |
| `insight` | General insights and observations | General observations, patterns, insights |

## Advanced Usage

### Memory Workflows

#### Bug Tracking Workflow

```bash
# 1. Document the bug
octocode memory memorize \
  --title "User Login Timeout Issue" \
  --content "Users experiencing 30-second timeouts during login. Appears to be related to database connection pool exhaustion during peak usage." \
  --memory-type bug_fix \
  --importance 0.9 \
  --tags login,timeout,database,critical \
  --files src/auth/login.rs,src/database/pool.rs

# 2. Document the investigation
octocode memory memorize \
  --title "Login Timeout Root Cause Analysis" \
  --content "Found that connection pool size of 10 is insufficient for peak load. Database queries are queuing and timing out after 30 seconds." \
  --memory-type bug_fix \
  --importance 0.8 \
  --tags login,timeout,database,analysis

# 3. Document the solution
octocode memory memorize \
  --title "Login Timeout Fix - Increased Pool Size" \
  --content "Increased database connection pool from 10 to 50 connections. Added connection health checks and monitoring. Issue resolved." \
  --memory-type bug_fix \
  --importance 0.7 \
  --tags login,timeout,database,fixed

# 4. Search related memories later
octocode memory remember "login timeout issues"
```

#### Architecture Documentation Workflow

```bash
# Document architectural decisions
octocode memory memorize \
  --title "Microservices vs Monolith Decision" \
  --content "Decided to use modular monolith architecture instead of microservices for initial MVP. Reasons: team size (3 developers), complexity overhead, deployment simplicity. Plan to extract services later if needed." \
  --memory-type architecture \
  --importance 0.9 \
  --tags architecture,decision,monolith,microservices

# Document component relationships
octocode memory memorize \
  --title "Authentication Service Integration" \
  --content "Auth service integrates with user service via direct function calls, with database via connection pool, and with external OAuth providers via HTTP client." \
  --memory-type architecture \
  --importance 0.7 \
  --tags architecture,auth,integration \
  --files src/auth/,src/user/,src/database/
```

#### Performance Optimization Workflow

```bash
# Document performance baseline
octocode memory memorize \
  --title "API Response Time Baseline" \
  --content "Current API response times: login 500ms, search 200ms, data fetch 1.2s. Target: all under 200ms." \
  --memory-type performance \
  --importance 0.8 \
  --tags performance,baseline,api

# Document optimization attempts
octocode memory memorize \
  --title "Database Query Optimization Results" \
  --content "Added indexes on user_id and created_at columns. Login time reduced from 500ms to 150ms. Search time unchanged. Data fetch still slow." \
  --memory-type performance \
  --importance 0.7 \
  --tags performance,database,optimization
```

### Integration with MCP Server

The memory system is available through the MCP server for AI assistants:

```bash
# Start MCP server
octocode mcp --path /path/to/project
```

**Available MCP tools:**
- `memorize` - Store new information
- `remember` - Search memories semantically
- `forget` - Remove memories

**Example with Claude:**
> "Remember that we fixed the authentication bug by adding proper error handling to the JWT validation logic"

Claude will use the `memorize` tool to store this information with appropriate tags and categorization.

### Memory Search Strategies

#### Semantic Search Tips

```bash
# Use descriptive phrases
octocode memory remember "database connection issues"

# Combine multiple concepts
octocode memory remember "authentication" "security" "vulnerabilities"

# Use technical terms
octocode memory remember "JWT token validation"

# Use problem descriptions
octocode memory remember "slow API response times"
```

#### Filtering Strategies

```bash
# Find all security-related memories
octocode memory by-tags security

# Find all bug fixes
octocode memory by-type bug_fix

# Find memories for specific components
octocode memory for-files src/auth/

# Combine filters for precise results
octocode memory remember "performance" \
  --memory-types performance,bug_fix \
  --tags database,optimization
```

## Configuration

### Memory System Settings

```bash
# Enable/disable memory system
octocode config --memory-enabled true

# Set maximum number of memories
octocode config --max-memories 10000

# View memory configuration
octocode config --show | grep memory
```

### Configuration File

```toml
[memory]
enabled = true
max_memories = 10000
cleanup_threshold = 0.3  # Cleanup memories below this importance
auto_cleanup = false     # Automatically cleanup old memories
```

## Best Practices

### Effective Memory Management

1. **Use Descriptive Titles**: Make titles searchable and clear
2. **Include Context**: Add enough detail for future understanding
3. **Tag Consistently**: Develop a consistent tagging strategy
4. **Set Importance**: Use importance scores to prioritize memories
5. **Link to Files**: Associate memories with relevant code files
6. **Regular Cleanup**: Periodically clean up outdated memories

### Memory Organization

```bash
# Good memory structure
octocode memory memorize \
  --title "Redis Cache Implementation for User Sessions" \
  --content "Implemented Redis-based session caching to reduce database load. Configuration in config/redis.rs. Reduced session lookup time from 50ms to 5ms. Key pattern: session:{user_id}. TTL set to 24 hours." \
  --memory-type performance \
  --importance 0.8 \
  --tags redis,cache,sessions,performance \
  --files src/session/cache.rs,config/redis.rs
```

### Search Optimization

```bash
# Use multiple related terms
octocode memory remember "cache" "redis" "performance"

# Be specific about problems
octocode memory remember "session timeout issues"

# Include context
octocode memory remember "user authentication JWT token validation"
```

## Integration Examples

### With Development Workflow

```bash
# During development
git add .
octocode commit
# AI generates commit message

# Store context about the change
octocode memory memorize \
  --title "Added User Profile Caching" \
  --content "Implemented profile caching to reduce API calls. Uses Redis with 1-hour TTL." \
  --memory-type feature \
  --tags cache,profile,api

# Later, search for related work
octocode memory remember "profile caching implementation"
```

### With Code Review

```bash
# After code review
octocode memory memorize \
  --title "Code Review Feedback - Error Handling" \
  --content "Reviewer suggested using Result<T> instead of panicking on errors. Updated all database operations to return proper error types." \
  --memory-type learning \
  --tags code-review,error-handling,best-practices
```

### With Debugging

```bash
# Document debugging process
octocode memory memorize \
  --title "Memory Leak in User Service Debug Process" \
  --content "Used valgrind to identify memory leak in user profile loading. Issue was in string allocation in profile_parser.rs line 45. Fixed by using string references instead of owned strings." \
  --memory-type bug_fix \
  --importance 0.9 \
  --tags memory-leak,debugging,valgrind \
  --files src/user/profile_parser.rs
```

## Troubleshooting

### Memory Search Not Finding Results

1. **Check similarity threshold**: Lower the threshold for broader results
2. **Try different search terms**: Use synonyms or related concepts
3. **Check memory types**: Ensure you're searching the right categories
4. **Use multi-query search**: Combine multiple related terms

### Memory Storage Issues

1. **Check disk space**: Ensure sufficient storage for the database
2. **Check permissions**: Ensure write access to the data directory
3. **Check memory limits**: Verify max_memories configuration
4. **Check embedding configuration**: Ensure embedding models are working

### Performance Issues

1. **Limit search results**: Use smaller limit values
2. **Clean up old memories**: Remove outdated or low-importance memories
3. **Optimize queries**: Use more specific search terms
4. **Check embedding performance**: Ensure embedding generation is fast

For more information, see:
- [Getting Started](GETTING_STARTED.md) - Basic memory usage
- [MCP Integration](MCP_INTEGRATION.md) - Using memory with AI assistants
- [Configuration](CONFIGURATION.md) - Memory system configuration



================================================
FILE: doc/PERFORMANCE.md
================================================
# Performance Guide

## Performance Metrics

### Typical Performance Characteristics

| Metric | Small Project (<1k files) | Medium Project (1k-10k files) | Large Project (10k+ files) |
|--------|---------------------------|-------------------------------|----------------------------|
| **Indexing Speed** | 500+ files/second | 200-400 files/second | 100-200 files/second |
| **Search Latency** | <50ms | <100ms | <200ms |
| **Memory Usage** | 50-100MB | 100-500MB | 500MB-2GB |
| **Storage Size** | 1-10MB | 10-100MB | 100MB-1GB |
| **Startup Time** | <1s | 1-3s | 3-10s |

### Factors Affecting Performance

1. **File Size**: Larger files take longer to parse and embed
2. **Language Complexity**: Complex languages (C++, TypeScript) slower than simple ones (JSON, Markdown)
3. **Embedding Model**: Local models faster than cloud APIs
4. **Hardware**: CPU, RAM, and storage speed impact performance
5. **Network**: Cloud embedding providers depend on network latency

## Optimization Strategies

### 1. Embedding Model Selection

#### For Speed (Local Models)
```bash
# FastEmbed - Fastest local option
octocode config \
  --code-embedding-model "fastembed:all-MiniLM-L6-v2" \
  --text-embedding-model "fastembed:multilingual-e5-small"

# Optimized configuration
[embedding]
code_model = "fastembed:all-MiniLM-L6-v2"      # 384 dim, very fast
text_model = "fastembed:multilingual-e5-small"  # 384 dim, multilingual
```

#### For Quality vs Speed Balance
```bash
# SentenceTransformer - Good balance
octocode config \
  --code-embedding-model "huggingface:microsoft/codebert-base" \
  --text-embedding-model "huggingface:sentence-transformers/all-MiniLM-L6-v2"
```

#### For Maximum Quality (Cloud)
```bash
# High-quality cloud models (slower due to API calls)
octocode config \
  --code-embedding-model "voyage:voyage-code-3" \
  --text-embedding-model "voyage:voyage-3.5-lite"
```

### 2. Indexing Configuration

#### Speed-Optimized Settings
```toml
[index]
chunk_size = 1000                # Smaller chunks process faster
embeddings_batch_size = 64       # Larger batches for efficiency
graphrag_enabled = false         # Disable for faster indexing

[search]
max_results = 20                 # Limit results for faster response
similarity_threshold = 0.2       # Higher threshold = fewer results
```

#### Quality-Optimized Settings
```toml
[index]
chunk_size = 2000                # Larger chunks for better context
embeddings_batch_size = 32       # Smaller batches for stability
graphrag_enabled = true          # Enable for relationship analysis

[search]
max_results = 50                 # More comprehensive results
similarity_threshold = 0.1       # Lower threshold = more results
```

### 3. Hardware Optimization

#### CPU Optimization
- **Multi-core**: Embedding generation uses multiple cores
- **CPU Type**: Modern CPUs with AVX2 support perform better
- **Recommended**: 4+ cores for optimal performance

#### Memory Optimization
```toml
[memory]
max_memories = 10000             # Adjust based on available RAM

[search]
max_results = 30                 # Reduce for lower memory usage
```

#### Storage Optimization
- **SSD**: Significantly faster than HDD for database operations
- **NVMe**: Best performance for large codebases
- **Network Storage**: Avoid for database files

### 4. Network Optimization (Cloud Providers)

#### Reduce API Calls
```bash
# Use local models when possible
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"

# Batch operations
octocode clear && octocode index  # Index all at once vs incremental
```

#### API Rate Limiting & LLM Batching
```toml
[embedding]
# Adjust batch sizes for API limits
embeddings_batch_size = 16       # Smaller batches for cloud APIs

[graphrag.llm]
# Optimize AI call costs and rate limits
ai_batch_size = 8               # Process multiple files per AI call
max_batch_tokens = 16384        # Stay within model context limits
fallback_to_individual = true   # Reliability if batch processing fails
```

**LLM Cost Optimization**: The `ai_batch_size` parameter significantly reduces API costs by processing multiple files in a single request. With the default value of 8, you get ~87% fewer API calls compared to individual processing. Increase for more savings, decrease if hitting rate limits.

## Performance Monitoring

### Built-in Metrics

```bash
# Enable debug logging for performance metrics
RUST_LOG=debug octocode index

# Monitor indexing progress
octocode clear && octocode index 2>&1 | grep "Processed"

# Check database size
ls -lh ~/.local/share/octocode/
```

### Custom Monitoring

```bash
#!/bin/bash
# Performance monitoring script

echo "=== Octocode Performance Report ==="
echo "Date: $(date)"
echo

# Database size
echo "Database size:"
du -sh ~/.local/share/octocode/

# Index timing
echo "Indexing performance:"
time (octocode clear && octocode index)

# Search timing
echo "Search performance:"
time octocode search "authentication" > /dev/null

# Memory usage
echo "Memory usage during search:"
/usr/bin/time -v octocode search "database" > /dev/null 2>&1 | grep "Maximum resident"
```

## Troubleshooting Performance Issues

### Slow Indexing

**Symptoms**: Indexing takes much longer than expected

**Solutions**:
1. **Reduce chunk size**: `chunk_size = 1000`
2. **Use faster embedding model**: Switch to FastEmbed
3. **Disable GraphRAG**: Set `graphrag_enabled = false`
4. **Check disk space**: Ensure sufficient free space
5. **Monitor CPU usage**: Ensure no other heavy processes

```bash
# Quick fix for slow indexing
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"
octocode config --graphrag-enabled false
octocode clear && octocode index
```

### Slow Search

**Symptoms**: Search queries take several seconds

**Solutions**:
1. **Increase similarity threshold**: `similarity_threshold = 0.3`
2. **Reduce max results**: `max_results = 20`
3. **Check database corruption**: `octocode clear && octocode index`
4. **Optimize query**: Use more specific search terms

```bash
# Quick fix for slow search
octocode config --max-results 20
octocode config --similarity-threshold 0.3
```

### High Memory Usage

**Symptoms**: Octocode uses excessive RAM

**Solutions**:
1. **Reduce max memories**: `max_memories = 5000`
2. **Clear old data**: `octocode clear`
3. **Use smaller embedding models**: Switch to 384-dim models
4. **Limit search results**: `max_results = 20`

```bash
# Quick fix for memory issues
octocode config --max-results 20
octocode clear
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"
```

### API Rate Limiting

**Symptoms**: Errors from cloud embedding providers

**Solutions**:
1. **Reduce batch size**: `embeddings_batch_size = 8`
2. **Add delays**: Use local models for development
3. **Switch providers**: Try different cloud providers
4. **Use local models**: Switch to FastEmbed/SentenceTransformer

```bash
# Quick fix for rate limiting
octocode config --code-embedding-model "fastembed:all-MiniLM-L6-v2"
octocode config --text-embedding-model "fastembed:multilingual-e5-small"
```

## Benchmarking

### Standard Benchmark

```bash
#!/bin/bash
# Octocode benchmark script

PROJECT_PATH="/path/to/test/project"
cd "$PROJECT_PATH"

echo "=== Octocode Benchmark ==="
echo "Project: $PROJECT_PATH"
echo "Files: $(find . -type f -name "*.rs" -o -name "*.py" -o -name "*.js" | wc -l)"
echo

# Clear previous data
octocode clear

# Benchmark indexing
echo "Indexing benchmark:"
time octocode index

# Benchmark search
echo "Search benchmark (10 queries):"
queries=("authentication" "database" "API" "error handling" "configuration" "testing" "middleware" "validation" "logging" "security")

for query in "${queries[@]}"; do
    echo -n "Query '$query': "
    time octocode search "$query" > /dev/null
done

# Database size
echo "Final database size:"
du -sh ~/.local/share/octocode/
```

### Performance Comparison

| Configuration | Indexing (1000 files) | Search Latency | Memory Usage | Quality Score |
|---------------|----------------------|----------------|--------------|---------------|
| **FastEmbed** | 30s | 50ms | 200MB | 7/10 |
| **SentenceTransformer** | 60s | 80ms | 400MB | 8/10 |
| **Cloud (Jina)** | 120s | 100ms | 300MB | 9/10 |
| **Cloud (Voyage)** | 150s | 120ms | 350MB | 9.5/10 |

## Best Practices

### Development Workflow

1. **Use local models** during development
2. **Enable cloud models** for production/final indexing
3. **Regular cleanup**: `octocode clear` periodically
4. **Monitor performance**: Track indexing and search times

### Production Deployment

1. **Optimize for your use case**: Speed vs quality tradeoff
2. **Monitor resource usage**: CPU, memory, storage
3. **Plan for scaling**: Consider hardware requirements
4. **Backup strategy**: Regular database backups

### Configuration Templates

#### Development (Speed Focus)
```toml
[embedding]
code_model = "fastembed:all-MiniLM-L6-v2"
text_model = "fastembed:multilingual-e5-small"

[index]
chunk_size = 1000
graphrag_enabled = false

[search]
max_results = 20
similarity_threshold = 0.3
```

#### Production (Quality Focus)
```toml
[embedding]
code_model = "huggingface:microsoft/codebert-base"
text_model = "huggingface:sentence-transformers/all-mpnet-base-v2"

[index]
chunk_size = 2000
graphrag_enabled = true

[search]
max_results = 50
similarity_threshold = 0.1
```

#### Large Scale (Balanced)
```toml
[embedding]
code_model = "fastembed:BAAI/bge-small-en-v1.5"
text_model = "fastembed:multilingual-e5-small"

[index]
chunk_size = 1500
graphrag_enabled = true

[search]
max_results = 30
similarity_threshold = 0.2

[memory]
max_memories = 50000
```



================================================
FILE: doc/RELEASE_MANAGEMENT.md
================================================
# Release Management Guide

Complete guide to Octocode's AI-powered release management system with automatic version calculation and changelog generation.

## Overview

Octocode provides intelligent release automation that analyzes your commit history using conventional commits to determine appropriate semantic version bumps and generates structured changelogs. It supports multiple project types and integrates seamlessly with your git workflow.

## Key Features

- **AI Version Calculation**: Analyzes commit history to determine semantic version bumps
- **Automatic Changelog**: Generates structured changelogs from commit messages
- **Multi-Project Support**: Works with Rust, Node.js, PHP, and Go projects
- **Git Integration**: Creates release commits and annotated tags automatically
- **Dry Run Mode**: Preview changes before execution
- **Conventional Commits**: Supports conventional commit format for precise version calculation

## Quick Start

### Basic Release Workflow

```bash
# 1. Preview what would be done (recommended first step)
octocode release --dry-run

# 2. Create the release
octocode release

# 3. Push to remote
git push origin main --tags
```

### With Confirmation

```bash
# Skip confirmation prompt for automation
octocode release --yes

# Force a specific version (bypasses AI calculation)
octocode release --force-version "2.0.0"

# Use custom changelog file
octocode release --changelog "HISTORY.md"
```

## Supported Project Types

### Rust Projects (Cargo.toml)

```toml
[package]
name = "my-project"
version = "0.1.0"  # Updated automatically
```

**Files updated:**
- `Cargo.toml` - Package version
- `CHANGELOG.md` - Release notes

### Node.js Projects (package.json)

```json
{
  "name": "my-project",
  "version": "0.1.0"
}
```

**Files updated:**
- `package.json` - Package version
- `CHANGELOG.md` - Release notes

### PHP Projects (composer.json)

```json
{
  "name": "vendor/my-project",
  "version": "0.1.0"
}
```

**Files updated:**
- `composer.json` - Package version
- `CHANGELOG.md` - Release notes

### Go Projects (go.mod)

```go
module github.com/user/my-project

go 1.21
```

**Files updated:**
- `VERSION` file - Version string
- `CHANGELOG.md` - Release notes

**Note**: Go projects use a `VERSION` file since `go.mod` doesn't contain version information.

## How It Works

### 1. Project Detection

Octocode automatically detects your project type by scanning for:
- `Cargo.toml` → Rust project
- `package.json` → Node.js project
- `composer.json` → PHP project
- `go.mod` → Go project

### 2. Version Analysis

Extracts current version from:
- Project files (Cargo.toml, package.json, etc.)
- Git tags (if no version in project files)
- Defaults to 0.1.0 for new projects

### 3. Commit Analysis

Analyzes commits since the last release using:
- Git log between current HEAD and last version tag
- Conventional commit format parsing
- Commit message categorization

### 4. AI Version Calculation

Uses LLM to determine appropriate version bump based on:
- Conventional commit types
- Breaking change indicators
- Commit message content
- Project context

### 5. Changelog Generation

Creates structured changelog with:
- Categorized changes (Features, Bug Fixes, etc.)
- Commit references
- Breaking change highlights
- Release date

### 6. File Updates and Git Operations

- Updates project version files
- Adds changelog entry
- Creates release commit
- Creates annotated git tag

## Conventional Commits Support

### Commit Types and Version Bumps

| Commit Type | Version Bump | Example |
|-------------|--------------|---------|
| `feat:` | Minor (0.1.0 → 0.2.0) | `feat: add user authentication` |
| `fix:` | Patch (0.1.0 → 0.1.1) | `fix: resolve login timeout issue` |
| `BREAKING CHANGE` | Major (0.1.0 → 1.0.0) | `feat!: redesign API endpoints` |
| `chore:` | Patch | `chore: update dependencies` |
| `docs:` | Patch | `docs: update API documentation` |
| `style:` | Patch | `style: fix code formatting` |
| `refactor:` | Patch | `refactor: simplify auth logic` |
| `test:` | Patch | `test: add integration tests` |
| `perf:` | Patch | `perf: optimize database queries` |

### Breaking Changes

Breaking changes trigger major version bumps:

```bash
# Using exclamation mark
feat!: redesign user API endpoints

# Using BREAKING CHANGE footer
feat: add new authentication system

BREAKING CHANGE: The old auth endpoints have been removed
```

### Scoped Commits

Scopes help organize changelog entries:

```bash
feat(auth): add OAuth2 support
fix(database): resolve connection pool issues
docs(api): update endpoint documentation
```

## Command Options

### Basic Commands

```bash
# Preview release (no changes made)
octocode release --dry-run

# Create release with prompts
octocode release

# Create release without prompts
octocode release --yes
```

### Version Control

```bash
# Force specific version (bypasses AI calculation)
octocode release --force-version "2.0.0"
octocode release --force-version "1.5.0-beta.1"

# Specify version type manually
octocode release --version-type major
octocode release --version-type minor
octocode release --version-type patch
```

### Changelog Options

```bash
# Use custom changelog file
octocode release --changelog "HISTORY.md"
octocode release --changelog "docs/RELEASES.md"

# Skip changelog generation
octocode release --no-changelog
```

### Git Options

```bash
# Custom commit message
octocode release --commit-message "Release v{version}"

# Custom tag format
octocode release --tag-format "v{version}"
octocode release --tag-format "release-{version}"

# Skip git tag creation
octocode release --no-tag
```

## Example Workflows

### Standard Development Workflow

```bash
# 1. Development with conventional commits
git add .
octocode commit  # AI generates conventional commit

# 2. More development...
git add .
octocode commit

# 3. Ready for release
octocode release --dry-run  # Preview
octocode release            # Create release

# 4. Deploy
git push origin main --tags
```

### Pre-release Workflow

```bash
# Create beta release
octocode release --force-version "1.0.0-beta.1"

# Test the beta...

# Create release candidate
octocode release --force-version "1.0.0-rc.1"

# Final release
octocode release --force-version "1.0.0"
```

### Hotfix Workflow

```bash
# On main branch, fix critical bug
git add .
octocode commit  # AI generates "fix: critical security vulnerability"

# Create patch release
octocode release  # AI determines patch bump (e.g., 1.0.0 → 1.0.1)
```

### Feature Release Workflow

```bash
# Develop features with conventional commits
git add .
octocode commit  # "feat: add user dashboard"

git add .
octocode commit  # "feat: add data export functionality"

git add .
octocode commit  # "fix: resolve dashboard loading issue"

# Create minor release
octocode release  # AI determines minor bump (e.g., 1.0.0 → 1.1.0)
```

## Changelog Format

### Generated Changelog Structure

```markdown
# Changelog

## [1.2.0] - 2025-01-27

### ✨ Features

- Add user dashboard with analytics
- Implement data export functionality
- Add OAuth2 authentication support

### 🐛 Bug Fixes

- Fix dashboard loading timeout issue
- Resolve authentication token refresh bug
- Fix data export CSV formatting

### 📚 Documentation

- Update API documentation
- Add user guide for new dashboard

### 🔧 Maintenance

- Update dependencies to latest versions
- Improve test coverage
- Refactor authentication module

### ⚠️ Breaking Changes

- Removed deprecated `/api/v1/auth` endpoint
- Changed user profile data structure

## [1.1.0] - 2025-01-15

...
```

### Customizing Changelog Format

```bash
# Use custom changelog template
octocode release --changelog-template "templates/release.md"

# Custom section headers
octocode config --changelog-sections "Features,Fixes,Changes"
```

## Integration with CI/CD

### GitHub Actions

```yaml
name: Release
on:
  push:
    branches: [main]
    paths-ignore: ['CHANGELOG.md']

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Need full history for release analysis

      - name: Install Octocode
        run: curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh

      - name: Create Release
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          octocode release --yes
          git push origin main --tags
```

### GitLab CI

```yaml
release:
  stage: release
  script:
    - curl -fsSL https://raw.githubusercontent.com/muvon/octocode/master/install.sh | sh
    - octocode release --yes
    - git push origin main --tags
  only:
    - main
  variables:
    OPENROUTER_API_KEY: $OPENROUTER_API_KEY
```

### Manual Automation

```bash
#!/bin/bash
# release.sh - Automated release script

set -e

echo "🚀 Starting release process..."

# Ensure we're on main branch
git checkout main
git pull origin main

# Run tests
echo "🧪 Running tests..."
cargo test  # or npm test, composer test, go test

# Create release
echo "📦 Creating release..."
octocode release --dry-run
read -p "Continue with release? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    octocode release --yes
    git push origin main --tags
    echo "✅ Release complete!"
else
    echo "❌ Release cancelled"
    exit 1
fi
```

## Configuration

### Release Configuration

```bash
# Set default changelog file
octocode config --changelog-file "RELEASES.md"

# Set default commit message format
octocode config --release-commit-format "chore: release v{version}"

# Set default tag format
octocode config --release-tag-format "v{version}"
```

### Configuration File

```toml
[release]
changelog_file = "CHANGELOG.md"
commit_message = "chore: release v{version}"
tag_format = "v{version}"
auto_push = false
require_conventional_commits = true

[openrouter]
model = "openai/gpt-4o-mini"  # Model for version calculation
```

## Advanced Features

### Pre-release Hooks

```bash
# Run tests before release
octocode release --pre-hook "cargo test"
octocode release --pre-hook "npm run test"

# Multiple hooks
octocode release --pre-hook "cargo test" --pre-hook "cargo clippy"
```

### Post-release Hooks

```bash
# Deploy after release
octocode release --post-hook "deploy.sh"

# Notify team
octocode release --post-hook "notify-team.sh"
```

### Custom Version Calculation

```bash
# Use different model for version calculation
octocode config --release-model "anthropic/claude-3.5-sonnet"

# Custom version calculation prompt
octocode release --version-prompt "Calculate version based on semantic changes"
```

## Troubleshooting

### Version Calculation Issues

**Problem**: AI suggests wrong version bump

**Solutions:**
1. Use `--force-version` to override
2. Improve commit message quality
3. Use conventional commit format consistently
4. Try different LLM model

### Git Integration Issues

**Problem**: Git operations fail

**Solutions:**
1. Ensure git repository is clean
2. Check git credentials and permissions
3. Verify branch permissions
4. Check for conflicting tags

### Project Detection Issues

**Problem**: Project type not detected

**Solutions:**
1. Ensure project files exist (Cargo.toml, package.json, etc.)
2. Run from project root directory
3. Check file permissions
4. Manually specify project type

### Changelog Generation Issues

**Problem**: Changelog format is incorrect

**Solutions:**
1. Check conventional commit format
2. Verify commit message quality
3. Use custom changelog template
4. Check LLM model configuration

## Best Practices

### Commit Message Quality

```bash
# Good conventional commits
feat(auth): add OAuth2 authentication support
fix(database): resolve connection pool timeout issue
docs(api): update authentication endpoint documentation

# Poor commits (avoid)
fix stuff
update code
changes
```

### Release Timing

1. **Regular Releases**: Schedule regular releases (weekly/monthly)
2. **Feature Releases**: Release when significant features are complete
3. **Hotfix Releases**: Release immediately for critical bugs
4. **Pre-releases**: Use for testing major changes

### Version Strategy

1. **Semantic Versioning**: Follow semver strictly
2. **Breaking Changes**: Clearly mark breaking changes
3. **Pre-releases**: Use for beta testing
4. **LTS Versions**: Consider long-term support versions

### Changelog Maintenance

1. **Clear Descriptions**: Write clear, user-focused descriptions
2. **Breaking Changes**: Highlight breaking changes prominently
3. **Migration Guides**: Include migration instructions for major versions
4. **Regular Updates**: Keep changelog up to date with each release

For more information, see:
- [Commands Reference](COMMANDS.md) - Complete command documentation
- [Advanced Usage](ADVANCED_USAGE.md) - Advanced workflows
- [Configuration](CONFIGURATION.md) - Configuration options



================================================
FILE: scripts/format.sh
================================================
#!/bin/bash
# Script to format all Rust code in the project

echo "Running cargo fmt on all Rust files..."
cargo fmt --all

echo "Checking formatting with cargo fmt --check..."
if cargo fmt --all -- --check; then
    echo "✅ All files are properly formatted"
else
    echo "❌ Some files need formatting. Run 'cargo fmt --all' to fix."
    exit 1
fi



================================================
FILE: scripts/install-completions.sh
================================================
#!/bin/bash
# Installation script for octocode shell completions

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Try release binary first, then debug binary
if [[ -f "${SCRIPT_DIR}/../target/release/octocode" ]]; then
	OCTOCODE_BIN="${SCRIPT_DIR}/../target/release/octocode"
elif [[ -f "${SCRIPT_DIR}/../target/debug/octocode" ]]; then
	OCTOCODE_BIN="${SCRIPT_DIR}/../target/debug/octocode"
else
	echo "Error: octocode binary not found"
	echo "Please run 'cargo build --release' or 'cargo build' first"
	exit 1
fi

echo "Installing shell completions for octocode..."

# Detect the shell and install accordingly
detect_shell() {
	if [[ -n "$ZSH_VERSION" ]]; then
		echo "zsh"
	elif [[ -n "$BASH_VERSION" ]]; then
		echo "bash"
	else
		# Try to detect from SHELL environment variable
		case "$SHELL" in
			*/zsh) echo "zsh" ;;
			*/bash) echo "bash" ;;
			*) echo "unknown" ;;
		esac
	fi
}

install_bash_completion() {
	echo "Installing bash completion..."

	# Standard bash completion directories (in order of preference)
	BASH_COMPLETION_DIRS=(
		"$HOME/.local/share/bash-completion/completions"
		"$HOME/.bash_completion.d"
		"/usr/local/etc/bash_completion.d"
		"/etc/bash_completion.d"
	)

	# Find the first writable directory
	BASH_DIR=""
	for dir in "${BASH_COMPLETION_DIRS[@]}"; do
		if [[ -d "$(dirname "$dir")" ]] && [[ -w "$(dirname "$dir")" ]]; then
			BASH_DIR="$dir"
			break
		fi
	done

	if [[ -z "$BASH_DIR" ]]; then
		# Create user directory as fallback
		BASH_DIR="$HOME/.local/share/bash-completion/completions"
		mkdir -p "$BASH_DIR"
	else
		mkdir -p "$BASH_DIR"
	fi

	"$OCTOCODE_BIN" completion bash > "$BASH_DIR/octocode"
	echo "✓ Bash completion installed to: $BASH_DIR/octocode"

	# Check if bash-completion is properly configured
	if ! grep -q "bash-completion" "$HOME/.bashrc" 2>/dev/null &&
							! grep -q "bash_completion" "$HOME/.bash_profile" 2>/dev/null; then
		echo ""
		echo "📝 To enable bash completion, add this to your ~/.bashrc:"
		echo "   # Enable bash completion"
		echo "   if [[ -f /usr/share/bash-completion/bash_completion ]]; then"
		echo "       source /usr/share/bash-completion/bash_completion"
		echo "   elif [[ -f /usr/local/etc/bash_completion ]]; then"
		echo "       source /usr/local/etc/bash_completion"
		echo "   fi"
		echo ""
		echo "   # Load user completions"
		echo "   if [[ -d ~/.local/share/bash-completion/completions ]]; then"
		echo "       for completion in ~/.local/share/bash-completion/completions/*; do"
		echo "           [[ -r \$completion ]] && source \$completion"
		echo "       done"
		echo "   fi"
	fi
}

install_zsh_completion() {
	echo "Installing zsh completion..."

	# Standard zsh completion directories (in order of preference)
	ZSH_COMPLETION_DIRS=(
		"$HOME/.local/share/zsh/site-functions"
		"$HOME/.zsh/completions"
		"$HOME/.config/zsh/completions"
		"/usr/local/share/zsh/site-functions"
		"/usr/share/zsh/site-functions"
	)

	# Find the first writable directory
	ZSH_DIR=""
	for dir in "${ZSH_COMPLETION_DIRS[@]}"; do
		if [[ -d "$(dirname "$dir")" ]] && [[ -w "$(dirname "$dir")" ]]; then
			ZSH_DIR="$dir"
			break
		fi
	done

	if [[ -z "$ZSH_DIR" ]]; then
		# Create user directory as fallback
		ZSH_DIR="$HOME/.local/share/zsh/site-functions"
		mkdir -p "$ZSH_DIR"
	else
		mkdir -p "$ZSH_DIR"
	fi

	"$OCTOCODE_BIN" completion zsh > "$ZSH_DIR/_octocode"
	echo "✓ Zsh completion installed to: $ZSH_DIR/_octocode"

	# Check if the directory is in fpath
	echo ""
	echo "📝 To enable zsh completion, ensure your ~/.zshrc contains:"
	echo "   # Add completion directory to fpath"
	echo "   fpath=($ZSH_DIR \$fpath)"
	echo "   autoload -U compinit && compinit"
	echo ""
	echo "   Alternatively, add this line to regenerate completions:"
	echo "   autoload -U compinit && compinit -d ~/.zcompdump"
	echo ""

	# Offer to fix common zsh completion issues
	if [[ -n "$ZSH_VERSION" ]]; then
		echo "🔧 Current session setup:"
		echo "   Run: autoload -U compinit && compinit -d ~/.zcompdump"
		echo "   Then: exec zsh  # to restart your shell"
	fi
}

# Main installation logic
SHELL_TYPE=$(detect_shell)

case "$1" in
	bash)
		install_bash_completion
		;;
	zsh)
		install_zsh_completion
		;;
	both|"")
		install_bash_completion
		install_zsh_completion
		;;
	*)
		echo "Usage: $0 [bash|zsh|both]"
		echo "  bash - Install bash completion only"
		echo "  zsh  - Install zsh completion only"
		echo "  both - Install both completions (default)"
		echo ""
		echo "Auto-detected shell: $SHELL_TYPE"
		exit 1
		;;
esac

echo ""
echo "✅ Shell completion installation complete!"
echo ""
echo "💡 Quick test:"
echo "   octocode <TAB>        # Should show available commands"
echo "   octocode session <TAB> # Should show session options"
echo ""
echo "🔄 If completions don't work immediately:"
echo "   - Restart your shell: exec \$SHELL"
echo "   - Or source your config: source ~/.bashrc (bash) or source ~/.zshrc (zsh)"



================================================
FILE: scripts/test-completions.sh
================================================
#!/bin/bash
# Test script for shell completions

set -e

echo "Testing octocode shell completions..."
echo ""

# Try release binary first, then debug binary
if [[ -f "./target/release/octocode" ]]; then
	OCTOCODE_BIN="./target/release/octocode"
elif [[ -f "./target/debug/octocode" ]]; then
	OCTOCODE_BIN="./target/debug/octocode"
else
	echo "Error: octocode binary not found. Run 'cargo build' or 'cargo build --release' first."
	exit 1
fi

echo "✓ Binary found at $OCTOCODE_BIN"

# Test completion generation
echo "Testing completion generation..."

echo "- Testing bash completion generation..."
if "$OCTOCODE_BIN" completion bash > /tmp/test_bash_completion; then
	echo "✓ Bash completion generated successfully"
	echo "  Generated $(wc -l < /tmp/test_bash_completion) lines"
else
	echo "✗ Failed to generate bash completion"
	exit 1
fi

echo "- Testing zsh completion generation..."
if "$OCTOCODE_BIN" completion zsh > /tmp/test_zsh_completion; then
	echo "✓ Zsh completion generated successfully"
	echo "  Generated $(wc -l < /tmp/test_zsh_completion) lines"
else
	echo "✗ Failed to generate zsh completion"
	exit 1
fi

echo "- Testing all available shells..."
for shell in bash elvish fish powershell zsh; do
	if "$OCTOCODE_BIN" completion "$shell" > "/tmp/test_${shell}_completion"; then
		echo "✓ $shell completion: $(wc -l < "/tmp/test_${shell}_completion") lines"
	else
		echo "✗ Failed to generate $shell completion"
	fi
done

echo ""
echo "Testing completion content..."

# Check if bash completion contains expected patterns
if grep -q "_octocode()" /tmp/test_bash_completion; then
	echo "✓ Bash completion contains function definition"
else
	echo "✗ Bash completion missing function definition"
fi

if grep -q "octocode__session" /tmp/test_bash_completion; then
	echo "✓ Bash completion contains subcommand definitions"
else
	echo "✗ Bash completion missing subcommand definitions"
fi

# Check if zsh completion contains expected patterns
if grep -q "#compdef octocode" /tmp/test_zsh_completion; then
	echo "✓ Zsh completion contains compdef directive"
else
	echo "✗ Zsh completion missing compdef directive"
fi

echo ""
echo "✓ All completion tests passed!"
echo ""
echo "To install completions, run:"
echo "  ./scripts/install-completions.sh"
echo ""
echo "Or manually:"
echo "  # Bash"
echo "  $OCTOCODE_BIN completion bash > ~/.local/share/bash-completion/completions/octocode"
echo ""
echo "  # Zsh"
echo "  mkdir -p ~/.config/zsh/completions"
echo "  $OCTOCODE_BIN completion zsh > ~/.config/zsh/completions/_octocode"

# Cleanup
rm -f /tmp/test_*_completion



================================================
FILE: src/config.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;

use crate::embedding::types::EmbeddingConfig;
use crate::storage;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
	pub description_model: String,
	pub relationship_model: String,
	pub ai_batch_size: usize,
	pub max_batch_tokens: usize,
	pub batch_timeout_seconds: u64,
	pub fallback_to_individual: bool,
	pub max_sample_tokens: usize,
	pub confidence_threshold: f32,
	pub architectural_weight: f32,
	pub relationship_system_prompt: String,
	pub description_system_prompt: String,
}

// NOTE: This Default implementation should NEVER be used in practice
// All LLM values must come from the config template file
// This exists only to satisfy serde's requirements for deserialization
impl Default for LLMConfig {
	fn default() -> Self {
		panic!("LLM config must be loaded from template file - defaults not allowed")
	}
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphRAGConfig {
	pub enabled: bool,
	pub use_llm: bool,
	pub llm: LLMConfig,
}

// NOTE: This Default implementation should NEVER be used in practice
// All GraphRAG values must come from the config template file
// This exists only to satisfy serde's requirements for deserialization
impl Default for GraphRAGConfig {
	fn default() -> Self {
		panic!("GraphRAG config must be loaded from template file - defaults not allowed")
	}
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenRouterConfig {
	pub model: String,
	pub base_url: String,
	pub timeout: u64,
	pub api_key: Option<String>,
}

impl Default for OpenRouterConfig {
	fn default() -> Self {
		Self {
			model: "openai/gpt-4.1-mini".to_string(),
			base_url: "https://openrouter.ai/api/v1".to_string(),
			timeout: 120,
			api_key: None,
		}
	}
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexConfig {
	pub chunk_size: usize,
	pub chunk_overlap: usize,
	pub embeddings_batch_size: usize,

	/// Maximum tokens per batch for embeddings generation (global limit).
	/// This prevents API errors like "max allowed tokens per submitted batch is 120000".
	/// Uses tiktoken cl100k_base tokenizer for counting. Default: 100000
	pub embeddings_max_tokens_per_batch: usize,

	/// How often to flush data to storage during indexing (in batches).
	/// 1 = flush after every batch (safest, slower)
	/// 5 = flush every 5 batches (faster, less safe)
	/// Default: 1 for maximum data safety
	pub flush_frequency: usize,

	/// Require git repository for indexing (default: true)
	pub require_git: bool,
}

impl Default for IndexConfig {
	fn default() -> Self {
		Self {
			chunk_size: 2000,
			chunk_overlap: 100,
			embeddings_batch_size: 16,
			embeddings_max_tokens_per_batch: 100000,
			flush_frequency: 2,
			require_git: true,
		}
	}
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchConfig {
	pub max_results: usize,
	pub similarity_threshold: f32,
	pub output_format: String,
	pub max_files: usize,
	pub context_lines: usize,

	/// Maximum characters to display per code/text/doc block in search results.
	/// If 0, displays full content. Default: 1000
	pub search_block_max_characters: usize,
}

impl Default for SearchConfig {
	fn default() -> Self {
		Self {
			max_results: 50,
			similarity_threshold: 0.6,
			output_format: "markdown".to_string(),
			max_files: 20,
			context_lines: 3,
			search_block_max_characters: 1000,
		}
	}
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
	/// Configuration version for future migrations
	#[serde(default = "default_version")]
	pub version: u32,

	#[serde(default)]
	pub openrouter: OpenRouterConfig,

	#[serde(default)]
	pub index: IndexConfig,

	#[serde(default)]
	pub search: SearchConfig,

	#[serde(default)]
	pub embedding: EmbeddingConfig,

	#[serde(default)]
	pub graphrag: GraphRAGConfig,
}

fn default_version() -> u32 {
	1
}

impl Default for Config {
	fn default() -> Self {
		Self {
			version: default_version(),
			openrouter: OpenRouterConfig::default(),
			index: IndexConfig::default(),
			search: SearchConfig::default(),
			embedding: EmbeddingConfig::default(),
			// This should never be reached - template loading should provide GraphRAG config
			graphrag: GraphRAGConfig::default(),
		}
	}
}

impl Config {
	pub fn load() -> Result<Self> {
		let config_path = Self::get_system_config_path()?;

		let mut config = if config_path.exists() {
			let content = fs::read_to_string(&config_path)?;
			toml::from_str(&content)?
		} else {
			// Load from template first, then save to system config
			let template_config = Self::load_from_template()?;

			// Ensure the parent directory exists
			if let Some(parent) = config_path.parent() {
				if !parent.exists() {
					fs::create_dir_all(parent)?;
				}
			}

			// Save template as the new config
			let toml_content = toml::to_string_pretty(&template_config)?;
			fs::write(&config_path, toml_content)?;
			template_config
		};

		// Environment variables take precedence over config file values
		if let Ok(api_key) = std::env::var("OPENROUTER_API_KEY") {
			config.openrouter.api_key = Some(api_key);
		}

		Ok(config)
	}

	/// Load configuration from the default template
	pub fn load_from_template() -> Result<Self> {
		// Try to load from embedded template first
		let template_content = Self::get_default_template_content()?;
		let config: Config = toml::from_str(&template_content)?;
		Ok(config)
	}

	/// Get the default template content
	fn get_default_template_content() -> Result<String> {
		// First try to read from config-templates/default.toml in the current directory
		let template_path = std::path::Path::new("config-templates/default.toml");
		if template_path.exists() {
			return Ok(fs::read_to_string(template_path)?);
		}

		// If not found, use embedded template
		Ok(include_str!("../config-templates/default.toml").to_string())
	}

	pub fn save(&self) -> Result<()> {
		let config_path = Self::get_system_config_path()?;

		// Ensure the parent directory exists
		if let Some(parent) = config_path.parent() {
			if !parent.exists() {
				fs::create_dir_all(parent)?;
			}
		}

		let toml_content = toml::to_string_pretty(self)?;
		fs::write(config_path, toml_content)?;
		Ok(())
	}

	/// Get the system-wide config file path
	/// Stored at ~/.local/share/octocode/config.toml (same level as fastembed cache)
	pub fn get_system_config_path() -> Result<PathBuf> {
		let system_storage = storage::get_system_storage_dir()?;
		Ok(system_storage.join("config.toml"))
	}

	pub fn get_model(&self) -> &str {
		&self.openrouter.model
	}

	pub fn get_base_url(&self) -> &str {
		&self.openrouter.base_url
	}

	pub fn get_timeout(&self) -> u64 {
		self.openrouter.timeout
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn test_default_config() {
		// Use template loading instead of Config::default() to avoid GraphRAG panic
		let config = Config::load_from_template().expect("Failed to load template config");
		assert_eq!(config.version, 1);
		assert_eq!(config.openrouter.model, "openai/gpt-4.1-mini");
		assert_eq!(config.index.chunk_size, 2000);
		assert_eq!(config.search.max_results, 20);

		assert_eq!(
			config.embedding.get_active_provider(),
			crate::embedding::types::EmbeddingProviderType::Voyage
		);
		// Test new GraphRAG configuration structure
		assert!(!config.graphrag.enabled);
		assert!(!config.graphrag.use_llm);
		assert_eq!(config.graphrag.llm.description_model, "openai/gpt-4.1-mini");
		assert_eq!(
			config.graphrag.llm.relationship_model,
			"openai/gpt-4.1-mini"
		);
		assert_eq!(config.graphrag.llm.ai_batch_size, 8);
		assert_eq!(config.graphrag.llm.max_batch_tokens, 16384);
		assert_eq!(config.graphrag.llm.batch_timeout_seconds, 60);
		assert!(config.graphrag.llm.fallback_to_individual);
		assert_eq!(config.graphrag.llm.max_sample_tokens, 1500);
		assert_eq!(config.graphrag.llm.confidence_threshold, 0.6);
		assert_eq!(config.graphrag.llm.architectural_weight, 0.9);
		assert!(config
			.graphrag
			.llm
			.relationship_system_prompt
			.contains("expert software architect"));
		assert!(config
			.graphrag
			.llm
			.description_system_prompt
			.contains("ROLE and PURPOSE"));
	}

	#[test]
	fn test_template_loading() {
		let result = Config::load_from_template();
		assert!(result.is_ok(), "Should be able to load from template");

		let config = result.unwrap();
		assert_eq!(config.version, 1);
		assert_eq!(config.openrouter.model, "openai/gpt-4.1-mini");
		assert_eq!(config.index.chunk_size, 2000);
		assert_eq!(config.search.max_results, 20);
		assert_eq!(config.embedding.code_model, "voyage:voyage-code-3");
		assert_eq!(config.embedding.text_model, "voyage:voyage-3.5-lite");
		// Test new GraphRAG configuration structure from template
		assert!(!config.graphrag.enabled);
		assert!(!config.graphrag.use_llm);
		assert_eq!(config.graphrag.llm.description_model, "openai/gpt-4.1-mini");
		assert_eq!(
			config.graphrag.llm.relationship_model,
			"openai/gpt-4.1-mini"
		);
		assert_eq!(config.graphrag.llm.ai_batch_size, 8);
		assert_eq!(config.graphrag.llm.max_batch_tokens, 16384);
		assert_eq!(config.graphrag.llm.batch_timeout_seconds, 60);
		assert!(config.graphrag.llm.fallback_to_individual);
		assert_eq!(config.graphrag.llm.max_sample_tokens, 1500);
		assert_eq!(config.graphrag.llm.confidence_threshold, 0.6);
		assert_eq!(config.graphrag.llm.architectural_weight, 0.9);
		assert!(config
			.graphrag
			.llm
			.relationship_system_prompt
			.contains("expert software architect"));
		assert!(config
			.graphrag
			.llm
			.description_system_prompt
			.contains("ROLE and PURPOSE"));
	}

	#[test]
	#[should_panic(expected = "GraphRAG config must be loaded from template file")]
	fn test_graphrag_default_panics() {
		// Verify that GraphRAGConfig::default() panics to enforce strict config loading
		let _ = GraphRAGConfig::default();
	}
}



================================================
FILE: src/constants.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! Application-wide constants

/// Maximum number of queries allowed in multi-query operations
pub const MAX_QUERIES: usize = 5;

/// Embedding input type prefixes for manual injection (non-API providers)
pub const QUERY_PREFIX: &str = "Represent the query for retrieving supporting documents: ";
pub const DOCUMENT_PREFIX: &str = "Represent the document for retrieval: ";



================================================
FILE: src/lib.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Main lib.rs file that exports our modules
pub mod config;
pub mod constants;
pub mod embedding;
pub mod indexer;
pub mod mcp;
pub mod memory;
pub mod reranker;
pub mod state;
pub mod storage;
pub mod store;
pub mod utils;
pub mod watcher_config;

// Re-export commonly used items for convenience
pub use config::Config;
pub use store::Store;



================================================
FILE: src/main.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Octocode - Intelligent Code Indexer and Graph Builder
// Copyright (c) 2025 Muvon Un Limited

use clap::{CommandFactory, Parser, Subcommand};
use clap_complete::{generate, Shell};

use octocode::config::Config;
use octocode::store::Store;

mod commands;

#[derive(Parser)]
#[command(name = "octocode")]
#[command(version = env!("CARGO_PKG_VERSION"))]
#[command(about = "Octocode is a smart code indexer and search tool")]
struct OctocodeArgs {
	#[command(subcommand)]
	command: Commands,
}

#[derive(Subcommand)]
enum Commands {
	/// Index the current directory's codebase
	Index(commands::IndexArgs),

	/// Search the codebase with a natural language query
	Search(commands::SearchArgs),

	/// View file signatures (functions, methods, etc.)
	View(commands::ViewArgs),

	/// Watch for changes in the codebase and reindex automatically
	Watch(commands::WatchArgs),

	/// Generate a default configuration file
	Config(commands::ConfigArgs),

	/// Query and explore the code relationship graph (GraphRAG)
	#[command(name = "graphrag")]
	GraphRAG(commands::GraphRAGArgs),

	/// Start MCP (Model Context Protocol) server
	Mcp(commands::McpArgs),

	/// Start MCP proxy server for multiple repositories
	#[command(name = "mcp-proxy")]
	McpProxy(commands::McpProxyArgs),

	/// Memory management for storing and retrieving information
	Memory(commands::MemoryArgs),

	/// Clear database tables (useful for debugging)
	Clear(commands::ClearArgs),

	/// Generate and create git commit with AI assistance
	Commit(commands::CommitArgs),

	/// Review staged changes for best practices and potential issues
	Review(commands::ReviewArgs),

	/// Create a new release with AI-powered version calculation and changelog generation
	Release(commands::ReleaseArgs),

	/// Format code according to .editorconfig rules
	Format(commands::FormatArgs),

	/// View MCP server logs
	Logs(commands::LogsArgs),

	/// Model management and discovery commands
	Models {
		#[command(subcommand)]
		command: commands::ModelsCommand,
	},

	/// Generate shell completion scripts
	Completion {
		/// The shell to generate completion for
		#[arg(value_enum)]
		shell: Shell,
	},
}

#[tokio::main]
async fn main() -> Result<(), anyhow::Error> {
	dotenvy::dotenv().ok();
	let args = OctocodeArgs::parse();

	// Load configuration - ensure .octocode directory exists
	let config = Config::load()?;

	// Handle the config command separately
	if let Commands::Config(config_args) = &args.command {
		return commands::config::execute(config_args, config);
	}

	// Handle the MCP command separately (doesn't need store)
	if let Commands::Mcp(mcp_args) = &args.command {
		return commands::mcp::run(mcp_args.clone()).await;
	}

	// Handle the MCP Proxy command separately (doesn't need store)
	if let Commands::McpProxy(mcp_proxy_args) = &args.command {
		return commands::mcp_proxy::run(mcp_proxy_args.clone()).await;
	}

	// Handle the Commit command separately (doesn't need store)
	if let Commands::Commit(commit_args) = &args.command {
		return commands::commit::execute(&config, commit_args).await;
	}

	// Handle the Review command separately (doesn't need store)
	if let Commands::Review(review_args) = &args.command {
		return commands::review::execute(&config, review_args).await;
	}

	// Handle the Release command separately (doesn't need store)
	if let Commands::Release(release_args) = &args.command {
		return commands::release::execute(&config, release_args).await;
	}

	// Handle the Format command separately (doesn't need store)
	if let Commands::Format(format_args) = &args.command {
		return commands::format::execute(format_args).await;
	}

	// Handle the Memory command separately (doesn't need store)
	if let Commands::Memory(memory_args) = &args.command {
		return commands::memory::execute(&config, memory_args).await;
	}

	// Handle the Logs command separately (doesn't need store)
	if let Commands::Logs(logs_args) = &args.command {
		return commands::logs::execute(logs_args).await;
	}

	// Handle the Models command separately (doesn't need store)
	if let Commands::Models { command } = &args.command {
		return commands::models::execute_models_command(command.clone()).await;
	}

	// Handle the Completion command separately (doesn't need store)
	if let Commands::Completion { shell } = &args.command {
		let mut app = OctocodeArgs::command();
		let name = app.get_name().to_string();
		generate(*shell, &mut app, name, &mut std::io::stdout());
		return Ok(());
	}

	// Initialize the store
	let store = Store::new().await?;
	store.initialize_collections().await?;

	// Execute the appropriate command
	match &args.command {
		Commands::Index(index_args) => {
			commands::index::execute(&store, &config, index_args).await?
		}
		Commands::Search(search_args) => {
			commands::search::execute(&store, search_args, &config).await?
		}
		Commands::View(view_args) => commands::view::execute(view_args).await?,
		Commands::Watch(watch_args) => {
			commands::watch::execute(&store, &config, watch_args).await?
		}
		Commands::GraphRAG(graphrag_args) => {
			commands::graphrag::execute(&store, graphrag_args, &config).await?
		}
		Commands::Clear(clear_args) => commands::clear::execute(&store, clear_args).await?,
		Commands::Config(_) => unreachable!(), // Already handled above
		Commands::Mcp(_) => unreachable!(),    // Already handled above
		Commands::McpProxy(_) => unreachable!(), // Already handled above
		Commands::Commit(_) => unreachable!(), // Already handled above
		Commands::Review(_) => unreachable!(), // Already handled above
		Commands::Release(_) => unreachable!(), // Already handled above
		Commands::Format(_) => unreachable!(), // Already handled above
		Commands::Logs(_) => unreachable!(),   // Already handled above
		Commands::Models { .. } => unreachable!(), // Already handled above
		Commands::Memory(_) => unreachable!(), // Already handled above
		Commands::Completion { .. } => unreachable!(), // Already handled above
	}

	Ok(())
}



================================================
FILE: src/reranker.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use crate::store::{CodeBlock, DocumentBlock, TextBlock};
use std::collections::HashMap;

/// Reranking strategies for improving search results
pub struct Reranker;

impl Reranker {
	/// Combined reranking using multiple signals
	pub fn rerank_code_blocks(mut blocks: Vec<CodeBlock>, query: &str) -> Vec<CodeBlock> {
		if blocks.is_empty() {
			return blocks;
		}

		// Apply multiple reranking strategies
		let query_lower = query.to_lowercase();

		for block in &mut blocks {
			let mut score = block.distance.unwrap_or(1.0);

			// 1. Exact text matches (most important)
			score *= Self::text_match_factor(&block.content, &query_lower);

			// 2. Symbol matches
			score *= Self::symbol_match_factor(&block.symbols, &query_lower);

			// 3. Path relevance
			score *= Self::path_relevance_factor(&block.path, &query_lower);

			// 4. Content length factor (prefer reasonable sized blocks)
			score *= Self::content_length_factor(&block.content);

			// Update the distance with the reranked score
			block.distance = Some(score);
		}

		// Sort by the new reranked scores
		blocks.sort_by(|a, b| {
			let score_a = a.distance.unwrap_or(1.0);
			let score_b = b.distance.unwrap_or(1.0);
			score_a
				.partial_cmp(&score_b)
				.unwrap_or(std::cmp::Ordering::Equal)
		});

		blocks
	}

	/// Rerank document blocks
	pub fn rerank_document_blocks(
		mut blocks: Vec<DocumentBlock>,
		query: &str,
	) -> Vec<DocumentBlock> {
		if blocks.is_empty() {
			return blocks;
		}

		let query_lower = query.to_lowercase();

		for block in &mut blocks {
			let mut score = block.distance.unwrap_or(1.0);

			// 1. Title matches (very important for docs)
			score *= Self::title_match_factor(&block.title, &query_lower);

			// 2. Content matches
			score *= Self::text_match_factor(&block.content, &query_lower);

			// 3. Path relevance
			score *= Self::path_relevance_factor(&block.path, &query_lower);

			// 4. Header level factor (prefer higher level headers)
			score *= Self::header_level_factor(block.level);

			block.distance = Some(score);
		}

		blocks.sort_by(|a, b| {
			let score_a = a.distance.unwrap_or(1.0);
			let score_b = b.distance.unwrap_or(1.0);
			score_a
				.partial_cmp(&score_b)
				.unwrap_or(std::cmp::Ordering::Equal)
		});

		blocks
	}

	/// Rerank text blocks
	pub fn rerank_text_blocks(mut blocks: Vec<TextBlock>, query: &str) -> Vec<TextBlock> {
		if blocks.is_empty() {
			return blocks;
		}

		let query_lower = query.to_lowercase();

		for block in &mut blocks {
			let mut score = block.distance.unwrap_or(1.0);

			// 1. Text matches
			score *= Self::text_match_factor(&block.content, &query_lower);

			// 2. Path relevance
			score *= Self::path_relevance_factor(&block.path, &query_lower);

			// 3. Content length factor
			score *= Self::content_length_factor(&block.content);

			block.distance = Some(score);
		}

		blocks.sort_by(|a, b| {
			let score_a = a.distance.unwrap_or(1.0);
			let score_b = b.distance.unwrap_or(1.0);
			score_a
				.partial_cmp(&score_b)
				.unwrap_or(std::cmp::Ordering::Equal)
		});

		blocks
	}

	/// Calculate text match factor - exact matches get strong boost
	fn text_match_factor(content: &str, query: &str) -> f32 {
		let content_lower = content.to_lowercase();

		// Exact phrase match
		if content_lower.contains(query) {
			let word_count = query.split_whitespace().count();
			return match word_count {
				1 => 0.7,     // Single word exact match
				2..=3 => 0.5, // 2-3 word phrase match
				_ => 0.6,     // Longer phrase match
			};
		}

		// Individual word matches
		let query_words: Vec<&str> = query.split_whitespace().collect();
		let content_words: Vec<&str> = content_lower.split_whitespace().collect();

		let mut matches = 0;
		for query_word in &query_words {
			if content_words
				.iter()
				.any(|&word| word.contains(query_word) || query_word.contains(word))
			{
				matches += 1;
			}
		}

		if matches > 0 {
			let match_ratio = matches as f32 / query_words.len() as f32;
			return 0.8 + (match_ratio * 0.15); // 0.8 to 0.95 range
		}

		1.0 // No change if no matches
	}

	/// Calculate title match factor - titles are very important for relevance
	fn title_match_factor(title: &str, query: &str) -> f32 {
		let title_lower = title.to_lowercase();

		// Exact title match
		if title_lower == query {
			return 0.4; // Very strong boost
		}

		// Title contains query
		if title_lower.contains(query) {
			return 0.5;
		}

		// Query contains title (for short titles)
		if query.contains(&title_lower) && title_lower.len() > 2 {
			return 0.6;
		}

		// Individual word matches in title
		let query_words: Vec<&str> = query.split_whitespace().collect();
		let title_words: Vec<&str> = title_lower.split_whitespace().collect();

		let mut matches = 0;
		for query_word in &query_words {
			if title_words
				.iter()
				.any(|&word| word.contains(query_word) || query_word.contains(word))
			{
				matches += 1;
			}
		}

		if matches > 0 {
			let match_ratio = matches as f32 / query_words.len() as f32;
			return 0.6 + (match_ratio * 0.2); // 0.6 to 0.8 range
		}

		1.0 // No change if no matches
	}

	/// Calculate symbol match factor
	fn symbol_match_factor(symbols: &[String], query: &str) -> f32 {
		for symbol in symbols {
			let symbol_lower = symbol.to_lowercase();
			if symbol_lower.contains(&query.to_lowercase())
				|| query.to_lowercase().contains(&symbol_lower)
			{
				return 0.6; // Strong boost for symbol matches
			}
		}
		1.0
	}

	/// Calculate path relevance factor
	fn path_relevance_factor(path: &str, query: &str) -> f32 {
		let path_lower = path.to_lowercase();
		let query_lower = query.to_lowercase();

		// Check filename
		if let Some(filename) = path_lower.split('/').next_back() {
			if filename.contains(&query_lower) {
				return 0.75;
			}
		}

		// Check directory names
		if path_lower.contains(&query_lower) {
			return 0.85;
		}

		1.0
	}

	/// Content length factor - prefer reasonably sized blocks
	fn content_length_factor(content: &str) -> f32 {
		let length = content.len();

		match length {
			0..=50 => 0.9,       // Very short - might lack context
			51..=500 => 0.95,    // Good size - easy to read
			501..=2000 => 1.0,   // Ideal size - good context
			2001..=5000 => 0.98, // Long but manageable
			_ => 0.95,           // Very long - harder to scan
		}
	}

	/// Header level factor - prefer higher level headers in docs
	fn header_level_factor(level: usize) -> f32 {
		match level {
			1 => 0.9,  // H1 - main topics
			2 => 0.85, // H2 - sections
			3 => 0.9,  // H3 - subsections
			4 => 0.95, // H4 - details
			_ => 1.0,  // H5+ or other
		}
	}

	/// Calculate tf-idf style boosting based on term frequency
	pub fn tf_idf_boost(blocks: &mut [CodeBlock], query: &str) {
		let query_lower = query.to_lowercase();
		let query_terms: Vec<&str> = query_lower.split_whitespace().collect();

		// Calculate document frequency for each term
		let mut doc_freq: HashMap<String, usize> = HashMap::new();
		let total_docs = blocks.len();

		for block in blocks.iter() {
			let content_lower = block.content.to_lowercase();
			let mut seen_terms = std::collections::HashSet::new();

			for term in &query_terms {
				if content_lower.contains(term) && !seen_terms.contains(term) {
					*doc_freq.entry(term.to_string()).or_insert(0) += 1;
					seen_terms.insert(term);
				}
			}
		}

		// Apply tf-idf scoring
		for block in blocks.iter_mut() {
			let content_lower = block.content.to_lowercase();
			let mut tf_idf_score = 0.0;

			for term in &query_terms {
				// Term frequency in this document
				let tf = content_lower.matches(term).count() as f32;

				// Inverse document frequency
				let df = doc_freq.get(*term).unwrap_or(&1);
				let idf = (total_docs as f32 / *df as f32).ln();

				tf_idf_score += tf * idf;
			}

			if tf_idf_score > 0.0 {
				// Apply tf-idf boost (reduce distance for higher tf-idf)
				let boost_factor = (1.0 - (tf_idf_score / 10.0).min(0.3)).max(0.5);
				if let Some(distance) = block.distance {
					block.distance = Some(distance * boost_factor);
				}
			}
		}
	}
}



================================================
FILE: src/state.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use parking_lot::RwLock;
use std::path::PathBuf;
use std::sync::Arc;

#[derive(Default)]
pub struct IndexState {
	pub current_directory: PathBuf,
	pub indexed_files: usize,
	pub total_files: usize,
	pub skipped_files: usize, // Files skipped due to being unchanged
	pub embedding_calls: usize,
	pub indexing_complete: bool,
	pub status_message: String,
	pub force_reindex: bool,
	// GraphRAG state tracking
	pub graphrag_enabled: bool,
	pub graphrag_blocks: usize,
	// File counting state
	pub counting_files: bool,
	// Quiet mode for MCP server (no console output)
	pub quiet_mode: bool,
}

pub type SharedState = Arc<RwLock<IndexState>>;

pub fn create_shared_state() -> SharedState {
	Arc::new(RwLock::new(IndexState::default()))
}



================================================
FILE: src/storage.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use anyhow::Result;
use sha2::{Digest, Sha256};
use std::fs;
use std::path::{Path, PathBuf};
use std::process::Command;

/// Get the system-wide storage directory for Octocode
/// Following XDG Base Directory specification on Unix-like systems
/// and proper conventions on other systems
pub fn get_system_storage_dir() -> Result<PathBuf> {
	let base_dir = if cfg!(target_os = "macos") {
		// macOS: ~/.local/share/octocode
		dirs::home_dir()
			.ok_or_else(|| anyhow::anyhow!("Unable to determine home directory"))?
			.join(".local")
			.join("share")
			.join("octocode")
	} else if cfg!(target_os = "windows") {
		// Windows: %APPDATA%/octocode
		dirs::data_dir()
			.ok_or_else(|| anyhow::anyhow!("Unable to determine data directory"))?
			.join("octocode")
	} else {
		// Linux and other Unix-like: ~/.local/share/octocode or $XDG_DATA_HOME/octocode
		if let Ok(xdg_data_home) = std::env::var("XDG_DATA_HOME") {
			PathBuf::from(xdg_data_home).join("octocode")
		} else {
			dirs::home_dir()
				.ok_or_else(|| anyhow::anyhow!("Unable to determine home directory"))?
				.join(".local")
				.join("share")
				.join("octocode")
		}
	};

	// Create the directory if it doesn't exist
	if !base_dir.exists() {
		fs::create_dir_all(&base_dir)?;
	}

	Ok(base_dir)
}

/// Get the project identifier for a given directory
/// First tries to get Git remote URL, falls back to path hash
pub fn get_project_identifier(project_path: &Path) -> Result<String> {
	// Try to get git remote URL first
	if let Ok(git_remote) = get_git_remote_url(project_path) {
		// Create a hash from the git remote URL
		let mut hasher = Sha256::new();
		hasher.update(git_remote.as_bytes());
		let result = hasher.finalize();
		return Ok(format!("{:x}", result)[..16].to_string()); // Use first 16 chars
	}

	// Fallback to absolute path hash
	let absolute_path = project_path.canonicalize().or_else(|_| {
		// If canonicalize fails, try to get absolute path manually
		if project_path.is_absolute() {
			Ok(project_path.to_path_buf())
		} else {
			std::env::current_dir().map(|cwd| cwd.join(project_path))
		}
	})?;

	let mut hasher = Sha256::new();
	hasher.update(absolute_path.to_string_lossy().as_bytes());
	let result = hasher.finalize();
	Ok(format!("{:x}", result)[..16].to_string()) // Use first 16 chars
}

/// Try to get the Git remote URL for a project
fn get_git_remote_url(project_path: &Path) -> Result<String> {
	let output = Command::new("git")
		.arg("-C")
		.arg(project_path)
		.arg("remote")
		.arg("get-url")
		.arg("origin")
		.output()?;

	if output.status.success() {
		let url = String::from_utf8(output.stdout)?.trim().to_string();

		if !url.is_empty() {
			return Ok(normalize_git_url(&url));
		}
	}

	Err(anyhow::anyhow!("No git remote found"))
}

/// Normalize git URL to be consistent regardless of protocol
/// e.g., https://github.com/user/repo.git and git@github.com:user/repo.git
/// both become github.com/user/repo
fn normalize_git_url(url: &str) -> String {
	let url = url.trim();

	// Remove .git suffix if present
	let url = if let Some(stripped) = url.strip_suffix(".git") {
		stripped
	} else {
		url
	};

	// Handle SSH format: git@host:user/repo
	if url.contains("@") && url.contains(":") && !url.contains("://") {
		if let Some(at_pos) = url.find('@') {
			if let Some(colon_pos) = url[at_pos..].find(':') {
				let host = &url[at_pos + 1..at_pos + colon_pos];
				let path = &url[at_pos + colon_pos + 1..];
				return format!("{}/{}", host, path);
			}
		}
	}

	// Handle HTTPS format: https://host/user/repo
	if url.starts_with("http://") || url.starts_with("https://") {
		if let Some(scheme_end) = url.find("://") {
			return url[scheme_end + 3..].to_string();
		}
	}

	// Return as-is if we can't parse it
	url.to_string()
}

/// Get the storage path for a specific project
pub fn get_project_storage_path(project_path: &Path) -> Result<PathBuf> {
	let system_dir = get_system_storage_dir()?;
	let project_id = get_project_identifier(project_path)?;

	Ok(system_dir.join(project_id))
}

/// Get the database path for a specific project
pub fn get_project_database_path(project_path: &Path) -> Result<PathBuf> {
	let project_storage = get_project_storage_path(project_path)?;
	Ok(project_storage.join("storage"))
}

/// Get the config path for a specific project (local to project)
/// Config remains local to projects for project-specific settings
pub fn get_project_config_path(project_path: &Path) -> Result<PathBuf> {
	Ok(project_path.join(".octocode"))
}

/// Get the system-wide FastEmbed cache directory
/// Stored directly under ~/.local/share/octocode/fastembed/ on all systems
pub fn get_fastembed_cache_dir() -> Result<PathBuf> {
	let cache_dir = get_system_storage_dir()?.join("fastembed");

	// Create the directory if it doesn't exist
	if !cache_dir.exists() {
		fs::create_dir_all(&cache_dir)?;
	}

	Ok(cache_dir)
}

/// Get the system-wide SentenceTransformer cache directory
/// Stored directly under ~/.local/share/octocode/sentencetransformer/ on all systems
pub fn get_huggingface_cache_dir() -> Result<PathBuf> {
	let cache_dir = get_system_storage_dir()?.join("sentencetransformer");

	// Create the directory if it doesn't exist
	if !cache_dir.exists() {
		fs::create_dir_all(&cache_dir)?;
	}

	Ok(cache_dir)
}

/// Ensure the project storage directory exists
pub fn ensure_project_storage_exists(project_path: &Path) -> Result<PathBuf> {
	let storage_path = get_project_storage_path(project_path)?;

	if !storage_path.exists() {
		fs::create_dir_all(&storage_path)?;
	}

	Ok(storage_path)
}

#[cfg(test)]
mod tests {
	use super::*;
	use std::env;

	#[test]
	fn test_normalize_git_url() {
		// HTTPS URLs
		assert_eq!(
			normalize_git_url("https://github.com/user/repo.git"),
			"github.com/user/repo"
		);
		assert_eq!(
			normalize_git_url("https://github.com/user/repo"),
			"github.com/user/repo"
		);

		// SSH URLs
		assert_eq!(
			normalize_git_url("git@github.com:user/repo.git"),
			"github.com/user/repo"
		);
		assert_eq!(
			normalize_git_url("git@github.com:user/repo"),
			"github.com/user/repo"
		);

		// Other formats should remain unchanged
		assert_eq!(
			normalize_git_url("local/path/to/repo"),
			"local/path/to/repo"
		);
	}

	#[test]
	fn test_project_identifier() {
		let temp_dir = env::temp_dir().join("test_octocode");
		let _ = fs::create_dir_all(&temp_dir);

		// Should not panic and should return a consistent hash
		let id1 = get_project_identifier(&temp_dir).unwrap();
		let id2 = get_project_identifier(&temp_dir).unwrap();

		assert_eq!(id1, id2);
		assert_eq!(id1.len(), 16); // Should be 16 characters

		let _ = fs::remove_dir_all(&temp_dir);
	}

	#[test]
	fn test_system_storage_dir() {
		let storage_dir = get_system_storage_dir().unwrap();

		// Should contain "octocode" in the path
		assert!(storage_dir.to_string_lossy().contains("octocode"));

		// Should be an absolute path
		assert!(storage_dir.is_absolute());
	}

	#[test]
	fn test_fastembed_cache_dir() {
		let fastembed_cache = get_fastembed_cache_dir().unwrap();

		// Should contain "octocode" and "fastembed" in the path
		assert!(fastembed_cache.to_string_lossy().contains("octocode"));
		assert!(fastembed_cache.to_string_lossy().contains("fastembed"));

		// Should be an absolute path
		assert!(fastembed_cache.is_absolute());

		// Should be a direct subdirectory of system storage directory
		let storage_dir = get_system_storage_dir().unwrap();
		assert!(fastembed_cache.starts_with(&storage_dir));
		assert_eq!(fastembed_cache, storage_dir.join("fastembed"));
	}

	#[test]
	fn test_sentencetransformer_cache_dir() {
		let st_cache = get_huggingface_cache_dir().unwrap();

		// Should contain "octocode" and "sentencetransformer" in the path
		assert!(st_cache.to_string_lossy().contains("octocode"));
		assert!(st_cache.to_string_lossy().contains("sentencetransformer"));

		// Should be an absolute path
		assert!(st_cache.is_absolute());

		// Should be a direct subdirectory of system storage directory
		let storage_dir = get_system_storage_dir().unwrap();
		assert!(st_cache.starts_with(&storage_dir));
		assert_eq!(st_cache, storage_dir.join("sentencetransformer"));
	}
}



================================================
FILE: src/watcher_config.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! Shared configuration for file watching and debouncing across MCP and Watch commands

use std::collections::HashSet;
use std::path::{Path, PathBuf};

/// Default debounce time in milliseconds for MCP server
pub const MCP_DEFAULT_DEBOUNCE_MS: u64 = 2000; // 2 seconds

/// Maximum debounce time in milliseconds
pub const MAX_DEBOUNCE_MS: u64 = 10000; // 10 seconds

/// Minimum debounce time in milliseconds for file watcher
pub const MIN_DEBOUNCE_MS: u64 = 500; // 500ms

/// Additional delay after debounce in milliseconds
pub const DEFAULT_ADDITIONAL_DELAY_MS: u64 = 1000; // 1 second

/// Maximum additional delay in milliseconds
pub const MAX_ADDITIONAL_DELAY_MS: u64 = 5000; // 5 seconds

/// Default debounce time in seconds for watch command
pub const WATCH_DEFAULT_DEBOUNCE_SECS: u64 = 2;

/// Maximum debounce time in seconds for watch command
pub const WATCH_MAX_DEBOUNCE_SECS: u64 = 30;

/// Minimum debounce time in seconds for watch command
pub const WATCH_MIN_DEBOUNCE_SECS: u64 = 1;

/// Ignore patterns manager for file watching
pub struct IgnorePatterns {
	gitignore_patterns: HashSet<String>,
	noindex_patterns: HashSet<String>,
	working_directory: PathBuf,
}

impl IgnorePatterns {
	/// Create a new IgnorePatterns instance by reading .gitignore and .noindex files
	pub fn new(working_directory: PathBuf) -> Self {
		let mut ignore_patterns = Self {
			gitignore_patterns: HashSet::new(),
			noindex_patterns: HashSet::new(),
			working_directory,
		};

		ignore_patterns.load_gitignore();
		ignore_patterns.load_noindex();
		ignore_patterns
	}

	/// Load patterns from .gitignore file
	fn load_gitignore(&mut self) {
		let gitignore_path = self.working_directory.join(".gitignore");
		if let Ok(content) = std::fs::read_to_string(&gitignore_path) {
			for line in content.lines() {
				let line = line.trim();
				if !line.is_empty() && !line.starts_with('#') {
					// Convert gitignore patterns to simple contains checks for now
					// This is a simplified implementation - a full implementation would use glob patterns
					let pattern = line.trim_start_matches('/').trim_end_matches('/');
					self.gitignore_patterns.insert(pattern.to_string());
				}
			}
		}
	}

	/// Load patterns from .noindex file
	fn load_noindex(&mut self) {
		let noindex_path = self.working_directory.join(".noindex");
		if let Ok(content) = std::fs::read_to_string(&noindex_path) {
			for line in content.lines() {
				let line = line.trim();
				if !line.is_empty() && !line.starts_with('#') {
					let pattern = line.trim_start_matches('/').trim_end_matches('/');
					self.noindex_patterns.insert(pattern.to_string());
				}
			}
		}
	}

	/// Check if a path should be ignored during file watching
	pub fn should_ignore_path(&self, path: &Path) -> bool {
		let path_str = path.to_string_lossy();

		// Get relative path from working directory
		let relative_path = if let Ok(rel_path) = path.strip_prefix(&self.working_directory) {
			rel_path.to_string_lossy().to_string()
		} else {
			path_str.to_string()
		};

		// Check gitignore patterns
		if self.matches_patterns(&relative_path, &self.gitignore_patterns) {
			return true;
		}

		// Check noindex patterns
		if self.matches_patterns(&relative_path, &self.noindex_patterns) {
			return true;
		}

		false
	}

	/// Check if a path matches any of the given patterns
	fn matches_patterns(&self, path: &str, patterns: &HashSet<String>) -> bool {
		for pattern in patterns {
			if self.matches_pattern(path, pattern) {
				return true;
			}
		}
		false
	}

	/// Simple pattern matching - supports basic wildcards and directory patterns
	fn matches_pattern(&self, path: &str, pattern: &str) -> bool {
		// Handle exact matches
		if path == pattern {
			return true;
		}

		// Handle directory patterns (pattern ends with /)
		if pattern.ends_with('/') {
			let dir_pattern = pattern.trim_end_matches('/');
			if path.starts_with(&format!("{}/", dir_pattern)) || path == dir_pattern {
				return true;
			}
		}

		// Handle patterns that should match anywhere in the path
		if path.contains(pattern) {
			return true;
		}

		// Handle simple wildcard patterns
		if pattern.contains('*') {
			return self.matches_wildcard(path, pattern);
		}

		// Handle patterns that match file extensions
		if let Some(ext) = pattern.strip_prefix("*.") {
			if path.ends_with(&format!(".{}", ext)) {
				return true;
			}
		}

		false
	}

	/// Simple wildcard matching for * patterns
	fn matches_wildcard(&self, path: &str, pattern: &str) -> bool {
		// Very basic wildcard implementation
		// For a full implementation, consider using the `glob` crate
		if pattern == "*" {
			return true;
		}

		if let Some(star_pos) = pattern.find('*') {
			let before = &pattern[..star_pos];
			let after = &pattern[star_pos + 1..];

			if path.starts_with(before) && path.ends_with(after) {
				return true;
			}
		}

		false
	}

	/// Reload ignore patterns (useful when files change)
	pub fn reload(&mut self) {
		self.gitignore_patterns.clear();
		self.noindex_patterns.clear();
		self.load_gitignore();
		self.load_noindex();
	}
}



================================================
FILE: src/commands/clear.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use clap::Args;
use octocode::store::Store;

#[derive(Args, Debug)]
pub struct ClearArgs {
	/// Clear mode: all (default), code, docs, or text
	#[arg(long, default_value = "all")]
	pub mode: String,
}

/// Clear database tables based on mode
pub async fn execute(store: &Store, args: &ClearArgs) -> Result<(), anyhow::Error> {
	match args.mode.as_str() {
		"all" => {
			println!("Clearing all database tables except memory data...");
			store.clear_non_memory_tables().await?;
			println!("Successfully dropped all tables and schemas (memory data preserved).");
			println!(
				"Note: Tables will be recreated with current schema on next indexing operation."
			);
		}
		"code" => {
			println!("Clearing code blocks table...");
			store.clear_code_table().await?;
			store.clear_git_metadata().await?;
			println!("Successfully cleared code blocks table and git metadata.");
			println!("Note: Code content will be re-indexed on next indexing operation.");
		}
		"docs" => {
			println!("Clearing document blocks table...");
			store.clear_docs_table().await?;
			store.clear_git_metadata().await?;
			println!("Successfully cleared document blocks table and git metadata.");
			println!("Note: Documentation content will be re-indexed on next indexing operation.");
		}
		"text" => {
			println!("Clearing text blocks table...");
			store.clear_text_table().await?;
			store.clear_git_metadata().await?;
			println!("Successfully cleared text blocks table and git metadata.");
			println!("Note: Text content will be re-indexed on next indexing operation.");
		}
		_ => {
			return Err(anyhow::anyhow!(
				"Invalid mode '{}'. Valid modes are: all, code, docs, text",
				args.mode
			));
		}
	}
	Ok(())
}



================================================
FILE: src/commands/commit.rs
================================================
// Copyright 2025 Muvon Un Limited
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use anyhow::Result;
use clap::Args;
use std::io::{self, Write};
use std::process::Command;

use octocode::config::Config;
use octocode::indexer::git_utils::GitUtils;

#[derive(Args, Debug)]
pub struct CommitArgs {
	/// Add all changes before committing
	#[arg(short, long)]
	pub all: bool,

	/// Additional context to help AI generate better commit message (guidance, not the base message)
	#[arg(short, long)]
	pub message: Option<String>,

	/// Skip confirmation prompt
	#[arg(short, long)]
	pub yes: bool,

	/// Skip pre-commit hooks and commit-msg hooks
	/// Note: Pre-commit hooks run automatically if pre-commit binary and config are detected
	#[arg(short, long)]
	pub no_verify: bool,
}

/// Execute the commit command with intelligent pre-commit hook integration.
///
/// Pre-commit hooks are automatically detected and run if:
/// - The `pre-commit` binary is available in PATH
/// - A `.pre-commit-config.yaml` or `.pre-commit-config.yml` file exists
/// - The `--no-verify` flag is not used
///
/// When `--all` is specified, pre-commit runs with `--all-files`.
/// Otherwise, it runs only on staged files (default behavior).
///
/// If pre-commit modifies files, they are automatically re-staged before
/// generating the commit message with AI.
pub async fn execute(config: &Config, args: &CommitArgs) -> Result<()> {
	let current_dir = std::env::current_dir()?;

	// Find git repository root
	let git_root = GitUtils::find_git_root(&current_dir)
		.ok_or_else(|| anyhow::anyhow!("❌ Not in a git repository!"))?;

	// Use git root as working directory for all operations
	let current_dir = git_root;

	// Add all files if requested
	if args.all {
		println!("📂 Adding all changes...");
		let output = Command::new("git")
			.args(["add", "."])
			.current_dir(&current_dir)
			.output()?;

		if !output.status.success() {
			return Err(anyhow::anyhow!(
				"Failed to add files: {}",
				String::from_utf8_lossy(&output.stderr)
			));
		}
	}

	// Check if there are staged changes
	let output = Command::new("git")
		.args(["diff", "--cached", "--name-only"])
		.current_dir(&current_dir)
		.output()?;

	if !output.status.success() {
		return Err(anyhow::anyhow!(
			"Failed to check staged changes: {}",
			String::from_utf8_lossy(&output.stderr)
		));
	}

	let staged_files = String::from_utf8(output.stdout)?;
	if staged_files.trim().is_empty() {
		return Err(anyhow::anyhow!(
			"❌ No staged changes to commit. Use 'git add' or --all flag."
		));
	}

	println!("📋 Staged files:");
	for file in staged_files.lines() {
		println!("  • {}", file);
	}

	// Run pre-commit hooks if available and not skipped
	if !args.no_verify {
		let originally_staged_files: Vec<String> =
			staged_files.lines().map(|s| s.to_string()).collect();
		run_precommit_hooks(&current_dir, args.all, &originally_staged_files).await?;
	}

	// Check staged changes again after pre-commit (files might have been modified)
	let output = Command::new("git")
		.args(["diff", "--cached", "--name-only"])
		.current_dir(&current_dir)
		.output()?;

	if !output.status.success() {
		return Err(anyhow::anyhow!(
			"Failed to check staged changes after pre-commit: {}",
			String::from_utf8_lossy(&output.stderr)
		));
	}

	let final_staged_files = String::from_utf8(output.stdout)?;
	if final_staged_files.trim().is_empty() {
		return Err(anyhow::anyhow!(
			"❌ No staged changes remaining after pre-commit hooks."
		));
	}

	// Show updated staged files if they changed
	if final_staged_files != staged_files {
		println!("\n📋 Updated staged files after pre-commit:");
		for file in final_staged_files.lines() {
			println!("  • {}", file);
		}
	}

	// Generate commit message using AI (always, but with optional context)
	println!("\n🤖 Generating commit message...");
	let commit_message =
		generate_commit_message(&current_dir, config, args.message.as_deref()).await?;

	println!("\n📝 Generated commit message:");
	println!("═══════════════════════════════════");
	println!("{}", commit_message);
	println!("═══════════════════════════════════");

	// Confirm with user (unless --yes flag is used)
	if !args.yes {
		print!("\nProceed with this commit? [y/N] ");
		io::stdout().flush()?;

		let mut input = String::new();
		io::stdin().read_line(&mut input)?;

		if !input.trim().to_lowercase().starts_with('y') {
			println!("❌ Commit cancelled.");
			return Ok(());
		}
	}

	// Perform the commit
	println!("💾 Committing changes...");
	let mut git_args = vec!["commit", "-m", &commit_message];
	if args.no_verify {
		git_args.push("--no-verify");
	}
	let output = Command::new("git")
		.args(&git_args)
		.current_dir(&current_dir)
		.output()?;

	if !output.status.success() {
		return Err(anyhow::anyhow!(
			"Failed to commit: {}",
			String::from_utf8_lossy(&output.stderr)
		));
	}

	println!("✅ Successfully committed changes!");

	// Show commit info
	let output = Command::new("git")
		.args(["log", "--oneline", "-1"])
		.current_dir(&current_dir)
		.output()?;

	if output.status.success() {
		let commit_info = String::from_utf8_lossy(&output.stdout);
		println!("📄 Commit: {}", commit_info.trim());
	}

	Ok(())
}

async fn generate_commit_message(
	repo_path: &std::path::Path,
	config: &Config,
	extra_context: Option<&str>,
) -> Result<String> {
	// Get the diff of staged changes
	let output = Command::new("git")
		.args(["diff", "--cached"])
		.current_dir(repo_path)
		.output()?;

	if !output.status.success() {
		return Err(anyhow::anyhow!(
			"Failed to get diff: {}",
			String::from_utf8_lossy(&output.stderr)
		));
	}

	let diff = String::from_utf8(output.stdout)?;

	if diff.trim().is_empty() {
		return Err(anyhow::anyhow!("No staged changes found"));
	}

	// Get list of staged files to analyze extensions
	let staged_files = GitUtils::get_staged_files(repo_path)?;
	let changed_files = staged_files.join("\n");

	// Analyze file extensions
	let has_markdown_files = changed_files
		.lines()
		.any(|file| file.ends_with(".md") || file.ends_with(".markdown") || file.ends_with(".rst"));

	let has_non_markdown_files = changed_files.lines().any(|file| {
		!file.ends_with(".md")
			&& !file.ends_with(".markdown")
			&& !file.ends_with(".rst")
			&& !file.trim().is_empty()
	});

	// Count files and changes
	let file_count = diff.matches("diff --git").count();
	let additions = diff
		.matches("\n+")
		.count()
		.saturating_sub(diff.matches("\n+++").count());
	let deletions = diff
		.matches("\n-")
		.count()
		.saturating_sub(diff.matches("\n---").count());

	// Build the guidance section
	let mut guidance_section = String::new();
	if let Some(context) = extra_context {
		guidance_section = format!("\n\nUser guidance for commit intent:\n{}", context);
	}

	// Build docs type restriction based on file analysis
	let docs_restriction = if has_non_markdown_files && !has_markdown_files {
		// Only non-markdown files changed - explicitly forbid docs
		"\n\nCRITICAL - DOCS TYPE RESTRICTION:\n\
		- NEVER use 'docs(...)' when only non-markdown files are changed\n\
		- Current changes include ONLY non-markdown files (.rs, .js, .py, .toml, etc.)\n\
		- Use 'fix', 'feat', 'refactor', 'chore', etc. instead of 'docs'\n\
		- 'docs' is ONLY for .md, .markdown, .rst files or documentation-only changes"
	} else if has_non_markdown_files && has_markdown_files {
		// Mixed files - provide guidance
		"\n\nDOCS TYPE GUIDANCE:\n\
		- Use 'docs(...)' ONLY if the primary change is documentation\n\
		- If code changes are the main focus, use appropriate code type (fix, feat, refactor)\n\
		- Mixed changes: prioritize the most significant change type"
	} else {
		// Only markdown files or no files detected - allow docs
		""
	};

	// Prepare the enhanced prompt for the LLM
	let prompt = format!(
		"Analyze this Git diff and create an appropriate commit message. Be specific and concise.\n\n\
		STRICT FORMATTING RULES:\n\
		- Format: type(scope): description (under 50 chars)\n\
		- Types: feat, fix, docs, style, refactor, test, chore, perf, ci, build\n\
		- Add '!' after type for breaking changes: feat!: or fix!:\n\
		- Be specific, avoid generic words like \"update\", \"change\", \"modify\", \"various\", \"several\"\n\
		- Use imperative mood: \"add\" not \"added\", \"fix\" not \"fixed\"\n\
		- Focus on WHAT functionality changed, not implementation details\n\
		- If user guidance provided, use it to understand the INTENT but create your own message{}\n\n\
		COMMIT TYPE SELECTION (READ CAREFULLY):\n\
		- feat: NEW functionality being added (new features, capabilities, commands)\n\
		- fix: CORRECTING bugs, errors, or broken functionality (including fixes to existing features)\n\
		- refactor: IMPROVING existing code without changing functionality (code restructuring)\n\
		- perf: OPTIMIZING performance without adding features\n\
		- docs: DOCUMENTATION changes ONLY (.md, .markdown, .rst files)\n\
		- test: ADDING or fixing tests\n\
		- style: CODE formatting, whitespace, missing semicolons (no logic changes)\n\
		- chore: MAINTENANCE tasks (dependencies, build, tooling, config)\n\
		- ci: CONTINUOUS integration changes (workflows, pipelines)\n\
		- build: BUILD system changes (Cargo.toml, package.json, Makefile){}\n\n\
		FEATURE vs FIX DECISION GUIDE:\n\
		- If code was working but had bugs/errors → use 'fix' (even for new features with bugs)\n\
		- If adding completely new functionality that didn't exist → use 'feat'\n\
		- If improving existing working code structure → use 'refactor' or 'perf'\n\
		- Examples: 'fix(auth): resolve token validation error', 'feat(auth): add OAuth2 support'\n\
		- When fixing issues in recently added features → use 'fix(scope): correct feature-name issue'\n\
		- When in doubt between feat/fix: choose 'fix' if addressing problems, 'feat' if adding completely new\n\n\
		BREAKING CHANGE DETECTION:\n\
		- Look for function signature changes, API modifications, removed public methods\n\
		- Check for interface/trait changes, configuration schema changes\n\
		- Identify database migrations, dependency version bumps with breaking changes\n\
		- If breaking changes detected, use type! format and add BREAKING CHANGE footer\n\n\
		BODY RULES (add body with bullet points if ANY of these apply):\n\
		- 4+ files changed OR 25+ lines changed\n\
		- Multiple different types of changes (feat+fix, refactor+feat, etc.)\n\
		- Complex refactoring or architectural changes\n\
		- Breaking changes or major feature additions\n\
		- Changes affect multiple modules/components\n\n\
		Body format when needed:\n\
		- Blank line after subject\n\
		- Start each point with \"- \"\n\
		- Focus on key changes and their purpose\n\
		- Explain WHY if not obvious from subject\n\
		- Keep each bullet concise (1 line max)\n\
		- For breaking changes, add footer: \"BREAKING CHANGE: description\"\n\n\
		Changes: {} files (+{} -{} lines)\n\n\
		Git diff:\n\
		```\n{}\n```\n\n\
		Generate commit message:",
		guidance_section,
		docs_restriction,
		file_count,
		additions,
		deletions,
		// Truncate diff if it's too long (keep first 4000 chars for better analysis)
		if diff.chars().count() > 4000 {
			let truncated: String = diff.chars().take(4000).collect();
			format!("{}...\n[diff truncated for brevity]", truncated)
		} else {
			diff
		}
	);

	// Call the LLM using existing infrastructure
	match call_llm_for_commit_message(&prompt, config).await {
		Ok(message) => {
			// Clean up the response but preserve multi-line structure
			let cleaned = message
				.trim()
				.trim_matches('"') // Remove quotes if present
				.trim();

			// Validate the message
			if cleaned.is_empty() {
				Ok("chore: update files".to_string())
			} else {
				// Split into lines and validate subject line length
				let lines: Vec<&str> = cleaned.lines().collect();
				if let Some(subject) = lines.first() {
					let subject = subject.trim();
					if subject.len() > 72 {
						// Truncate subject if too long but keep body if present
						let truncated_subject = if subject.chars().count() > 69 {
							let truncated: String = subject.chars().take(69).collect();
							format!("{}...", truncated)
						} else {
							format!("{}...", subject)
						};
						if lines.len() > 1 {
							let body = lines[1..].join("\n");
							Ok(format!("{}\n{}", truncated_subject, body))
						} else {
							Ok(truncated_subject)
						}
					} else {
						Ok(cleaned.to_string())
					}
				} else {
					Ok("chore: update files".to_string())
				}
			}
		}
		Err(e) => {
			eprintln!("Warning: LLM call failed ({}), using fallback", e);
			Ok("chore: update files".to_string())
		}
	}
}

async fn call_llm_for_commit_message(prompt: &str, config: &Config) -> Result<String> {
	use reqwest::Client;
	use serde_json::{json, Value};

	let client = Client::new();

	// Get API key
	let api_key = if let Some(key) = &config.openrouter.api_key {
		key.clone()
	} else if let Ok(key) = std::env::var("OPENROUTER_API_KEY") {
		key
	} else {
		return Err(anyhow::anyhow!("No OpenRouter API key found"));
	};

	// Prepare the request
	let payload = json!({
		"model": config.openrouter.model,
		"messages": [
			{
				"role": "user",
				"content": prompt
			}
		],
		"temperature": 0.1,
		"max_tokens": 180
	});

	let response = client
		.post(format!(
			"{}/chat/completions",
			config.openrouter.base_url.trim_end_matches('/')
		))
		.header("Authorization", format!("Bearer {}", api_key))
		.header("HTTP-Referer", "https://github.com/muvon/octocode")
		.header("X-Title", "Octocode")
		.header("Content-Type", "application/json")
		.json(&payload)
		.timeout(std::time::Duration::from_secs(config.openrouter.timeout))
		.send()
		.await?;

	if !response.status().is_success() {
		let error_text = response.text().await?;
		return Err(anyhow::anyhow!("LLM API error: {}", error_text));
	}

	let response_json: Value = response.json().await?;

	let message = response_json
		.get("choices")
		.and_then(|choices| choices.get(0))
		.and_then(|choice| choice.get("message"))
		.and_then(|message| message.get("content"))
		.and_then(|content| content.as_str())
		.ok_or_else(|| anyhow::anyhow!("Invalid response format from LLM"))?;

	Ok(message.to_string())
}

/// Check if pre-commit binary is available in PATH
fn is_precommit_available() -> bool {
